{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 記号の使い分け\n",
    "  - 統語範疇: `英大文字`\n",
    "  - 動詞（形式）: `ギリシャ小文字`\n",
    "  - 名詞（形式）: `英小文字`\n",
    "  - 動詞（意味表示内の定数）: `_ギリシャ小文字`\n",
    "  - 名詞（意味表示内の定数）: `_英小文字`\n",
    "  - 動詞（意味表示内の変数）: `X`\n",
    "  - 名詞（意味表示内の変数）: `x`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "import ast\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 動詞にはギリシャ文字を使用する\n",
    "def generate_noun(n_sorts: str, stopword: str = \"\"):\n",
    "    assert n_sorts <= len(string.ascii_lowercase)\n",
    "    chars_wo_stopword = [char for char in string.ascii_lowercase[:n_sorts] if char not in stopword]\n",
    "    return random.choice(chars_wo_stopword)\n",
    "def generate_verb(n_sorts: str, stopword: str = \"\"):\n",
    "    greek_lowercase = [chr(i) for i in range(945, 970)]\n",
    "    chars_wo_stopword = [char for char in greek_lowercase[:n_sorts] if char not in stopword]\n",
    "    return random.choice(chars_wo_stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NOUNS = [\n",
    "    \"john\", \"alice\", \"bob\", \"carol\", \"eve\", \"frank\", \"grace\", \"helen\", \n",
    "    \"ivan\", \"jack\", \"karen\", \"larry\", \"mike\", \"nina\", \"oscar\", \"paul\", \n",
    "    \"quincy\", \"rachel\", \"steve\", \"tracy\", \"ursula\", \"victor\", \"wendy\", \n",
    "    \"xander\", \"yasmine\", \"zach\"\n",
    "]\n",
    "\n",
    "VERBS = [\n",
    "    \"know\", \"admire\", \"like\", \"kick\", \"meet\", \"call\", \"defend\", \"encourage\"\n",
    "    , \"follow\", \"greet\", \"help\", \"invite\", \"judge\", \"notice\", \"obey\", \n",
    "    \"praise\", \"question\", \"respect\", \"support\", \"trust\", \"understand\", \n",
    "    \"value\", \"warn\", \"x-ray\", \"yell\", \"zap\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 動詞にはギリシャ文字を使用する\n",
    "def generate_noun_sem(n_sorts: str, stopword: str = \"\"):\n",
    "    assert n_sorts <= len(NOUNS)\n",
    "    chars_wo_stopword = [char for char in NOUNS[:n_sorts] if char not in stopword]\n",
    "    return random.choice(chars_wo_stopword)\n",
    "def generate_verb_sem(n_sorts: str, stopword: str = \"\"):\n",
    "    chars_wo_stopword = [char for char in VERBS[:n_sorts] if char not in stopword]\n",
    "    return random.choice(chars_wo_stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 全体論的なルールの生成\n",
    "def generate_holistic_rules(n):\n",
    "    result = \"\"\n",
    "    for i in range(n):\n",
    "        subj = generate_noun(5, [\"x\", \"y\"])\n",
    "        obj = generate_noun(5, [subj, \"x\", \"y\"])\n",
    "        verb = generate_verb(5)\n",
    "        sentence = f\"{subj}{verb}{obj}\"\n",
    "        subjsem = generate_noun_sem(5)\n",
    "        objsem = generate_noun_sem(5, [subjsem])\n",
    "        verbsem = generate_verb_sem(5)\n",
    "        semrepr = f\"_{verbsem}(_{subjsem},_{objsem})\"\n",
    "        result += f\"S/{semrepr} -> {sentence}\\n\"\n",
    "    return result[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: remove duplicates\n",
    "def generate_queries(n):\n",
    "    result = []\n",
    "    for i in range(n):\n",
    "        subjsem = generate_noun_sem(5)\n",
    "        objsem = generate_noun_sem(5, [subjsem])\n",
    "        verbsem = generate_verb_sem(5)\n",
    "        semrepr = f\"_{verbsem}(_{subjsem},_{objsem})\"\n",
    "        result.append(semrepr)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S/_meet(_bob,_carol) -> cαa\n",
      "S/_like(_eve,_carol) -> bγa\n",
      "S/_kick(_carol,_john) -> cαb\n",
      "S/_like(_eve,_bob) -> cεb\n",
      "S/_meet(_eve,_bob) -> dβc\n",
      "S/_meet(_eve,_carol) -> aδb\n",
      "S/_kick(_carol,_eve) -> eγd\n",
      "S/_kick(_eve,_bob) -> aβe\n",
      "S/_like(_alice,_eve) -> aεc\n",
      "S/_meet(_carol,_alice) -> aβc\n"
     ]
    }
   ],
   "source": [
    "print(generate_holistic_rules(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.sem.logic import *\n",
    "\n",
    "import random\n",
    "import string\n",
    "import copy\n",
    "\n",
    "def replace_at_index(s, index, replacement):\n",
    "    return s[:index] + replacement + s[index + 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_SORTS = 5\n",
    "NONTERMINALS = [char for char in string.ascii_uppercase if char != \"S\"]\n",
    "INDIVIDUAL_VARIABLES = [\"x\", \"y\"]\n",
    "GREEK_LOWER = [chr(i) for i in range(945, 970)]\n",
    "ENGLISH_LOWER = [char for char in string.ascii_lowercase[:N_SORTS] if char not in [\"x\", \"y\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Grammar():\n",
    "    def __init__(self):\n",
    "        self.rules = []\n",
    "\n",
    "    def from_string(self, string: str):\n",
    "        self.rules = []\n",
    "        rules_str = string.split(\"\\n\")\n",
    "        for rule_str in rules_str:\n",
    "            if rule_str != \"\\n\":\n",
    "                rule_split = rule_str.split(\" \")\n",
    "                lhs_cat = rule_split[0].split(\"/\")[0]\n",
    "                lhs_sem = rule_split[0].split(\"/\")[1]\n",
    "                rhs = rule_split[2]\n",
    "                rule = {\"lhs\":{\"cat\":lhs_cat, \"sem\":lhs_sem}, \"rhs\":rhs}\n",
    "                self.rules.append(rule)\n",
    "    \n",
    "    def add_rule(self, string: str):\n",
    "        rules_str = string.split(\"\\n\")\n",
    "        for rule_str in rules_str:\n",
    "            if rule_str != \"\\n\":\n",
    "                rule_split = rule_str.split(\" \")\n",
    "                lhs_cat = rule_split[0].split(\"/\")[0]\n",
    "                lhs_sem = rule_split[0].split(\"/\")[1]\n",
    "                rhs = rule_split[2]\n",
    "                rule = {\"lhs\":{\"cat\":lhs_cat, \"sem\":lhs_sem}, \"rhs\":rhs}\n",
    "                self.rules.append(rule)\n",
    "    \n",
    "    def to_string(self):\n",
    "        rules_list = []\n",
    "        rules_str = \"\"\n",
    "        for rule in self.rules:\n",
    "            rules_list.append(f\"{rule['lhs']['cat']}/{rule['lhs']['sem']} -> {rule['rhs']}\\n\")\n",
    "        sorted_list = sorted(rules_list, key=lambda x: -len(x))\n",
    "        rules_str += ''.join(sorted_list)+\"\\n\"\n",
    "        return rules_str\n",
    "    \n",
    "    def str2dict(self, string):\n",
    "        return ast.literal_eval(string)\n",
    "    \n",
    "    def sem_list(self):\n",
    "        return [d['lhs']['sem'] for d in self.rules]\n",
    "    \n",
    "    def cat_list(self):\n",
    "        return [d['lhs']['cat'] for d in self.rules]\n",
    "    \n",
    "    def sentence_list(self):\n",
    "        return [d['rhs'] for d in self.rules]\n",
    "\n",
    "    def can_chunk01(self, rule1, rule2):\n",
    "        str1, str2 = rule1[\"rhs\"], rule2[\"rhs\"]\n",
    "        sem1_logic = Expression.fromstring(rule1[\"lhs\"][\"sem\"])\n",
    "        sem2_logic = Expression.fromstring(rule2[\"lhs\"][\"sem\"])\n",
    "        sem1, sem2 = [sem1_logic.args[0], sem1_logic.pred, sem1_logic.args[1]], [sem2_logic.args[0], sem2_logic.pred, sem2_logic.args[1]]\n",
    "\n",
    "        if len(str1) != 3 or len(str2) != 3:\n",
    "            return False\n",
    "\n",
    "        # print(str1, str2)\n",
    "        diff_count_str = 0\n",
    "        diff_positions_str = []\n",
    "        for i, (char1, char2) in enumerate(zip(str1, str2)):\n",
    "            if (char1 != char2):\n",
    "                diff_count_str += 1\n",
    "                diff_positions_str.append(i)\n",
    "                if not (char1.islower() and char2.islower()):\n",
    "                    return None, False\n",
    "        diff_count_sem = 0\n",
    "        diff_positions_sem = []\n",
    "        for i, (elm1, elm2) in enumerate(zip(sem1, sem2)):\n",
    "            if (elm1 != elm2):\n",
    "                diff_count_sem += 1\n",
    "                diff_positions_sem.append(i)\n",
    "        if diff_count_str == diff_count_sem == 1:\n",
    "            if diff_positions_str[0] == diff_positions_sem[0]:\n",
    "                return diff_positions_str[0], True\n",
    "        return None, False\n",
    "    \n",
    "    def find_diff_position_for_chunk01(self, str1, str2):\n",
    "        if len(str1) != 3 or len(str2) != 3:\n",
    "            return None\n",
    "\n",
    "        diff_count = 0\n",
    "        diff_index = None\n",
    "        for index, (char1, char2) in enumerate(zip(str1, str2)):\n",
    "            if char1 != char2:\n",
    "                diff_count += 1\n",
    "                diff_index = index\n",
    "                if not (char1.islower() and char2.islower()):\n",
    "                    return None\n",
    "\n",
    "        if diff_count == 1:\n",
    "            if diff_index == 0:\n",
    "                diff_index_sem = 1\n",
    "            if diff_index == 1:\n",
    "                diff_index_sem = 0\n",
    "            if diff_index == 2:\n",
    "                diff_index_sem = 2\n",
    "            return diff_index, diff_index_sem\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def can_chunk02(self, rule1, rule2):\n",
    "        str1, str2 = rule1[\"rhs\"], rule2[\"rhs\"]\n",
    "        sem1_logic = Expression.fromstring(rule1[\"lhs\"][\"sem\"])\n",
    "        sem2_logic = Expression.fromstring(rule2[\"lhs\"][\"sem\"])\n",
    "        sem1, sem2 = [sem1_logic.args[0], sem1_logic.pred, sem1_logic.args[1]], [sem2_logic.args[0], sem2_logic.pred, sem2_logic.args[1]]\n",
    "\n",
    "        if len(str1) != 3 or len(str2) != 3:\n",
    "            return None, False, None, None\n",
    "\n",
    "        # print(str1, str2)\n",
    "        diff_count_str = 0\n",
    "        diff_positions_str = []\n",
    "        for i, (char1, char2) in enumerate(zip(str1, str2)):\n",
    "            if (char1 != char2):\n",
    "                if (char1.islower() and char2.isupper()):\n",
    "                    diff_count_str += 1\n",
    "                    diff_positions_str.append(i)\n",
    "                    lower_in_str = 0\n",
    "                    upper_in_str = 1\n",
    "                elif (char1.isupper() and char2.islower()):\n",
    "                    diff_count_str += 1\n",
    "                    diff_positions_str.append(i)\n",
    "                    lower_in_str = 1\n",
    "                    upper_in_str = 0\n",
    "                else:\n",
    "                    None, False, None, None\n",
    "        diff_count_sem = 0\n",
    "        diff_positions_sem = []\n",
    "        for i, (elm1, elm2) in enumerate(zip(sem1, sem2)):\n",
    "            if (elm1 != elm2):\n",
    "                diff_count_sem += 1\n",
    "                diff_positions_sem.append(i)\n",
    "        if diff_count_str == diff_count_sem == 1:\n",
    "            if diff_positions_str[0] == diff_positions_sem[0]:\n",
    "                return diff_positions_str[0], True, upper_in_str, lower_in_str\n",
    "        return None, False, None, None\n",
    "    \n",
    "    def find_diff_position_for_chunk02(self, str1, str2):\n",
    "        if len(str1) != 3 or len(str2) != 3:\n",
    "            return None, None\n",
    "\n",
    "        diff_count = 0\n",
    "        diff_index = None\n",
    "        upper_in_str = None\n",
    "\n",
    "        for index, (char1, char2) in enumerate(zip(str1, str2)):\n",
    "            if char1 != char2:\n",
    "                diff_count += 1\n",
    "                diff_index = index\n",
    "                if (char1.islower() and char2.isupper()):\n",
    "                    lower_in_str = 0\n",
    "                    upper_in_str = 1\n",
    "                elif (char1.isupper() and char2.islower()):\n",
    "                    lower_in_str = 1\n",
    "                    upper_in_str = 0\n",
    "                else:\n",
    "                    return None, None\n",
    "\n",
    "        if diff_count == 1:\n",
    "            if diff_index == 0:\n",
    "                diff_index_sem = 1\n",
    "            if diff_index == 1:\n",
    "                diff_index_sem = 0\n",
    "            if diff_index == 2:\n",
    "                diff_index_sem = 2\n",
    "            return diff_index, diff_index_sem, upper_in_str, lower_in_str\n",
    "        else:\n",
    "            return None, None\n",
    "\n",
    "    def highlight_for_replace(self, str1, str2):\n",
    "        if (len(str1) == 3) & (len(str2) == 1):\n",
    "            if str1.count(str2) == 1:\n",
    "                non_word_level, word_level = 0, 1\n",
    "                return non_word_level, word_level\n",
    "            else:\n",
    "                return None, None\n",
    "        elif (len(str1) == 1) & (len(str2) == 3):\n",
    "            if str2.count(str1) == 1:\n",
    "                non_word_level, word_level = 1, 0\n",
    "                return non_word_level, word_level\n",
    "            else:\n",
    "                return None, None\n",
    "        else:\n",
    "            return None, None\n",
    "\n",
    "    def find_diff_position_for_replace(self, str1, str2):\n",
    "        if len(str1) != 3 or len(str2) != 1:\n",
    "            return None\n",
    "\n",
    "        for i, char in enumerate(str1):\n",
    "            if str2 == char:\n",
    "                diff_index = i\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        if diff_index == 0:\n",
    "            diff_index_sem = 1\n",
    "        if diff_index == 1:\n",
    "            diff_index_sem = 0\n",
    "        if diff_index == 2:\n",
    "            diff_index_sem = 2\n",
    "        return diff_index, diff_index_sem\n",
    "    \n",
    "    def existing_variables(self, elements: list):\n",
    "        result = [str(element) for element in elements if not isinstance(element, ConstantExpression)]\n",
    "        return result\n",
    "    # TODO: sentence-meaningの位置は必ずしも対応しなくて良い\n",
    "    def chunk01(self):\n",
    "        rules = self.rules\n",
    "\n",
    "        chuncked_rules = []\n",
    "        new_rules = []\n",
    "\n",
    "        for i in range(len(rules)):\n",
    "            for j in range(i+1, len(rules)):\n",
    "                first_sem = Expression.fromstring(rules[i][\"lhs\"][\"sem\"])\n",
    "                second_sem = Expression.fromstring(rules[j][\"lhs\"][\"sem\"])\n",
    "                first_sentence = rules[i][\"rhs\"]\n",
    "                second_sentence = rules[j][\"rhs\"]\n",
    "                if isinstance(first_sem, ApplicationExpression) & isinstance(second_sem, ApplicationExpression):\n",
    "                    diff_index, can_chunk = self.can_chunk01(rules[i], rules[j])\n",
    "                    if can_chunk:\n",
    "                        first_sem_elements = [first_sem.pred, first_sem.args[0], first_sem.args[1]]\n",
    "                        second_sem_elements = [second_sem.pred, second_sem.args[0], second_sem.args[1]]\n",
    "\n",
    "                        if diff_index == 0:\n",
    "                            diff_index_sem = 1\n",
    "                        if diff_index == 1:\n",
    "                            diff_index_sem = 0\n",
    "                        if diff_index == 2:\n",
    "                            diff_index_sem = 2\n",
    "\n",
    "                        chuncked_rules.append(rules[i])\n",
    "                        chuncked_rules.append(rules[j])\n",
    "\n",
    "                        if diff_index_sem == 0:\n",
    "                            var = Expression.fromstring(\"X\")\n",
    "                        else:\n",
    "                            # FIX: efficiency\n",
    "                            existing_variables = self.existing_variables(first_sem_elements)\n",
    "                            var = [var for var in INDIVIDUAL_VARIABLES if var not in existing_variables][0]\n",
    "                        first_sem_elements_abstracted = copy.deepcopy(first_sem_elements)\n",
    "                        first_sem_elements_abstracted[diff_index_sem] = var\n",
    "                        new_sem_str = f\"{str(first_sem_elements_abstracted[0])}({str(first_sem_elements_abstracted[1])},{str(first_sem_elements_abstracted[2])})\"\n",
    "                        random_category = random.choice(NONTERMINALS)\n",
    "                        new_sen_str = replace_at_index(first_sentence, diff_index, random_category)\n",
    "                        new_rule_0 = {\"lhs\": {\"cat\":\"S\", \"sem\": new_sem_str}, \"rhs\": new_sen_str}\n",
    "\n",
    "                        new_rule_1 = {\"lhs\": {\"cat\":random_category, \"sem\": str(first_sem_elements[diff_index_sem])}, \"rhs\": first_sentence[diff_index]}\n",
    "                        new_rule_2 = {\"lhs\": {\"cat\":random_category, \"sem\": str(second_sem_elements[diff_index_sem])}, \"rhs\": second_sentence[diff_index]}\n",
    "                        new_rules += [new_rule_0] + [new_rule_1] + [new_rule_2]\n",
    "        rules = [rule for rule in rules if rule not in chuncked_rules]\n",
    "        rules = rules + new_rules\n",
    "        rules_unique = list(set([str(rule) for rule in rules]))\n",
    "        self.rules = [self.str2dict(rule) for rule in rules_unique]\n",
    "\n",
    "    def chunk02(self):\n",
    "        rules = self.rules\n",
    "\n",
    "        chuncked_rules = []\n",
    "        new_rules = []\n",
    "\n",
    "        for i in range(len(rules)):\n",
    "            for j in range(i+1, len(rules)):\n",
    "                first_sem = Expression.fromstring(rules[i][\"lhs\"][\"sem\"])\n",
    "                second_sem = Expression.fromstring(rules[j][\"lhs\"][\"sem\"])\n",
    "                first_sentence = rules[i][\"rhs\"]\n",
    "                second_sentence = rules[j][\"rhs\"]\n",
    "                if isinstance(first_sem, ApplicationExpression) & isinstance(second_sem, ApplicationExpression):\n",
    "                    diff_index, can_chunk, upper_in_str, lower_in_str = self.can_chunk02(rules[i], rules[j])\n",
    "                    if can_chunk:\n",
    "                        first_sem_elements = [first_sem.pred, first_sem.args[0], first_sem.args[1]]\n",
    "                        second_sem_elements = [second_sem.pred, second_sem.args[0], second_sem.args[1]]\n",
    "\n",
    "                        if diff_index == 0:\n",
    "                            diff_index_sem = 1\n",
    "                        if diff_index == 1:\n",
    "                            diff_index_sem = 0\n",
    "                        if diff_index == 2:\n",
    "                            diff_index_sem = 2\n",
    "\n",
    "                        target_position = [i,j][lower_in_str]\n",
    "                        nontarget_position = [i,j][upper_in_str]\n",
    "                        chuncked_rules.append(rules[target_position])\n",
    "\n",
    "                        target_sem = Expression.fromstring(rules[target_position][\"lhs\"][\"sem\"])\n",
    "                        target_sem_elements = [target_sem.pred, target_sem.args[0], target_sem.args[1]]\n",
    "                        target_sentence = rules[target_position][\"rhs\"]\n",
    "                        nontarget_sentence = rules[nontarget_position][\"rhs\"]\n",
    "                        new_rule = {\"lhs\": {\"cat\":nontarget_sentence[diff_index], \"sem\": str(target_sem_elements[diff_index_sem])}, \"rhs\": target_sentence[diff_index]}\n",
    "                        new_rules.append(new_rule)\n",
    "        rules = [rule for rule in rules if rule not in chuncked_rules]\n",
    "        rules = rules + new_rules\n",
    "        rules_unique = list(set([str(rule) for rule in rules]))\n",
    "        self.rules = [self.str2dict(rule) for rule in rules_unique]\n",
    "        \n",
    "    def abstract(self, element, var, idx, diff_idx):\n",
    "        if idx == diff_idx:\n",
    "            return var\n",
    "        else:\n",
    "            return element\n",
    "\n",
    "    def replace(self):\n",
    "        rules = self.rules\n",
    "\n",
    "        replaced_rules = []\n",
    "        new_rules = []\n",
    "\n",
    "        for i in range(len(rules)):\n",
    "            for j in range(i+1, len(rules)):\n",
    "                first_sem = Expression.fromstring(rules[i][\"lhs\"][\"sem\"])\n",
    "                second_sem = Expression.fromstring(rules[j][\"lhs\"][\"sem\"])\n",
    "                first_sentence = rules[i][\"rhs\"]\n",
    "                second_sentence = rules[j][\"rhs\"]\n",
    "                non_word_level, word_level = self.highlight_for_replace(first_sentence, second_sentence)\n",
    "                if (non_word_level is not None) and (word_level is not None):\n",
    "                    target_position = [i,j][non_word_level]\n",
    "                    nontarget_position  = [i,j][word_level]\n",
    "\n",
    "                    replaced_rules.append(rules[target_position])\n",
    "\n",
    "                    target_sem = Expression.fromstring(rules[target_position][\"lhs\"][\"sem\"])\n",
    "                    nontarget_sem = Expression.fromstring(rules[nontarget_position][\"lhs\"][\"sem\"])\n",
    "\n",
    "                    target_sentence = rules[target_position][\"rhs\"]\n",
    "                    nontarget_sentence = rules[nontarget_position][\"rhs\"]\n",
    "\n",
    "                    target_sem_elements = [target_sem.pred, target_sem.args[0], target_sem.args[1]]\n",
    "                    nontarget_cat = rules[nontarget_position][\"lhs\"][\"cat\"]\n",
    "\n",
    "                    diff_index, diff_index_sem = self.find_diff_position_for_replace(target_sentence, nontarget_sentence)\n",
    "\n",
    "                    if diff_index_sem == 0:\n",
    "                        var = Expression.fromstring(\"X\")\n",
    "                    else:\n",
    "                        # FIX: efficiency\n",
    "                        existing_variables = self.existing_variables(target_sem_elements)\n",
    "                        var = [var for var in INDIVIDUAL_VARIABLES if var not in existing_variables][0]\n",
    "                    target_sem_elements_abstract = [self.abstract(element, var, i, diff_index_sem) for i, element in enumerate(target_sem_elements)]\n",
    "\n",
    "                    new_sem_str = f\"{str(target_sem_elements_abstract[0])}({str(target_sem_elements_abstract[1])},{str(target_sem_elements_abstract[2])})\"\n",
    "                    new_sen_str = replace_at_index(target_sentence, diff_index, nontarget_cat)\n",
    "                    new_rule = {\"lhs\": {\"cat\":nontarget_cat, \"sem\": new_sem_str}, \"rhs\": new_sen_str}\n",
    "                    new_rules.append(new_rule)\n",
    "        rules = [rule for rule in rules if rule not in replaced_rules]\n",
    "        rules = rules + new_rules\n",
    "        rules_unique = list(set([str(rule) for rule in rules]))\n",
    "        self.rules = [self.str2dict(rule) for rule in rules_unique]\n",
    "    \n",
    "    def repaint_rule(self, rule, replacee, replacer):\n",
    "        # print(rule, replacee, replacer)\n",
    "        if rule[\"lhs\"][\"cat\"] == replacee[\"lhs\"][\"cat\"]:\n",
    "            assert len(rule[\"rhs\"]) == 1\n",
    "            # print(rule[\"lhs\"][\"cat\"], \"->\",replacer[\"lhs\"][\"cat\"])\n",
    "            rule[\"lhs\"][\"cat\"] = replacer[\"lhs\"][\"cat\"]\n",
    "        else:\n",
    "            rhs = copy.deepcopy(rule[\"rhs\"])\n",
    "            new_rhs = rhs.replace(replacee[\"lhs\"][\"cat\"], replacer[\"lhs\"][\"cat\"])\n",
    "            rule[\"rhs\"] = new_rhs\n",
    "            # print(rhs, \"->\", new_rhs)\n",
    "        return rule\n",
    "\n",
    "    def can_merge(self, rule1, rule2):\n",
    "        rule1_cat = rule1[\"lhs\"][\"cat\"]\n",
    "        rule1_sem = rule1[\"lhs\"][\"sem\"]\n",
    "        rule1_rhs = rule1[\"rhs\"]\n",
    "        rule2_cat = rule2[\"lhs\"][\"cat\"]\n",
    "        rule2_sem = rule2[\"lhs\"][\"sem\"]\n",
    "        rule2_rhs = rule2[\"rhs\"]\n",
    "        if (len(rule1_rhs) == len(rule2_rhs) == 1) & (rule1_sem == rule2_sem) & (rule1_rhs == rule2_rhs) & (rule1_cat != rule2_cat):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def merge(self):\n",
    "        rules = self.rules\n",
    "        invited = []\n",
    "        replacer_cats = []\n",
    "        for i in range(len(rules)):\n",
    "            for j in range(i+1, len(rules)):\n",
    "                if self.can_merge(rules[i], rules[j]):\n",
    "                    # print(f\"can merge {rules[i]} and {rules[j]}\")\n",
    "                    if (rules[i][\"lhs\"][\"cat\"] not in replacer_cats) and (rules[j][\"lhs\"][\"cat\"] not in replacer_cats):\n",
    "                        indices = [i, j]\n",
    "                        # print(indices)\n",
    "                        random.shuffle(indices)\n",
    "                        replacer_rule = copy.deepcopy(rules[indices[0]])\n",
    "                        replacee_rule = copy.deepcopy(rules[indices[1]])\n",
    "                        replacer_cats.append(replacer_rule[\"lhs\"][\"cat\"])\n",
    "                        # print(\"replacee_rule: \", replacee_rule)\n",
    "                        # print(\"replacer_rule: \", replacer_rule)\n",
    "                        rules = [self.repaint_rule(rule, replacee_rule, replacer_rule) for rule in rules]\n",
    "                    elif (rules[i][\"lhs\"][\"cat\"] in replacer_cats) and (rules[j][\"lhs\"][\"cat\"] in replacer_cats):\n",
    "                        continue\n",
    "                    elif rules[i][\"lhs\"][\"cat\"] in replacer_cats:\n",
    "                        replacer_rule = copy.deepcopy(rules[i])\n",
    "                        replacee_rule = copy.deepcopy(rules[j])\n",
    "                        # print(\"replacee_rule: \", replacee_rule)\n",
    "                        # print(\"replacer_rule: \", replacer_rule)\n",
    "                        rules = [self.repaint_rule(rule, replacee_rule, replacer_rule) for rule in rules]\n",
    "                    else:\n",
    "                        replacer_rule = copy.deepcopy(rules[j])\n",
    "                        replacee_rule = copy.deepcopy(rules[i])\n",
    "                        # print(\"replacee_rule: \", replacee_rule)\n",
    "                        # print(\"replacer_rule: \", replacer_rule)\n",
    "                        rules = [self.repaint_rule(rule, replacee_rule, replacer_rule) for rule in rules]\n",
    "                else:\n",
    "                    continue\n",
    "        rules_unique = list(set([str(rule) for rule in rules]))\n",
    "        self.rules = [self.str2dict(rule) for rule in rules_unique]\n",
    "\n",
    "    def invent_wordrule(self, sem):\n",
    "        random_category = random.choice(NONTERMINALS)\n",
    "        if str(sem).replace(\"_\",\"\") in NOUNS:\n",
    "            rhs = generate_noun(5, [\"x\", \"y\"])\n",
    "        if str(sem).replace(\"_\",\"\") in VERBS:\n",
    "            rhs = generate_verb(5)\n",
    "        self.add_rule(f\"{random_category}/{str(sem)} -> {rhs}\")\n",
    "        return rhs\n",
    "\n",
    "    def invent_holisticrule(self, sem):\n",
    "        subj = generate_noun(5, [\"x\", \"y\"])\n",
    "        obj = generate_noun(5, [subj, \"x\", \"y\"])\n",
    "        verb = generate_verb(5)\n",
    "        sentence = f\"{subj}{verb}{obj}\"\n",
    "        self.add_rule(f\"S/{sem} -> {sentence}\")\n",
    "        return sentence\n",
    "    # TODO: test\n",
    "    def generate(self, query_str, debug=False):\n",
    "        rules = self.rules\n",
    "        if debug:\n",
    "            print(\"query: \", query_str)\n",
    "        query = Expression.fromstring(query_str)\n",
    "        query_elements = [query.args[0], query.pred, query.args[1]]\n",
    "        sem_list = self.sem_list()\n",
    "        sentence_list = self.sentence_list()\n",
    "        if query_str in sem_list:\n",
    "            holistic_rule = rules[sem_list.index(query_str)]\n",
    "            holistic_rule_rhs = holistic_rule['rhs']\n",
    "            if debug:\n",
    "                print(\"generated by a holictic rule: \", f\"S/{str(query)} -> {holistic_rule_rhs}\")\n",
    "            return f\"S/{str(query)} -> {holistic_rule_rhs}\"\n",
    "        else:\n",
    "            for i, sem_str in enumerate(sem_list):\n",
    "                sem = Expression.fromstring(sem_str)\n",
    "                if isinstance(sem, ApplicationExpression):\n",
    "                    sem_elements = [sem.args[0], sem.pred, sem.args[1]]\n",
    "                    matches = [(i, (query_element,sem_element)) for i, (query_element,sem_element) in enumerate(zip(query_elements, sem_elements)) if (query_element==sem_element) & (isinstance(query_element, ConstantExpression) & isinstance(sem_element, ConstantExpression))]\n",
    "                    slots = [(i, (query_element,sem_element)) for i, (query_element,sem_element) in enumerate(zip(query_elements, sem_elements)) if (query_element!=sem_element) & (not isinstance(sem_element, ConstantExpression))]\n",
    "                    if (len(matches)==2) & (len(slots)==1):\n",
    "                        sentence_with_slot = sentence_list[i]\n",
    "                        slot_position = slots[0][0]\n",
    "                        slot_sem = slots[0][1][0]\n",
    "                        slot_category = sentence_with_slot[slot_position]\n",
    "                        word_rules = [rule for rule in rules if (rule['lhs']['cat']==slot_category) & (rule['lhs']['sem']==str(slot_sem))]\n",
    "                        if len(word_rules) > 0:\n",
    "                            selected_rule = self.str2dict(str(random.sample(word_rules, 1)[0]))\n",
    "                            if slot_position == 0:\n",
    "                                generated_sentence = selected_rule['rhs'] + sentence_with_slot[1:]\n",
    "                            if slot_position == 1:\n",
    "                                generated_sentence = sentence_with_slot[0] + selected_rule['rhs'] + sentence_with_slot[2]\n",
    "                            if slot_position == 2:\n",
    "                                generated_sentence = sentence_with_slot[:2] + selected_rule['rhs']\n",
    "                            return f\"S/{str(query)} -> {generated_sentence}\"\n",
    "                        if len(word_rules) == 0:\n",
    "                            invented_form = self.invent_wordrule(slot_sem)\n",
    "                            if slot_position == 0:\n",
    "                                generated_sentence = invented_form + sentence_with_slot[1:]\n",
    "                            if slot_position == 1:\n",
    "                                generated_sentence = sentence_with_slot[0] + invented_form + sentence_with_slot[2]\n",
    "                            if slot_position == 2:\n",
    "                                generated_sentence = sentence_with_slot[:2] + invented_form\n",
    "                            return f\"S/{str(query)} -> {generated_sentence}\"\n",
    "                    if (len(matches)==1) & (len(slots)==2):\n",
    "                        sentence_with_slot = sentence_list[i]\n",
    "                        slot_position = slots[0][0]\n",
    "                        slot_sem = slots[0][1][0]\n",
    "                        slot_category = sentence_with_slot[slot_position]\n",
    "                        word_rules = [rule for rule in rules if (rule['lhs']['cat']==slot_category) & (rule['lhs']['sem']==str(slots[0][1][0]))]\n",
    "                        if len(word_rules) > 0:\n",
    "                            selected_rule = self.str2dict(str(random.sample(word_rules, 1)[0]))\n",
    "                            if slot_position == 0:\n",
    "                                new_sentence_with_slot = selected_rule['rhs'] + sentence_with_slot[1:]\n",
    "                            if slot_position == 1:\n",
    "                                new_sentence_with_slot = sentence_with_slot[0] + selected_rule['rhs'] + sentence_with_slot[2]\n",
    "                            if slot_position == 2:\n",
    "                                new_sentence_with_slot = sentence_with_slot[:2] + selected_rule['rhs']\n",
    "                        if len(word_rules) == 0:\n",
    "                            invented_form = self.invent_wordrule(slot_sem)\n",
    "                            if slot_position == 0:\n",
    "                                new_sentence_with_slot = invented_form + sentence_with_slot[1:]\n",
    "                            if slot_position == 1:\n",
    "                                new_sentence_with_slot = sentence_with_slot[0] + invented_form + sentence_with_slot[2]\n",
    "                            if slot_position == 2:\n",
    "                                new_sentence_with_slot = sentence_with_slot[:2] + invented_form\n",
    "                        slot_position = slots[1][0]\n",
    "                        slot_category = new_sentence_with_slot[slot_position]\n",
    "                        word_rules = [rule for rule in rules if (rule['lhs']['cat']==slot_category) & (rule['lhs']['sem']==str(slots[1][1][0]))]\n",
    "                        if len(word_rules) > 0:\n",
    "                            selected_rule = self.str2dict(str(random.sample(word_rules, 1)[0]))\n",
    "                            if slot_position == 0:\n",
    "                                generated_sentence = selected_rule['rhs'] + sentence_with_slot[1:]\n",
    "                            if slot_position == 1:\n",
    "                                generated_sentence = sentence_with_slot[0] + selected_rule['rhs'] + sentence_with_slot[2]\n",
    "                            if slot_position == 2:\n",
    "                                generated_sentence = sentence_with_slot[:2] + selected_rule['rhs']\n",
    "                            return f\"S/{str(query)} -> {generated_sentence}\"\n",
    "                        if len(word_rules) == 0:\n",
    "                            generated_sentence = self.invent_holisticrule(query)\n",
    "                            return f\"S/{str(query)} -> {generated_sentence}\"\n",
    "                    if (len(matches)==0) & (len(slots)==3):\n",
    "                        sentence_with_slot = sentence_list[i]\n",
    "                        slot_position = slots[0][0]\n",
    "                        slot_sem = slots[0][1][0]\n",
    "                        slot_category = sentence_with_slot[slot_position]\n",
    "                        word_rules = [rule for rule in rules if (rule['lhs']['cat']==slot_category) & (rule['lhs']['sem']==str(slots[0][1][0]))]\n",
    "                        if len(word_rules) > 0:\n",
    "                            selected_rule = self.str2dict(str(random.sample(word_rules, 1)[0]))\n",
    "                            if slot_position == 0:\n",
    "                                new_sentence_with_slot = selected_rule['rhs'] + sentence_with_slot[1:]\n",
    "                            if slot_position == 1:\n",
    "                                new_sentence_with_slot = sentence_with_slot[0] + selected_rule['rhs'] + sentence_with_slot[2]\n",
    "                            if slot_position == 2:\n",
    "                                new_sentence_with_slot = sentence_with_slot[:2] + selected_rule['rhs']\n",
    "                        if len(word_rules) == 0:\n",
    "                            invented_form = self.invent_wordrule(slot_sem)\n",
    "                            if slot_position == 0:\n",
    "                                new_sentence_with_slot = invented_form + sentence_with_slot[1:]\n",
    "                            if slot_position == 1:\n",
    "                                new_sentence_with_slot = sentence_with_slot[0] + invented_form + sentence_with_slot[2]\n",
    "                            if slot_position == 2:\n",
    "                                new_sentence_with_slot = sentence_with_slot[:2] + invented_form\n",
    "                        slot_position = slots[1][0]\n",
    "                        slot_category = new_sentence_with_slot[slot_position]\n",
    "                        word_rules = [rule for rule in rules if (rule['lhs']['cat']==slot_category) & (rule['lhs']['sem']==str(slots[1][1][0]))]\n",
    "                        if len(word_rules) > 0:\n",
    "                            selected_rule = self.str2dict(str(random.sample(word_rules, 1)[0]))\n",
    "                            if slot_position == 0:\n",
    "                                new_sentence_with_slot = selected_rule['rhs'] + sentence_with_slot[1:]\n",
    "                            if slot_position == 1:\n",
    "                                new_sentence_with_slot = sentence_with_slot[0] + selected_rule['rhs'] + sentence_with_slot[2]\n",
    "                            if slot_position == 2:\n",
    "                                new_sentence_with_slot = sentence_with_slot[:2] + selected_rule['rhs']\n",
    "                        slot_position = slots[2][0]\n",
    "                        slot_category = new_sentence_with_slot[slot_position]\n",
    "                        word_rules = [rule for rule in rules if (rule['lhs']['cat']==slot_category) & (rule['lhs']['sem']==str(slots[2][1][0]))]\n",
    "                        if len(word_rules) > 0:\n",
    "                            selected_rule = self.str2dict(str(random.sample(word_rules, 1)[0]))\n",
    "                            if slot_position == 0:\n",
    "                                generated_sentence = selected_rule['rhs'] + sentence_with_slot[1:]\n",
    "                            if slot_position == 1:\n",
    "                                generated_sentence = sentence_with_slot[0] + selected_rule['rhs'] + sentence_with_slot[2]\n",
    "                            if slot_position == 2:\n",
    "                                generated_sentence = sentence_with_slot[:2] + selected_rule['rhs']\n",
    "                            return f\"S/{str(query)} -> {generated_sentence}\"\n",
    "                        if len(word_rules) == 0:\n",
    "                            generated_sentence = self.invent_holisticrule(query)\n",
    "                            return f\"S/{str(query)} -> {generated_sentence}\"\n",
    "                    else:\n",
    "                        generated_sentence = self.invent_holisticrule(query)\n",
    "                        return f\"S/{str(query)} -> {generated_sentence}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grammar = Grammar()\n",
    "grammar.from_string(\"S/_admire(_alice,_carol) -> afc\\nS/_admire(_john,_carol) -> bfc\\nS/_know(_carol,_john) -> cgb\\nR/_know -> g\\nA/_john -> b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(grammar.rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S/_admire(_alice,_carol) -> afc\n",
      "S/_admire(_john,_carol) -> bfc\n",
      "S/_know(_carol,_john) -> cgb\n",
      "R/_know -> g\n",
      "A/_john -> b\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(grammar.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grammar.chunk01()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S/_know(_carol,_john) -> cgb\n",
      "S/_admire(x,_carol) -> Yfc\n",
      "Y/_alice -> a\n",
      "Y/_john -> b\n",
      "A/_john -> b\n",
      "R/_know -> g\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(grammar.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grammar.merge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S/_know(_carol,_john) -> cgb\n",
      "S/_admire(x,_carol) -> Yfc\n",
      "Y/_alice -> a\n",
      "Y/_john -> b\n",
      "R/_know -> g\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(grammar.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grammar.chunk02()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S/_know(_carol,_john) -> cgb\n",
      "S/_admire(x,_carol) -> Yfc\n",
      "Y/_alice -> a\n",
      "Y/_john -> b\n",
      "R/_know -> g\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(grammar.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grammar.replace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S/_admire(x,_carol) -> Yfc\n",
      "Y/_know(_carol,x) -> cgY\n",
      "R/X(_carol,_john) -> cRb\n",
      "Y/_alice -> a\n",
      "Y/_john -> b\n",
      "R/_know -> g\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(grammar.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grammar.replace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S/_admire(x,_carol) -> Yfc\n",
      "R/X(_carol,x) -> cRY\n",
      "Y/X(_carol,x) -> cRY\n",
      "Y/_alice -> a\n",
      "Y/_john -> b\n",
      "R/_know -> g\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(grammar.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grammar.merge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S/_admire(x,_carol) -> Yfc\n",
      "R/X(_carol,x) -> cRY\n",
      "Y/X(_carol,x) -> cRY\n",
      "Y/_alice -> a\n",
      "Y/_john -> b\n",
      "R/_know -> g\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(grammar.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S/_know(_carol,_john) -> cRb'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grammar.generate(\"_know(_carol,_john)\") #cga "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S/_admire(_alice,_carol) -> aεb'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grammar.generate(\"_admire(_alice,_carol)\") # afc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S/_like(_alice,_carol) -> eαc'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grammar.generate(\"_like(_alice,_carol)\") # ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = Grammar()\n",
    "grammar.from_string(generate_holistic_rules(100))\n",
    "utterances = grammar.to_string().split(\"\\n\") [:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S/_admire(_alice,_carol) -> bαc',\n",
       " 'S/_admire(_alice,_john) -> aαe',\n",
       " 'S/_admire(_john,_carol) -> aαb',\n",
       " 'S/_admire(_carol,_eve) -> bεd',\n",
       " 'S/_admire(_alice,_eve) -> cβb',\n",
       " 'S/_know(_carol,_alice) -> bγa',\n",
       " 'S/_admire(_eve,_carol) -> cγb',\n",
       " 'S/_admire(_eve,_alice) -> aβe',\n",
       " 'S/_admire(_bob,_carol) -> bδe',\n",
       " 'S/_kick(_alice,_carol) -> dδc']"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utterances[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPERATIONS = [\"chunk01\", \"chunk02\", \"merge\", \"replace\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]100%|██████████| 200/200 [22:45<00:00,  6.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S/_admire(_carol,_alice) -> bβd\n",
      "S/_admire(_john,_alice) -> eδd\n",
      "S/_admire(_carol,_john) -> aβb\n",
      "S/_kick(_alice,_carol) -> aαe\n",
      "S/_know(_carol,_alice) -> eεc\n",
      "S/_admire(_bob,_alice) -> aαb\n",
      "S/_admire(_alice,_bob) -> eεb\n",
      "S/_admire(_alice,_eve) -> eδd\n",
      "S/_know(_carol,_john) -> bγe\n",
      "S/_like(_john,_carol) -> bβd\n",
      "S/_admire(_bob,_john) -> eδb\n",
      "S/_know(_john,_carol) -> cεe\n",
      "S/_kick(_john,_carol) -> bδc\n",
      "S/_like(_carol,_john) -> bαa\n",
      "S/_kick(_john,_alice) -> cαa\n",
      "S/_like(_eve,_alice) -> cαe\n",
      "S/_meet(_alice,_bob) -> dβb\n",
      "S/_know(_carol,_eve) -> eαa\n",
      "S/_know(_alice,_eve) -> aαd\n",
      "S/_kick(_alice,_eve) -> cεd\n",
      "S/_meet(_eve,_carol) -> dεe\n",
      "S/_know(_bob,_carol) -> eεc\n",
      "S/_kick(_eve,_alice) -> dβa\n",
      "S/_like(_alice,_bob) -> dγa\n",
      "S/_admire(_bob,_eve) -> cεd\n",
      "S/_know(_bob,_alice) -> eδb\n",
      "S/_like(_carol,_eve) -> cαb\n",
      "S/_kick(_eve,_carol) -> eαc\n",
      "S/_kick(_alice,_bob) -> bγc\n",
      "S/_meet(_bob,_alice) -> cεb\n",
      "S/_like(_carol,_bob) -> cαb\n",
      "S/_know(_eve,_alice) -> eγc\n",
      "S/_meet(_eve,_alice) -> dβa\n",
      "S/_kick(_john,_eve) -> dαe\n",
      "S/_like(_john,_bob) -> dαc\n",
      "S/_kick(_bob,_john) -> aβb\n",
      "S/_like(_bob,_john) -> eαd\n",
      "S/_meet(_eve,_john) -> cαd\n",
      "S/_know(_eve,_bob) -> dαb\n",
      "S/_know(_bob,_eve) -> eδb\n",
      "S/_like(_bob,_eve) -> dεe\n",
      "S/_kick(_eve,_bob) -> dγa\n",
      "S/_kick(_bob,_eve) -> bβc\n",
      "S/X(_alice,_john) -> bNe\n",
      "S/_admire(_eve,x) -> eαV\n",
      "S/_meet(_john,x) -> eαC\n",
      "S/X(_john,_eve) -> dRe\n",
      "S/_meet(x,_eve) -> Rαb\n",
      "N/_admire -> γ\n",
      "R/_admire -> β\n",
      "V/_carol -> a\n",
      "C/_carol -> b\n",
      "R/_carol -> d\n",
      "R/_like -> β\n",
      "R/_john -> e\n",
      "R/_kick -> α\n",
      "N/_meet -> β\n",
      "V/_john -> c\n",
      "R/_know -> ε\n",
      "V/_bob -> c\n",
      "C/_bob -> d\n",
      "C/_eve -> b\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "grammar = Grammar()\n",
    "grammar.from_string(generate_holistic_rules(100))\n",
    "utterances = grammar.to_string().split(\"\\n\") [:-2]\n",
    "\n",
    "N_GENS = 200\n",
    "\n",
    "# N_GENS世代継承\n",
    "# TODO: 各世代のlog\n",
    "# TODO: 全クエリ生成させて、表現度を計算・ボトルネックをサンプリング\n",
    "for i in tqdm(range(N_GENS)):\n",
    "    last_generation = Grammar()\n",
    "    for utterance in utterances:\n",
    "        last_generation.add_rule(utterance)\n",
    "        operation = random.sample(OPERATIONS, 1)[0]\n",
    "        # print(operation)\n",
    "        # print(\"Before: \", last_generation.to_string())\n",
    "        if len(last_generation.rules) > 1:\n",
    "            if operation == \"chunk01\":\n",
    "                last_generation.chunk01()\n",
    "            if operation == \"chunk02\":\n",
    "                last_generation.chunk02()\n",
    "            if operation == \"merge\":\n",
    "                last_generation.merge()\n",
    "            if operation == \"replace\":\n",
    "                last_generation.chunk01()\n",
    "        # print(\"After: \", last_generation.to_string())\n",
    "\n",
    "    utterances = []\n",
    "\n",
    "    if i != (N_GENS - 1):\n",
    "        queries = generate_queries(100)\n",
    "        for query in queries:\n",
    "            utterance = last_generation.generate(query)\n",
    "            utterances.append(utterance)\n",
    "        utterances = random.sample(utterances, 80)\n",
    "\n",
    "print(last_generation.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_generation.chunk01()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S/_admire(_john,_carol) -> aβc\n",
      "S/_admire(_eve,_carol) -> bαa\n",
      "S/_meet(_carol,_alice) -> dγb\n",
      "S/_like(_alice,_carol) -> bβd\n",
      "S/_admire(_bob,_alice) -> bαa\n",
      "S/_admire(_eve,_alice) -> bβd\n",
      "S/_admire(_carol,_bob) -> aγc\n",
      "S/_admire(_john,_eve) -> dβe\n",
      "S/_like(_alice,_john) -> aδe\n",
      "S/_know(_carol,_john) -> aεd\n",
      "S/_know(_john,_carol) -> cαd\n",
      "S/_admire(_eve,_john) -> dγb\n",
      "S/_kick(_john,_alice) -> aδc\n",
      "S/_kick(_carol,_john) -> bδa\n",
      "S/_kick(_alice,_john) -> bδd\n",
      "S/_admire(_john,_bob) -> eβa\n",
      "S/_know(_bob,_carol) -> aεb\n",
      "S/_kick(_bob,_carol) -> cβd\n",
      "S/_meet(_bob,_carol) -> aδe\n",
      "S/_meet(_eve,_carol) -> aεb\n",
      "S/_know(_carol,_eve) -> aγb\n",
      "S/_know(_bob,_alice) -> cδb\n",
      "S/_kick(_bob,_alice) -> aγb\n",
      "S/_like(_bob,_alice) -> dδc\n",
      "S/_kick(_carol,_eve) -> eεb\n",
      "S/_know(_eve,_alice) -> eεb\n",
      "S/_admire(_eve,_bob) -> dβb\n",
      "S/_like(_eve,_alice) -> eβd\n",
      "S/_kick(_carol,_bob) -> bγd\n",
      "S/_meet(_alice,_eve) -> eεa\n",
      "S/_admire(_bob,_eve) -> aγc\n",
      "S/_like(_john,_bob) -> eγc\n",
      "S/_kick(_bob,_john) -> eαa\n",
      "S/_meet(_eve,_john) -> cγe\n",
      "S/_like(_eve,_john) -> cγa\n",
      "S/_know(_john,_eve) -> eδb\n",
      "S/_like(_john,_eve) -> cαe\n",
      "S/_meet(_bob,_john) -> eεc\n",
      "S/_like(_bob,_john) -> aδb\n",
      "S/_meet(_bob,_eve) -> eαc\n",
      "S/_kick(_bob,_eve) -> bβa\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(last_generation.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = Grammar()\n",
    "grammar.from_string(generate_holistic_rules(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar.chunk01()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S/_admire(x,_carol) -> Fβa\n",
      "S/_admire(_alice,x) -> eγX\n",
      "S/_admire(x,_alice) -> Oβd\n",
      "S/_admire(x,_carol) -> Kγd\n",
      "S/_admire(_alice,x) -> eεR\n",
      "S/_admire(x,_carol) -> Nγb\n",
      "S/_admire(x,_carol) -> Cβa\n",
      "S/_admire(_alice,x) -> eγQ\n",
      "S/_admire(_alice,x) -> eεM\n",
      "S/X(_carol,_alice) -> eZa\n",
      "S/X(_alice,_carol) -> cFd\n",
      "S/_know(_alice,x) -> eγL\n",
      "S/X(_carol,_john) -> eQa\n",
      "S/X(_john,_carol) -> bIa\n",
      "S/_like(_carol,x) -> dγK\n",
      "S/X(_carol,_john) -> eDa\n",
      "S/_admire(_bob,x) -> eγU\n",
      "S/_know(x,_carol) -> Bεa\n",
      "S/_admire(x,_eve) -> Bεa\n",
      "S/_kick(x,_carol) -> Vεe\n",
      "S/_kick(_alice,x) -> eβN\n",
      "S/X(_carol,_john) -> eTa\n",
      "S/X(_alice,_john) -> aHc\n",
      "S/_know(_alice,x) -> eγQ\n",
      "S/_kick(x,_carol) -> Nεe\n",
      "S/X(_alice,_eve) -> eOa\n",
      "S/X(_alice,_eve) -> eKa\n",
      "S/_know(_john,x) -> bεA\n",
      "S/X(_eve,_carol) -> cMe\n",
      "S/X(_alice,_bob) -> eCb\n",
      "S/X(_alice,_eve) -> eGa\n",
      "S/_know(x,_john) -> Fαa\n",
      "S/_know(_john,x) -> bεZ\n",
      "S/X(_eve,_alice) -> bNa\n",
      "S/X(_carol,_eve) -> dEc\n",
      "S/X(_alice,_eve) -> eWa\n",
      "S/X(_alice,_bob) -> eQb\n",
      "S/X(_eve,_alice) -> bPa\n",
      "S/X(_alice,_eve) -> eYa\n",
      "S/_like(x,_eve) -> Bγc\n",
      "S/_know(x,_bob) -> Tγb\n",
      "S/_know(x,_eve) -> Tβa\n",
      "S/_like(x,_bob) -> Rαc\n",
      "S/_kick(_eve,x) -> bβC\n",
      "S/_kick(_eve,x) -> cεK\n",
      "S/_like(x,_bob) -> Bδe\n",
      "F/_admire -> ε\n",
      "H/_admire -> α\n",
      "M/_admire -> δ\n",
      "P/_admire -> δ\n",
      "K/_admire -> ε\n",
      "Y/_admire -> ε\n",
      "N/_admire -> δ\n",
      "C/_admire -> γ\n",
      "O/_admire -> ε\n",
      "B/_alice -> e\n",
      "N/_carol -> a\n",
      "T/_carol -> d\n",
      "Q/_carol -> d\n",
      "Q/_carol -> b\n",
      "B/_carol -> d\n",
      "K/_carol -> e\n",
      "O/_carol -> c\n",
      "A/_alice -> e\n",
      "N/_carol -> d\n",
      "R/_alice -> a\n",
      "F/_carol -> c\n",
      "U/_carol -> d\n",
      "X/_carol -> b\n",
      "X/_carol -> d\n",
      "K/_alice -> b\n",
      "C/_alice -> c\n",
      "T/_alice -> e\n",
      "F/_carol -> e\n",
      "B/_alice -> c\n",
      "A/_carol -> a\n",
      "N/_alice -> e\n",
      "Z/_carol -> a\n",
      "K/_carol -> d\n",
      "B/_carol -> e\n",
      "B/_alice -> a\n",
      "Z/_carol -> e\n",
      "M/_kick -> ε\n",
      "B/_john -> b\n",
      "T/_know -> α\n",
      "N/_know -> β\n",
      "R/_john -> c\n",
      "I/_know -> β\n",
      "C/_john -> d\n",
      "E/_like -> γ\n",
      "V/_john -> b\n",
      "R/_john -> d\n",
      "K/_kick -> β\n",
      "W/_kick -> β\n",
      "H/_know -> γ\n",
      "D/_know -> α\n",
      "O/_john -> a\n",
      "Z/_kick -> β\n",
      "D/_meet -> ε\n",
      "Q/_know -> γ\n",
      "Y/_know -> α\n",
      "Q/_kick -> β\n",
      "W/_kick -> γ\n",
      "P/_know -> ε\n",
      "F/_meet -> β\n",
      "G/_meet -> ε\n",
      "Z/_meet -> ε\n",
      "T/_kick -> β\n",
      "E/_like -> α\n",
      "O/_kick -> γ\n",
      "Q/_know -> α\n",
      "G/_kick -> γ\n",
      "M/_john -> c\n",
      "Q/_kick -> γ\n",
      "I/_know -> ε\n",
      "F/_john -> d\n",
      "C/_kick -> β\n",
      "Q/_eve -> a\n",
      "Q/_bob -> b\n",
      "C/_eve -> e\n",
      "L/_eve -> a\n",
      "N/_eve -> b\n",
      "F/_bob -> b\n",
      "U/_eve -> a\n",
      "K/_eve -> c\n",
      "M/_eve -> a\n",
      "T/_bob -> b\n",
      "T/_eve -> e\n",
      "K/_bob -> b\n",
      "V/_eve -> c\n",
      "C/_eve -> a\n",
      "K/_bob -> e\n",
      "L/_bob -> b\n",
      "N/_bob -> b\n",
      "N/_eve -> c\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(grammar.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can merge {'lhs': {'cat': 'Q', 'sem': '_eve'}, 'rhs': 'a'} and {'lhs': {'cat': 'L', 'sem': '_eve'}, 'rhs': 'a'}\n",
      "replacee_rule:  {'lhs': {'cat': 'L', 'sem': '_eve'}, 'rhs': 'a'}\n",
      "replacer_rule:  {'lhs': {'cat': 'Q', 'sem': '_eve'}, 'rhs': 'a'}\n",
      "can merge {'lhs': {'cat': 'Q', 'sem': '_eve'}, 'rhs': 'a'} and {'lhs': {'cat': 'U', 'sem': '_eve'}, 'rhs': 'a'}\n",
      "replacee_rule:  {'lhs': {'cat': 'U', 'sem': '_eve'}, 'rhs': 'a'}\n",
      "replacer_rule:  {'lhs': {'cat': 'Q', 'sem': '_eve'}, 'rhs': 'a'}\n",
      "can merge {'lhs': {'cat': 'Q', 'sem': '_eve'}, 'rhs': 'a'} and {'lhs': {'cat': 'M', 'sem': '_eve'}, 'rhs': 'a'}\n",
      "replacee_rule:  {'lhs': {'cat': 'M', 'sem': '_eve'}, 'rhs': 'a'}\n",
      "replacer_rule:  {'lhs': {'cat': 'Q', 'sem': '_eve'}, 'rhs': 'a'}\n",
      "can merge {'lhs': {'cat': 'Q', 'sem': '_eve'}, 'rhs': 'a'} and {'lhs': {'cat': 'C', 'sem': '_eve'}, 'rhs': 'a'}\n",
      "replacee_rule:  {'lhs': {'cat': 'C', 'sem': '_eve'}, 'rhs': 'a'}\n",
      "replacer_rule:  {'lhs': {'cat': 'Q', 'sem': '_eve'}, 'rhs': 'a'}\n",
      "can merge {'lhs': {'cat': 'B', 'sem': '_john'}, 'rhs': 'b'} and {'lhs': {'cat': 'V', 'sem': '_john'}, 'rhs': 'b'}\n",
      "replacee_rule:  {'lhs': {'cat': 'V', 'sem': '_john'}, 'rhs': 'b'}\n",
      "replacer_rule:  {'lhs': {'cat': 'B', 'sem': '_john'}, 'rhs': 'b'}\n",
      "can merge {'lhs': {'cat': 'B', 'sem': '_alice'}, 'rhs': 'e'} and {'lhs': {'cat': 'A', 'sem': '_alice'}, 'rhs': 'e'}\n",
      "replacee_rule:  {'lhs': {'cat': 'A', 'sem': '_alice'}, 'rhs': 'e'}\n",
      "replacer_rule:  {'lhs': {'cat': 'B', 'sem': '_alice'}, 'rhs': 'e'}\n",
      "can merge {'lhs': {'cat': 'B', 'sem': '_alice'}, 'rhs': 'e'} and {'lhs': {'cat': 'T', 'sem': '_alice'}, 'rhs': 'e'}\n",
      "replacee_rule:  {'lhs': {'cat': 'T', 'sem': '_alice'}, 'rhs': 'e'}\n",
      "replacer_rule:  {'lhs': {'cat': 'B', 'sem': '_alice'}, 'rhs': 'e'}\n",
      "can merge {'lhs': {'cat': 'B', 'sem': '_alice'}, 'rhs': 'e'} and {'lhs': {'cat': 'N', 'sem': '_alice'}, 'rhs': 'e'}\n",
      "replacee_rule:  {'lhs': {'cat': 'N', 'sem': '_alice'}, 'rhs': 'e'}\n",
      "replacer_rule:  {'lhs': {'cat': 'B', 'sem': '_alice'}, 'rhs': 'e'}\n",
      "can merge {'lhs': {'cat': 'B', 'sem': '_carol'}, 'rhs': 'a'} and {'lhs': {'cat': 'Z', 'sem': '_carol'}, 'rhs': 'a'}\n",
      "replacee_rule:  {'lhs': {'cat': 'Z', 'sem': '_carol'}, 'rhs': 'a'}\n",
      "replacer_rule:  {'lhs': {'cat': 'B', 'sem': '_carol'}, 'rhs': 'a'}\n",
      "can merge {'lhs': {'cat': 'Q', 'sem': '_bob'}, 'rhs': 'b'} and {'lhs': {'cat': 'F', 'sem': '_bob'}, 'rhs': 'b'}\n",
      "replacee_rule:  {'lhs': {'cat': 'F', 'sem': '_bob'}, 'rhs': 'b'}\n",
      "replacer_rule:  {'lhs': {'cat': 'Q', 'sem': '_bob'}, 'rhs': 'b'}\n",
      "can merge {'lhs': {'cat': 'Q', 'sem': '_bob'}, 'rhs': 'b'} and {'lhs': {'cat': 'B', 'sem': '_bob'}, 'rhs': 'b'}\n",
      "can merge {'lhs': {'cat': 'Q', 'sem': '_bob'}, 'rhs': 'b'} and {'lhs': {'cat': 'K', 'sem': '_bob'}, 'rhs': 'b'}\n",
      "replacee_rule:  {'lhs': {'cat': 'K', 'sem': '_bob'}, 'rhs': 'b'}\n",
      "replacer_rule:  {'lhs': {'cat': 'Q', 'sem': '_bob'}, 'rhs': 'b'}\n",
      "can merge {'lhs': {'cat': 'Q', 'sem': '_bob'}, 'rhs': 'b'} and {'lhs': {'cat': 'B', 'sem': '_bob'}, 'rhs': 'b'}\n",
      "can merge {'lhs': {'cat': 'B', 'sem': '_know'}, 'rhs': 'α'} and {'lhs': {'cat': 'D', 'sem': '_know'}, 'rhs': 'α'}\n",
      "replacee_rule:  {'lhs': {'cat': 'D', 'sem': '_know'}, 'rhs': 'α'}\n",
      "replacer_rule:  {'lhs': {'cat': 'B', 'sem': '_know'}, 'rhs': 'α'}\n",
      "can merge {'lhs': {'cat': 'B', 'sem': '_know'}, 'rhs': 'α'} and {'lhs': {'cat': 'Y', 'sem': '_know'}, 'rhs': 'α'}\n",
      "replacee_rule:  {'lhs': {'cat': 'Y', 'sem': '_know'}, 'rhs': 'α'}\n",
      "replacer_rule:  {'lhs': {'cat': 'B', 'sem': '_know'}, 'rhs': 'α'}\n",
      "can merge {'lhs': {'cat': 'B', 'sem': '_know'}, 'rhs': 'α'} and {'lhs': {'cat': 'Q', 'sem': '_know'}, 'rhs': 'α'}\n",
      "can merge {'lhs': {'cat': 'Q', 'sem': '_admire'}, 'rhs': 'ε'} and {'lhs': {'cat': 'B', 'sem': '_admire'}, 'rhs': 'ε'}\n",
      "can merge {'lhs': {'cat': 'Q', 'sem': '_admire'}, 'rhs': 'ε'} and {'lhs': {'cat': 'O', 'sem': '_admire'}, 'rhs': 'ε'}\n",
      "replacee_rule:  {'lhs': {'cat': 'O', 'sem': '_admire'}, 'rhs': 'ε'}\n",
      "replacer_rule:  {'lhs': {'cat': 'Q', 'sem': '_admire'}, 'rhs': 'ε'}\n",
      "can merge {'lhs': {'cat': 'B', 'sem': '_carol'}, 'rhs': 'd'} and {'lhs': {'cat': 'Q', 'sem': '_carol'}, 'rhs': 'd'}\n",
      "can merge {'lhs': {'cat': 'B', 'sem': '_carol'}, 'rhs': 'd'} and {'lhs': {'cat': 'Q', 'sem': '_carol'}, 'rhs': 'd'}\n",
      "can merge {'lhs': {'cat': 'B', 'sem': '_carol'}, 'rhs': 'd'} and {'lhs': {'cat': 'X', 'sem': '_carol'}, 'rhs': 'd'}\n",
      "replacee_rule:  {'lhs': {'cat': 'X', 'sem': '_carol'}, 'rhs': 'd'}\n",
      "replacer_rule:  {'lhs': {'cat': 'B', 'sem': '_carol'}, 'rhs': 'd'}\n",
      "can merge {'lhs': {'cat': 'B', 'sem': '_carol'}, 'rhs': 'd'} and {'lhs': {'cat': 'Q', 'sem': '_carol'}, 'rhs': 'd'}\n",
      "can merge {'lhs': {'cat': 'B', 'sem': '_know'}, 'rhs': 'β'} and {'lhs': {'cat': 'I', 'sem': '_know'}, 'rhs': 'β'}\n",
      "replacee_rule:  {'lhs': {'cat': 'I', 'sem': '_know'}, 'rhs': 'β'}\n",
      "replacer_rule:  {'lhs': {'cat': 'B', 'sem': '_know'}, 'rhs': 'β'}\n",
      "can merge {'lhs': {'cat': 'R', 'sem': '_john'}, 'rhs': 'c'} and {'lhs': {'cat': 'Q', 'sem': '_john'}, 'rhs': 'c'}\n",
      "replacee_rule:  {'lhs': {'cat': 'R', 'sem': '_john'}, 'rhs': 'c'}\n",
      "replacer_rule:  {'lhs': {'cat': 'Q', 'sem': '_john'}, 'rhs': 'c'}\n",
      "can merge {'lhs': {'cat': 'Q', 'sem': '_eve'}, 'rhs': 'e'} and {'lhs': {'cat': 'B', 'sem': '_eve'}, 'rhs': 'e'}\n",
      "can merge {'lhs': {'cat': 'Q', 'sem': '_carol'}, 'rhs': 'd'} and {'lhs': {'cat': 'B', 'sem': '_carol'}, 'rhs': 'd'}\n",
      "can merge {'lhs': {'cat': 'Q', 'sem': '_carol'}, 'rhs': 'd'} and {'lhs': {'cat': 'B', 'sem': '_carol'}, 'rhs': 'd'}\n",
      "can merge {'lhs': {'cat': 'Q', 'sem': '_carol'}, 'rhs': 'd'} and {'lhs': {'cat': 'B', 'sem': '_carol'}, 'rhs': 'd'}\n",
      "can merge {'lhs': {'cat': 'Q', 'sem': '_carol'}, 'rhs': 'b'} and {'lhs': {'cat': 'B', 'sem': '_carol'}, 'rhs': 'b'}\n",
      "can merge {'lhs': {'cat': 'B', 'sem': '_carol'}, 'rhs': 'd'} and {'lhs': {'cat': 'Q', 'sem': '_carol'}, 'rhs': 'd'}\n",
      "can merge {'lhs': {'cat': 'B', 'sem': '_carol'}, 'rhs': 'd'} and {'lhs': {'cat': 'Q', 'sem': '_carol'}, 'rhs': 'd'}\n",
      "can merge {'lhs': {'cat': 'Q', 'sem': '_carol'}, 'rhs': 'e'} and {'lhs': {'cat': 'B', 'sem': '_carol'}, 'rhs': 'e'}\n",
      "can merge {'lhs': {'cat': 'Q', 'sem': '_carol'}, 'rhs': 'e'} and {'lhs': {'cat': 'B', 'sem': '_carol'}, 'rhs': 'e'}\n",
      "can merge {'lhs': {'cat': 'Q', 'sem': '_bob'}, 'rhs': 'b'} and {'lhs': {'cat': 'B', 'sem': '_bob'}, 'rhs': 'b'}\n",
      "can merge {'lhs': {'cat': 'Q', 'sem': '_bob'}, 'rhs': 'b'} and {'lhs': {'cat': 'B', 'sem': '_bob'}, 'rhs': 'b'}\n",
      "can merge {'lhs': {'cat': 'B', 'sem': '_carol'}, 'rhs': 'd'} and {'lhs': {'cat': 'Q', 'sem': '_carol'}, 'rhs': 'd'}\n",
      "can merge {'lhs': {'cat': 'B', 'sem': '_carol'}, 'rhs': 'd'} and {'lhs': {'cat': 'Q', 'sem': '_carol'}, 'rhs': 'd'}\n",
      "can merge {'lhs': {'cat': 'Q', 'sem': '_alice'}, 'rhs': 'a'} and {'lhs': {'cat': 'B', 'sem': '_alice'}, 'rhs': 'a'}\n",
      "can merge {'lhs': {'cat': 'Q', 'sem': '_admire'}, 'rhs': 'δ'} and {'lhs': {'cat': 'P', 'sem': '_admire'}, 'rhs': 'δ'}\n",
      "replacee_rule:  {'lhs': {'cat': 'P', 'sem': '_admire'}, 'rhs': 'δ'}\n",
      "replacer_rule:  {'lhs': {'cat': 'Q', 'sem': '_admire'}, 'rhs': 'δ'}\n",
      "can merge {'lhs': {'cat': 'Q', 'sem': '_admire'}, 'rhs': 'δ'} and {'lhs': {'cat': 'B', 'sem': '_admire'}, 'rhs': 'δ'}\n",
      "can merge {'lhs': {'cat': 'Q', 'sem': '_eve'}, 'rhs': 'c'} and {'lhs': {'cat': 'B', 'sem': '_eve'}, 'rhs': 'c'}\n",
      "can merge {'lhs': {'cat': 'Q', 'sem': '_eve'}, 'rhs': 'c'} and {'lhs': {'cat': 'B', 'sem': '_eve'}, 'rhs': 'c'}\n",
      "can merge {'lhs': {'cat': 'Q', 'sem': '_kick'}, 'rhs': 'β'} and {'lhs': {'cat': 'W', 'sem': '_kick'}, 'rhs': 'β'}\n",
      "replacee_rule:  {'lhs': {'cat': 'W', 'sem': '_kick'}, 'rhs': 'β'}\n",
      "replacer_rule:  {'lhs': {'cat': 'Q', 'sem': '_kick'}, 'rhs': 'β'}\n",
      "can merge {'lhs': {'cat': 'Q', 'sem': '_kick'}, 'rhs': 'β'} and {'lhs': {'cat': 'B', 'sem': '_kick'}, 'rhs': 'β'}\n",
      "can merge {'lhs': {'cat': 'Q', 'sem': '_kick'}, 'rhs': 'β'} and {'lhs': {'cat': 'B', 'sem': '_kick'}, 'rhs': 'β'}\n",
      "can merge {'lhs': {'cat': 'Q', 'sem': '_kick'}, 'rhs': 'β'} and {'lhs': {'cat': 'B', 'sem': '_kick'}, 'rhs': 'β'}\n",
      "can merge {'lhs': {'cat': 'Q', 'sem': '_kick'}, 'rhs': 'β'} and {'lhs': {'cat': 'B', 'sem': '_kick'}, 'rhs': 'β'}\n",
      "can merge {'lhs': {'cat': 'Q', 'sem': '_carol'}, 'rhs': 'd'} and {'lhs': {'cat': 'B', 'sem': '_carol'}, 'rhs': 'd'}\n",
      "can merge {'lhs': {'cat': 'H', 'sem': '_know'}, 'rhs': 'γ'} and {'lhs': {'cat': 'Q', 'sem': '_know'}, 'rhs': 'γ'}\n",
      "replacee_rule:  {'lhs': {'cat': 'H', 'sem': '_know'}, 'rhs': 'γ'}\n",
      "replacer_rule:  {'lhs': {'cat': 'Q', 'sem': '_know'}, 'rhs': 'γ'}\n",
      "can merge {'lhs': {'cat': 'B', 'sem': '_know'}, 'rhs': 'α'} and {'lhs': {'cat': 'Q', 'sem': '_know'}, 'rhs': 'α'}\n",
      "can merge {'lhs': {'cat': 'B', 'sem': '_kick'}, 'rhs': 'β'} and {'lhs': {'cat': 'Q', 'sem': '_kick'}, 'rhs': 'β'}\n",
      "can merge {'lhs': {'cat': 'B', 'sem': '_kick'}, 'rhs': 'β'} and {'lhs': {'cat': 'Q', 'sem': '_kick'}, 'rhs': 'β'}\n",
      "can merge {'lhs': {'cat': 'B', 'sem': '_meet'}, 'rhs': 'ε'} and {'lhs': {'cat': 'G', 'sem': '_meet'}, 'rhs': 'ε'}\n",
      "replacee_rule:  {'lhs': {'cat': 'G', 'sem': '_meet'}, 'rhs': 'ε'}\n",
      "replacer_rule:  {'lhs': {'cat': 'B', 'sem': '_meet'}, 'rhs': 'ε'}\n",
      "can merge {'lhs': {'cat': 'B', 'sem': '_bob'}, 'rhs': 'b'} and {'lhs': {'cat': 'Q', 'sem': '_bob'}, 'rhs': 'b'}\n",
      "can merge {'lhs': {'cat': 'B', 'sem': '_bob'}, 'rhs': 'b'} and {'lhs': {'cat': 'Q', 'sem': '_bob'}, 'rhs': 'b'}\n",
      "can merge {'lhs': {'cat': 'B', 'sem': '_carol'}, 'rhs': 'd'} and {'lhs': {'cat': 'Q', 'sem': '_carol'}, 'rhs': 'd'}\n",
      "can merge {'lhs': {'cat': 'Q', 'sem': '_alice'}, 'rhs': 'c'} and {'lhs': {'cat': 'B', 'sem': '_alice'}, 'rhs': 'c'}\n",
      "can merge {'lhs': {'cat': 'B', 'sem': '_know'}, 'rhs': 'α'} and {'lhs': {'cat': 'Q', 'sem': '_know'}, 'rhs': 'α'}\n",
      "can merge {'lhs': {'cat': 'Q', 'sem': '_kick'}, 'rhs': 'β'} and {'lhs': {'cat': 'B', 'sem': '_kick'}, 'rhs': 'β'}\n",
      "can merge {'lhs': {'cat': 'Q', 'sem': '_carol'}, 'rhs': 'e'} and {'lhs': {'cat': 'B', 'sem': '_carol'}, 'rhs': 'e'}\n",
      "can merge {'lhs': {'cat': 'Q', 'sem': '_carol'}, 'rhs': 'e'} and {'lhs': {'cat': 'B', 'sem': '_carol'}, 'rhs': 'e'}\n",
      "can merge {'lhs': {'cat': 'Q', 'sem': '_kick'}, 'rhs': 'γ'} and {'lhs': {'cat': 'B', 'sem': '_kick'}, 'rhs': 'γ'}\n",
      "can merge {'lhs': {'cat': 'Q', 'sem': '_know'}, 'rhs': 'ε'} and {'lhs': {'cat': 'B', 'sem': '_know'}, 'rhs': 'ε'}\n",
      "can merge {'lhs': {'cat': 'Q', 'sem': '_admire'}, 'rhs': 'δ'} and {'lhs': {'cat': 'B', 'sem': '_admire'}, 'rhs': 'δ'}\n",
      "can merge {'lhs': {'cat': 'Q', 'sem': '_bob'}, 'rhs': 'b'} and {'lhs': {'cat': 'B', 'sem': '_bob'}, 'rhs': 'b'}\n",
      "can merge {'lhs': {'cat': 'B', 'sem': '_kick'}, 'rhs': 'β'} and {'lhs': {'cat': 'Q', 'sem': '_kick'}, 'rhs': 'β'}\n",
      "can merge {'lhs': {'cat': 'Q', 'sem': '_admire'}, 'rhs': 'ε'} and {'lhs': {'cat': 'B', 'sem': '_admire'}, 'rhs': 'ε'}\n",
      "can merge {'lhs': {'cat': 'B', 'sem': '_admire'}, 'rhs': 'ε'} and {'lhs': {'cat': 'Q', 'sem': '_admire'}, 'rhs': 'ε'}\n",
      "can merge {'lhs': {'cat': 'Q', 'sem': '_kick'}, 'rhs': 'γ'} and {'lhs': {'cat': 'B', 'sem': '_kick'}, 'rhs': 'γ'}\n",
      "can merge {'lhs': {'cat': 'Q', 'sem': '_bob'}, 'rhs': 'b'} and {'lhs': {'cat': 'B', 'sem': '_bob'}, 'rhs': 'b'}\n",
      "can merge {'lhs': {'cat': 'B', 'sem': '_kick'}, 'rhs': 'γ'} and {'lhs': {'cat': 'Q', 'sem': '_kick'}, 'rhs': 'γ'}\n"
     ]
    }
   ],
   "source": [
    "grammar.merge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S/_admire(x,_carol) -> Bγb\n",
      "S/_admire(x,_carol) -> Qβa\n",
      "S/_admire(_alice,x) -> eγQ\n",
      "S/_admire(_alice,x) -> eεQ\n",
      "S/_admire(_alice,x) -> eγB\n",
      "S/_admire(x,_alice) -> Qβd\n",
      "S/_admire(x,_carol) -> Qγd\n",
      "S/X(_carol,_alice) -> eBa\n",
      "S/X(_alice,_carol) -> cQd\n",
      "S/X(_carol,_john) -> eQa\n",
      "S/_kick(x,_carol) -> Bεe\n",
      "S/X(_carol,_john) -> eBa\n",
      "S/_know(x,_carol) -> Bεa\n",
      "S/_admire(x,_eve) -> Bεa\n",
      "S/_like(_carol,x) -> dγQ\n",
      "S/_admire(_bob,x) -> eγQ\n",
      "S/X(_alice,_john) -> aQc\n",
      "S/_kick(_alice,x) -> eβB\n",
      "S/_know(_alice,x) -> eγQ\n",
      "S/X(_john,_carol) -> bBa\n",
      "S/_know(x,_john) -> Qαa\n",
      "S/X(_alice,_eve) -> eBa\n",
      "S/X(_eve,_alice) -> bBa\n",
      "S/_know(_john,x) -> bεB\n",
      "S/X(_eve,_alice) -> bQa\n",
      "S/X(_eve,_carol) -> cQe\n",
      "S/X(_alice,_eve) -> eQa\n",
      "S/X(_carol,_eve) -> dEc\n",
      "S/X(_alice,_bob) -> eQb\n",
      "S/_like(x,_eve) -> Bγc\n",
      "S/_know(x,_eve) -> Bβa\n",
      "S/_know(x,_bob) -> Bγb\n",
      "S/_kick(_eve,x) -> bβQ\n",
      "S/_like(x,_bob) -> Qαc\n",
      "S/_kick(_eve,x) -> cεQ\n",
      "S/_like(x,_bob) -> Bδe\n",
      "Q/_admire -> ε\n",
      "Q/_admire -> δ\n",
      "Q/_admire -> γ\n",
      "Q/_admire -> α\n",
      "B/_admire -> δ\n",
      "B/_admire -> ε\n",
      "Q/_alice -> c\n",
      "B/_alice -> e\n",
      "Q/_alice -> b\n",
      "B/_carol -> b\n",
      "Q/_carol -> d\n",
      "Q/_carol -> b\n",
      "B/_carol -> d\n",
      "Q/_carol -> c\n",
      "B/_alice -> c\n",
      "Q/_alice -> a\n",
      "Q/_carol -> e\n",
      "B/_carol -> e\n",
      "B/_alice -> a\n",
      "B/_carol -> a\n",
      "B/_know -> α\n",
      "B/_john -> b\n",
      "Q/_kick -> ε\n",
      "B/_kick -> γ\n",
      "B/_kick -> β\n",
      "Q/_john -> c\n",
      "E/_like -> γ\n",
      "Q/_john -> d\n",
      "B/_know -> ε\n",
      "B/_meet -> ε\n",
      "Q/_know -> γ\n",
      "Q/_kick -> β\n",
      "E/_like -> α\n",
      "Q/_know -> ε\n",
      "Q/_john -> a\n",
      "Q/_know -> α\n",
      "B/_know -> β\n",
      "Q/_kick -> γ\n",
      "Q/_meet -> β\n",
      "Q/_eve -> a\n",
      "Q/_bob -> b\n",
      "B/_eve -> e\n",
      "B/_eve -> c\n",
      "Q/_eve -> c\n",
      "B/_bob -> b\n",
      "Q/_eve -> e\n",
      "B/_eve -> b\n",
      "Q/_bob -> e\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(grammar.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "_5verbs_sem = [\"_kick\", \"_know\", \"_meet\", \"_like\", \"_admire\"]\n",
    "_5nouns_sem = [\"_john\", \"_alice\", \"_eve\", \"_carol\", \"_bob\"]\n",
    "_5nouns = [\"d\", \"a\", \"e\", \"c\", \"b\"]\n",
    "_5verbs = [\"α\", \"β\", \"γ\", \"δ\", \"ε\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate fully compositional utterances\n",
    "with open(\"semantic_space.txt\", \"w\") as f:\n",
    "    for verb, verb_sem in zip(_5verbs, _5verbs_sem):\n",
    "        for subj, subj_sem in zip(_5nouns, _5nouns_sem):\n",
    "            for obj, obj_sem in zip(_5nouns, _5nouns_sem):\n",
    "                if subj != obj:\n",
    "                    f.write(f\"S/{verb_sem}({subj_sem},{obj_sem}) -> {subj}{verb}{obj}\\n\")\n",
    "                else:\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whole semantic space / fully compositional utterances\n",
    "semantic_space = \"\"\"S/_kick(_john,_alice) -> dαa\n",
    "S/_kick(_john,_eve) -> dαe\n",
    "S/_kick(_john,_carol) -> dαc\n",
    "S/_kick(_john,_bob) -> dαb\n",
    "S/_kick(_alice,_john) -> aαd\n",
    "S/_kick(_alice,_eve) -> aαe\n",
    "S/_kick(_alice,_carol) -> aαc\n",
    "S/_kick(_alice,_bob) -> aαb\n",
    "S/_kick(_eve,_john) -> eαd\n",
    "S/_kick(_eve,_alice) -> eαa\n",
    "S/_kick(_eve,_carol) -> eαc\n",
    "S/_kick(_eve,_bob) -> eαb\n",
    "S/_kick(_carol,_john) -> cαd\n",
    "S/_kick(_carol,_alice) -> cαa\n",
    "S/_kick(_carol,_eve) -> cαe\n",
    "S/_kick(_carol,_bob) -> cαb\n",
    "S/_kick(_bob,_john) -> bαd\n",
    "S/_kick(_bob,_alice) -> bαa\n",
    "S/_kick(_bob,_eve) -> bαe\n",
    "S/_kick(_bob,_carol) -> bαc\n",
    "S/_know(_john,_alice) -> dβa\n",
    "S/_know(_john,_eve) -> dβe\n",
    "S/_know(_john,_carol) -> dβc\n",
    "S/_know(_john,_bob) -> dβb\n",
    "S/_know(_alice,_john) -> aβd\n",
    "S/_know(_alice,_eve) -> aβe\n",
    "S/_know(_alice,_carol) -> aβc\n",
    "S/_know(_alice,_bob) -> aβb\n",
    "S/_know(_eve,_john) -> eβd\n",
    "S/_know(_eve,_alice) -> eβa\n",
    "S/_know(_eve,_carol) -> eβc\n",
    "S/_know(_eve,_bob) -> eβb\n",
    "S/_know(_carol,_john) -> cβd\n",
    "S/_know(_carol,_alice) -> cβa\n",
    "S/_know(_carol,_eve) -> cβe\n",
    "S/_know(_carol,_bob) -> cβb\n",
    "S/_know(_bob,_john) -> bβd\n",
    "S/_know(_bob,_alice) -> bβa\n",
    "S/_know(_bob,_eve) -> bβe\n",
    "S/_know(_bob,_carol) -> bβc\n",
    "S/_meet(_john,_alice) -> dγa\n",
    "S/_meet(_john,_eve) -> dγe\n",
    "S/_meet(_john,_carol) -> dγc\n",
    "S/_meet(_john,_bob) -> dγb\n",
    "S/_meet(_alice,_john) -> aγd\n",
    "S/_meet(_alice,_eve) -> aγe\n",
    "S/_meet(_alice,_carol) -> aγc\n",
    "S/_meet(_alice,_bob) -> aγb\n",
    "S/_meet(_eve,_john) -> eγd\n",
    "S/_meet(_eve,_alice) -> eγa\n",
    "S/_meet(_eve,_carol) -> eγc\n",
    "S/_meet(_eve,_bob) -> eγb\n",
    "S/_meet(_carol,_john) -> cγd\n",
    "S/_meet(_carol,_alice) -> cγa\n",
    "S/_meet(_carol,_eve) -> cγe\n",
    "S/_meet(_carol,_bob) -> cγb\n",
    "S/_meet(_bob,_john) -> bγd\n",
    "S/_meet(_bob,_alice) -> bγa\n",
    "S/_meet(_bob,_eve) -> bγe\n",
    "S/_meet(_bob,_carol) -> bγc\n",
    "S/_like(_john,_alice) -> dδa\n",
    "S/_like(_john,_eve) -> dδe\n",
    "S/_like(_john,_carol) -> dδc\n",
    "S/_like(_john,_bob) -> dδb\n",
    "S/_like(_alice,_john) -> aδd\n",
    "S/_like(_alice,_eve) -> aδe\n",
    "S/_like(_alice,_carol) -> aδc\n",
    "S/_like(_alice,_bob) -> aδb\n",
    "S/_like(_eve,_john) -> eδd\n",
    "S/_like(_eve,_alice) -> eδa\n",
    "S/_like(_eve,_carol) -> eδc\n",
    "S/_like(_eve,_bob) -> eδb\n",
    "S/_like(_carol,_john) -> cδd\n",
    "S/_like(_carol,_alice) -> cδa\n",
    "S/_like(_carol,_eve) -> cδe\n",
    "S/_like(_carol,_bob) -> cδb\n",
    "S/_like(_bob,_john) -> bδd\n",
    "S/_like(_bob,_alice) -> bδa\n",
    "S/_like(_bob,_eve) -> bδe\n",
    "S/_like(_bob,_carol) -> bδc\n",
    "S/_admire(_john,_alice) -> dεa\n",
    "S/_admire(_john,_eve) -> dεe\n",
    "S/_admire(_john,_carol) -> dεc\n",
    "S/_admire(_john,_bob) -> dεb\n",
    "S/_admire(_alice,_john) -> aεd\n",
    "S/_admire(_alice,_eve) -> aεe\n",
    "S/_admire(_alice,_carol) -> aεc\n",
    "S/_admire(_alice,_bob) -> aεb\n",
    "S/_admire(_eve,_john) -> eεd\n",
    "S/_admire(_eve,_alice) -> eεa\n",
    "S/_admire(_eve,_carol) -> eεc\n",
    "S/_admire(_eve,_bob) -> eεb\n",
    "S/_admire(_carol,_john) -> cεd\n",
    "S/_admire(_carol,_alice) -> cεa\n",
    "S/_admire(_carol,_eve) -> cεe\n",
    "S/_admire(_carol,_bob) -> cεb\n",
    "S/_admire(_bob,_john) -> bεd\n",
    "S/_admire(_bob,_alice) -> bεa\n",
    "S/_admire(_bob,_eve) -> bεe\n",
    "S/_admire(_bob,_carol) -> bεc\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = Grammar()\n",
    "grammar.from_string(semantic_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test chunk01 on whole semantic space\n",
    "grammar.chunk01()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S/_admire(x,_carol) -> Rεc\n",
      "S/_admire(x,_alice) -> Gεa\n",
      "S/_admire(_alice,x) -> aεV\n",
      "S/_admire(x,_carol) -> Hεc\n",
      "S/_admire(_carol,x) -> cεL\n",
      "S/_admire(_alice,x) -> aεY\n",
      "S/_admire(_carol,x) -> cεN\n",
      "S/_admire(_alice,x) -> aεE\n",
      "S/_admire(x,_alice) -> Bεa\n",
      "S/_admire(x,_carol) -> Jεc\n",
      "S/_admire(_alice,x) -> aεX\n",
      "S/_admire(_alice,x) -> aεB\n",
      "S/_admire(x,_alice) -> Aεa\n",
      "S/_admire(_carol,x) -> cεU\n",
      "S/_admire(x,_alice) -> Cεa\n",
      "S/_admire(_carol,x) -> cεE\n",
      "S/_admire(x,_carol) -> Nεc\n",
      "S/_admire(_carol,x) -> cεA\n",
      "S/_admire(_alice,x) -> aεF\n",
      "S/_admire(x,_carol) -> Yεc\n",
      "S/_admire(x,_alice) -> Rεa\n",
      "S/X(_carol,_alice) -> cUa\n",
      "S/X(_alice,_carol) -> aWc\n",
      "S/X(_carol,_alice) -> cTa\n",
      "S/X(_alice,_carol) -> aKc\n",
      "S/X(_carol,_alice) -> cOa\n",
      "S/X(_alice,_carol) -> aVc\n",
      "S/_admire(x,_john) -> Dεd\n",
      "S/X(_alice,_carol) -> aBc\n",
      "S/_admire(_john,x) -> dεD\n",
      "S/_admire(_john,x) -> dεF\n",
      "S/X(_carol,_alice) -> cXa\n",
      "S/_admire(x,_john) -> Uεd\n",
      "S/X(_carol,_alice) -> cKa\n",
      "S/X(_carol,_alice) -> cBa\n",
      "S/X(_alice,_carol) -> aGc\n",
      "S/X(_carol,_alice) -> cQa\n",
      "S/X(_alice,_carol) -> aQc\n",
      "S/_admire(_john,x) -> dεQ\n",
      "S/_admire(x,_john) -> Qεd\n",
      "S/X(_alice,_carol) -> aDc\n",
      "S/X(_carol,_alice) -> cRa\n",
      "S/X(_alice,_carol) -> aCc\n",
      "S/_admire(_john,x) -> dεZ\n",
      "S/X(_alice,_carol) -> aXc\n",
      "S/X(_alice,_carol) -> aMc\n",
      "S/X(_carol,_alice) -> cCa\n",
      "S/_admire(x,_john) -> Wεd\n",
      "S/_meet(_alice,x) -> aγX\n",
      "S/_like(_alice,x) -> aδP\n",
      "S/_know(x,_carol) -> Cβc\n",
      "S/_kick(_alice,x) -> aαF\n",
      "S/_kick(_carol,x) -> cαY\n",
      "S/_kick(x,_alice) -> Pαa\n",
      "S/X(_alice,_john) -> aQd\n",
      "S/_know(_carol,x) -> cβO\n",
      "S/X(_carol,_john) -> cId\n",
      "S/_kick(_carol,x) -> cαU\n",
      "S/_like(_carol,x) -> cδE\n",
      "S/X(_alice,_john) -> aJd\n",
      "S/_admire(x,_bob) -> Lεb\n",
      "S/_know(_carol,x) -> cβY\n",
      "S/_know(x,_carol) -> Rβc\n",
      "S/_like(x,_carol) -> Kδc\n",
      "S/_like(x,_carol) -> Oδc\n",
      "S/_meet(_carol,x) -> cγU\n",
      "S/X(_alice,_john) -> aZd\n",
      "S/_meet(x,_carol) -> Oγc\n",
      "S/X(_john,_alice) -> dHa\n",
      "S/_know(x,_alice) -> Gβa\n",
      "S/_kick(_alice,x) -> aαC\n",
      "S/_know(_carol,x) -> cβJ\n",
      "S/X(_john,_carol) -> dQc\n",
      "S/_meet(_carol,x) -> cγK\n",
      "S/X(_alice,_john) -> aFd\n",
      "S/_kick(_carol,x) -> cαN\n",
      "S/X(_john,_carol) -> dKc\n",
      "S/_like(x,_carol) -> Lδc\n",
      "S/_like(x,_carol) -> Pδc\n",
      "S/_kick(x,_alice) -> Aαa\n",
      "S/_admire(x,_bob) -> Dεb\n",
      "S/_know(x,_carol) -> Iβc\n",
      "S/_like(_carol,x) -> cδJ\n",
      "S/_meet(x,_alice) -> Xγa\n",
      "S/X(_john,_alice) -> dTa\n",
      "S/X(_john,_carol) -> dZc\n",
      "S/_kick(_alice,x) -> aαU\n",
      "S/_admire(_bob,x) -> bεE\n",
      "S/_know(_alice,x) -> aβY\n",
      "S/_meet(x,_alice) -> Lγa\n",
      "S/_kick(_alice,x) -> aαY\n",
      "S/_meet(_alice,x) -> aγY\n",
      "S/X(_john,_alice) -> dKa\n",
      "S/X(_john,_carol) -> dTc\n",
      "S/_kick(_alice,x) -> aαB\n",
      "S/X(_carol,_john) -> cKd\n",
      "S/X(_john,_alice) -> dBa\n",
      "S/_meet(_carol,x) -> cγL\n",
      "S/X(_john,_carol) -> dYc\n",
      "S/_like(_alice,x) -> aδF\n",
      "S/X(_john,_carol) -> dEc\n",
      "S/_kick(_carol,x) -> cαK\n",
      "S/_meet(x,_carol) -> Xγc\n",
      "S/_admire(_eve,x) -> eεQ\n",
      "S/_kick(_carol,x) -> cαM\n",
      "S/X(_john,_carol) -> dIc\n",
      "S/_like(x,_alice) -> Zδa\n",
      "S/X(_carol,_john) -> cCd\n",
      "S/_meet(_alice,x) -> aγG\n",
      "S/_admire(_eve,x) -> eεF\n",
      "S/X(_carol,_john) -> cZd\n",
      "S/_know(_alice,x) -> aβM\n",
      "S/_admire(x,_eve) -> Xεe\n",
      "S/X(_alice,_john) -> aVd\n",
      "S/_meet(x,_carol) -> Fγc\n",
      "S/_admire(_eve,x) -> eεZ\n",
      "S/_like(x,_alice) -> Gδa\n",
      "S/_admire(x,_bob) -> Jεb\n",
      "S/_like(_alice,x) -> aδD\n",
      "S/_know(x,_carol) -> Mβc\n",
      "S/_know(x,_alice) -> Bβa\n",
      "S/_meet(x,_alice) -> Oγa\n",
      "S/_admire(_eve,x) -> eεN\n",
      "S/_like(_alice,x) -> aδZ\n",
      "S/_know(_alice,x) -> aβT\n",
      "S/_admire(x,_eve) -> Jεe\n",
      "S/X(_alice,_john) -> aXd\n",
      "S/_meet(_alice,x) -> aγM\n",
      "S/_admire(_bob,x) -> bεV\n",
      "S/X(_john,_alice) -> dPa\n",
      "S/_kick(x,_carol) -> Mαc\n",
      "S/_like(x,_carol) -> Tδc\n",
      "S/_know(x,_alice) -> Uβa\n",
      "S/_know(_carol,x) -> cβZ\n",
      "S/X(_carol,_john) -> cBd\n",
      "S/_meet(x,_carol) -> Wγc\n",
      "S/_like(x,_alice) -> Mδa\n",
      "S/_admire(_bob,x) -> bεH\n",
      "S/_kick(_alice,x) -> aαH\n",
      "S/_know(_carol,x) -> cβB\n",
      "S/_like(x,_alice) -> Iδa\n",
      "S/X(_carol,_john) -> cJd\n",
      "S/X(_carol,_john) -> cAd\n",
      "S/X(_carol,_john) -> cHd\n",
      "S/_like(_alice,x) -> aδE\n",
      "S/_kick(x,_carol) -> Rαc\n",
      "S/_kick(x,_alice) -> Vαa\n",
      "S/X(_john,_carol) -> dDc\n",
      "S/_admire(_bob,x) -> bεP\n",
      "S/_meet(x,_carol) -> Yγc\n",
      "S/_know(x,_alice) -> Yβa\n",
      "S/_meet(x,_alice) -> Eγa\n",
      "S/_kick(x,_alice) -> Uαa\n",
      "S/_admire(_eve,x) -> eεV\n",
      "S/_kick(x,_alice) -> Zαa\n",
      "S/_like(_carol,x) -> cδB\n",
      "S/_meet(_alice,x) -> aγW\n",
      "S/_know(x,_alice) -> Fβa\n",
      "S/X(_john,_alice) -> dLa\n",
      "S/X(_alice,_john) -> aNd\n",
      "S/_admire(x,_eve) -> Zεe\n",
      "S/_know(x,_carol) -> Fβc\n",
      "S/_admire(_bob,x) -> bεY\n",
      "S/_know(_alice,x) -> aβF\n",
      "S/_meet(_carol,x) -> cγN\n",
      "S/_meet(_alice,x) -> aγV\n",
      "S/X(_john,_carol) -> dJc\n",
      "S/X(_john,_carol) -> dHc\n",
      "S/_like(_alice,x) -> aδB\n",
      "S/_kick(x,_carol) -> Yαc\n",
      "S/X(_carol,_john) -> cYd\n",
      "S/_like(x,_alice) -> Fδa\n",
      "S/_admire(_bob,x) -> bεM\n",
      "S/_know(_carol,x) -> cβT\n",
      "S/_meet(x,_carol) -> Tγc\n",
      "S/_admire(x,_eve) -> Fεe\n",
      "S/X(_john,_alice) -> dAa\n",
      "S/_like(x,_alice) -> Tδa\n",
      "S/_kick(_carol,x) -> cαL\n",
      "S/X(_alice,_john) -> aGd\n",
      "S/_know(_alice,x) -> aβQ\n",
      "S/_admire(_eve,x) -> eεM\n",
      "S/_like(_carol,x) -> cδY\n",
      "S/X(_john,_alice) -> dQa\n",
      "S/_kick(x,_carol) -> Oαc\n",
      "S/_like(x,_carol) -> Aδc\n",
      "S/_meet(_carol,x) -> cγM\n",
      "S/_kick(x,_carol) -> Xαc\n",
      "S/_know(x,_alice) -> Lβa\n",
      "S/_meet(_carol,x) -> cγT\n",
      "S/_know(x,_carol) -> Eβc\n",
      "S/_like(_carol,x) -> cδU\n",
      "S/_admire(x,_bob) -> Yεb\n",
      "S/X(_john,_alice) -> dDa\n",
      "S/_kick(_john,x) -> dαC\n",
      "S/X(_bob,_carol) -> bRc\n",
      "S/_meet(_john,x) -> dγK\n",
      "S/_like(_john,x) -> dδF\n",
      "S/_kick(x,_john) -> Jαd\n",
      "S/_like(x,_john) -> Vδd\n",
      "S/X(_eve,_carol) -> eZc\n",
      "S/X(_carol,_bob) -> cPb\n",
      "S/X(_alice,_bob) -> aZb\n",
      "S/_kick(_john,x) -> dαQ\n",
      "S/X(_alice,_eve) -> aHe\n",
      "S/X(_alice,_eve) -> aYe\n",
      "S/_know(_john,x) -> dβV\n",
      "S/X(_eve,_carol) -> eCc\n",
      "S/X(_alice,_bob) -> aJb\n",
      "S/X(_carol,_eve) -> cLe\n",
      "S/X(_carol,_bob) -> cCb\n",
      "S/_kick(x,_john) -> Yαd\n",
      "S/X(_alice,_bob) -> aGb\n",
      "S/X(_bob,_carol) -> bAc\n",
      "S/X(_bob,_carol) -> bUc\n",
      "S/_know(x,_john) -> Zβd\n",
      "S/X(_alice,_eve) -> aRe\n",
      "S/_kick(x,_john) -> Rαd\n",
      "S/X(_bob,_carol) -> bIc\n",
      "S/_like(_john,x) -> dδT\n",
      "S/_meet(_john,x) -> dγV\n",
      "S/X(_eve,_carol) -> ePc\n",
      "S/_kick(x,_john) -> Cαd\n",
      "S/X(_alice,_eve) -> aPe\n",
      "S/_know(_john,x) -> dβB\n",
      "S/_know(_john,x) -> dβW\n",
      "S/X(_alice,_bob) -> aRb\n",
      "S/_like(x,_john) -> Hδd\n",
      "S/X(_bob,_alice) -> bXa\n",
      "S/_like(_john,x) -> dδW\n",
      "S/_like(x,_john) -> Uδd\n",
      "S/X(_alice,_eve) -> aXe\n",
      "S/_meet(_john,x) -> dγG\n",
      "S/X(_carol,_eve) -> cQe\n",
      "S/X(_bob,_carol) -> bZc\n",
      "S/X(_carol,_bob) -> cVb\n",
      "S/X(_carol,_bob) -> cMb\n",
      "S/X(_carol,_bob) -> cBb\n",
      "S/X(_carol,_eve) -> cHe\n",
      "S/_know(x,_john) -> Iβd\n",
      "S/_kick(_john,x) -> dαE\n",
      "S/X(_eve,_alice) -> eJa\n",
      "S/X(_alice,_bob) -> aXb\n",
      "S/X(_alice,_bob) -> aEb\n",
      "S/X(_carol,_bob) -> cWb\n",
      "S/_like(x,_john) -> Qδd\n",
      "S/X(_eve,_alice) -> eQa\n",
      "S/X(_eve,_alice) -> eGa\n",
      "S/X(_alice,_eve) -> aLe\n",
      "S/X(_bob,_alice) -> bQa\n",
      "S/X(_alice,_eve) -> aUe\n",
      "S/_meet(_john,x) -> dγU\n",
      "S/X(_eve,_carol) -> eJc\n",
      "S/X(_carol,_eve) -> cUe\n",
      "S/X(_carol,_eve) -> cIe\n",
      "S/X(_eve,_alice) -> eEa\n",
      "S/_like(_john,x) -> dδX\n",
      "S/_like(_john,x) -> dδL\n",
      "S/X(_carol,_bob) -> cJb\n",
      "S/X(_carol,_eve) -> cFe\n",
      "S/X(_alice,_bob) -> aYb\n",
      "S/_know(x,_john) -> Tβd\n",
      "S/X(_eve,_carol) -> eVc\n",
      "S/_meet(x,_john) -> Wγd\n",
      "S/X(_bob,_alice) -> bPa\n",
      "S/X(_eve,_alice) -> eXa\n",
      "S/X(_carol,_bob) -> cNb\n",
      "S/X(_eve,_carol) -> eLc\n",
      "S/X(_alice,_eve) -> aEe\n",
      "S/X(_bob,_carol) -> bWc\n",
      "S/X(_bob,_alice) -> bLa\n",
      "S/X(_carol,_eve) -> cBe\n",
      "S/X(_eve,_alice) -> eTa\n",
      "S/X(_alice,_bob) -> aQb\n",
      "S/_meet(_john,x) -> dγI\n",
      "S/_know(x,_john) -> Eβd\n",
      "S/_kick(_john,x) -> dαH\n",
      "S/_meet(x,_john) -> Uγd\n",
      "S/_kick(x,_john) -> Lαd\n",
      "S/_know(_john,x) -> dβM\n",
      "S/X(_bob,_alice) -> bDa\n",
      "S/X(_alice,_eve) -> aZe\n",
      "S/_kick(x,_john) -> Mαd\n",
      "S/X(_eve,_carol) -> eQc\n",
      "S/_like(x,_john) -> Yδd\n",
      "S/X(_bob,_carol) -> bXc\n",
      "S/_meet(x,_john) -> Oγd\n",
      "S/X(_bob,_carol) -> bCc\n",
      "S/X(_bob,_alice) -> bHa\n",
      "S/_meet(x,_john) -> Qγd\n",
      "S/X(_carol,_eve) -> cZe\n",
      "S/_kick(_john,x) -> dαD\n",
      "S/_know(x,_john) -> Hβd\n",
      "S/X(_eve,_alice) -> eLa\n",
      "S/X(_carol,_eve) -> cXe\n",
      "S/_meet(x,_john) -> Hγd\n",
      "S/_like(_john,x) -> dδN\n",
      "S/_meet(_john,x) -> dγZ\n",
      "S/X(_bob,_alice) -> bCa\n",
      "S/X(_eve,_carol) -> eKc\n",
      "S/_like(x,_john) -> Tδd\n",
      "S/_meet(x,_john) -> Tγd\n",
      "S/X(_eve,_carol) -> eHc\n",
      "S/X(_eve,_alice) -> eDa\n",
      "S/_like(_eve,x) -> eδW\n",
      "S/X(_john,_eve) -> dVe\n",
      "S/X(_eve,_john) -> eId\n",
      "S/X(_john,_bob) -> dXb\n",
      "S/X(_eve,_john) -> eYd\n",
      "S/_meet(x,_bob) -> Wγb\n",
      "S/_kick(_bob,x) -> bαW\n",
      "S/_know(_bob,x) -> bβW\n",
      "S/_know(_bob,x) -> bβK\n",
      "S/_kick(x,_eve) -> Yαe\n",
      "S/_like(_eve,x) -> eδQ\n",
      "S/_kick(_bob,x) -> bαE\n",
      "S/_like(_bob,x) -> bδQ\n",
      "S/X(_bob,_john) -> bHd\n",
      "S/_kick(x,_bob) -> Gαb\n",
      "S/_know(_eve,x) -> eβF\n",
      "S/X(_john,_eve) -> dTe\n",
      "S/_meet(x,_bob) -> Pγb\n",
      "S/_like(_eve,x) -> eδC\n",
      "S/X(_john,_bob) -> dKb\n",
      "S/_kick(_eve,x) -> eαA\n",
      "S/_know(_bob,x) -> bβG\n",
      "S/X(_john,_bob) -> dLb\n",
      "S/_know(x,_eve) -> Oβe\n",
      "S/_meet(_eve,x) -> eγU\n",
      "S/_like(x,_bob) -> Xδb\n",
      "S/_meet(_eve,x) -> eγE\n",
      "S/_kick(x,_eve) -> Aαe\n",
      "S/_kick(_eve,x) -> eαC\n",
      "S/X(_john,_bob) -> dVb\n",
      "S/_like(_bob,x) -> bδH\n",
      "S/X(_bob,_john) -> bNd\n",
      "S/_kick(x,_eve) -> Pαe\n",
      "S/_meet(_bob,x) -> bγW\n",
      "S/_meet(x,_eve) -> Vγe\n",
      "S/_kick(x,_bob) -> Xαb\n",
      "S/_like(x,_eve) -> Hδe\n",
      "S/_like(_bob,x) -> bδY\n",
      "S/_like(x,_eve) -> Lδe\n",
      "S/X(_eve,_john) -> eZd\n",
      "S/X(_eve,_john) -> eDd\n",
      "S/_like(x,_bob) -> Rδb\n",
      "S/X(_john,_bob) -> dWb\n",
      "S/_meet(_bob,x) -> bγV\n",
      "S/_meet(x,_eve) -> Wγe\n",
      "S/_like(x,_eve) -> Iδe\n",
      "S/_know(_eve,x) -> eβH\n",
      "S/_like(x,_eve) -> Vδe\n",
      "S/_kick(x,_eve) -> Eαe\n",
      "S/_know(_bob,x) -> bβY\n",
      "S/_know(x,_bob) -> Qβb\n",
      "S/X(_bob,_john) -> bKd\n",
      "S/X(_john,_eve) -> dRe\n",
      "S/_kick(_eve,x) -> eαO\n",
      "S/_kick(_eve,x) -> eαQ\n",
      "S/_like(x,_eve) -> Wδe\n",
      "S/X(_eve,_john) -> eFd\n",
      "S/_meet(x,_eve) -> Pγe\n",
      "S/_meet(x,_eve) -> Kγe\n",
      "S/_know(x,_eve) -> Xβe\n",
      "S/_kick(x,_bob) -> Lαb\n",
      "S/_meet(_eve,x) -> eγQ\n",
      "S/_meet(x,_bob) -> Qγb\n",
      "S/X(_bob,_john) -> bJd\n",
      "S/_kick(_eve,x) -> eαT\n",
      "S/X(_eve,_john) -> eHd\n",
      "S/X(_john,_bob) -> dCb\n",
      "S/_kick(x,_bob) -> Bαb\n",
      "S/X(_john,_bob) -> dJb\n",
      "S/X(_john,_bob) -> dPb\n",
      "S/X(_john,_bob) -> dFb\n",
      "S/X(_bob,_john) -> bVd\n",
      "S/X(_john,_eve) -> dPe\n",
      "S/_know(_eve,x) -> eβV\n",
      "S/_kick(_bob,x) -> bαP\n",
      "S/X(_john,_eve) -> dOe\n",
      "S/_know(x,_bob) -> Oβb\n",
      "S/_know(x,_eve) -> Aβe\n",
      "S/_like(_bob,x) -> bδZ\n",
      "S/X(_eve,_john) -> eBd\n",
      "S/_meet(x,_eve) -> Uγe\n",
      "S/_meet(x,_eve) -> Oγe\n",
      "S/_kick(_bob,x) -> bαY\n",
      "S/X(_john,_eve) -> dZe\n",
      "S/X(_eve,_john) -> eAd\n",
      "S/_meet(_bob,x) -> bγC\n",
      "S/_kick(x,_bob) -> Hαb\n",
      "S/_like(_eve,x) -> eδH\n",
      "S/X(_john,_eve) -> dAe\n",
      "S/_know(x,_eve) -> Cβe\n",
      "S/X(_bob,_john) -> bPd\n",
      "S/_like(x,_bob) -> Kδb\n",
      "S/_know(x,_bob) -> Cβb\n",
      "S/_know(x,_bob) -> Nβb\n",
      "S/_know(x,_eve) -> Nβe\n",
      "S/_meet(_bob,x) -> bγL\n",
      "S/_meet(_eve,x) -> eγD\n",
      "S/_kick(x,_eve) -> Wαe\n",
      "S/_kick(x,_bob) -> Cαb\n",
      "S/_know(_eve,x) -> eβW\n",
      "S/X(_john,_eve) -> dCe\n",
      "S/_meet(_eve,x) -> eγK\n",
      "S/_like(_eve,x) -> eδF\n",
      "S/_meet(x,_bob) -> Yγb\n",
      "S/_kick(_bob,x) -> bαD\n",
      "S/_kick(x,_eve) -> Hαe\n",
      "S/_meet(x,_bob) -> Fγb\n",
      "S/_kick(_eve,x) -> eαG\n",
      "S/_know(_eve,x) -> eβJ\n",
      "S/_like(_bob,x) -> bδE\n",
      "S/_know(_bob,x) -> bβC\n",
      "S/_like(x,_eve) -> Oδe\n",
      "S/_know(x,_eve) -> Rβe\n",
      "S/_know(_bob,x) -> bβV\n",
      "S/_meet(_bob,x) -> bγZ\n",
      "S/_like(x,_bob) -> Pδb\n",
      "S/X(_bob,_john) -> bLd\n",
      "S/_know(_eve,x) -> eβA\n",
      "S/_know(x,_bob) -> Dβb\n",
      "S/X(_bob,_eve) -> bGe\n",
      "S/X(_bob,_eve) -> bJe\n",
      "S/X(_eve,_bob) -> eEb\n",
      "S/X(_bob,_eve) -> bEe\n",
      "S/X(_eve,_bob) -> eGb\n",
      "S/X(_eve,_bob) -> eQb\n",
      "S/X(_eve,_bob) -> eWb\n",
      "S/X(_bob,_eve) -> bXe\n",
      "S/X(_eve,_bob) -> eMb\n",
      "S/X(_eve,_bob) -> eXb\n",
      "S/X(_bob,_eve) -> bAe\n",
      "S/X(_eve,_bob) -> eAb\n",
      "S/X(_bob,_eve) -> bKe\n",
      "S/X(_eve,_bob) -> eUb\n",
      "S/X(_bob,_eve) -> bOe\n",
      "F/_admire -> ε\n",
      "J/_admire -> ε\n",
      "W/_admire -> ε\n",
      "X/_admire -> ε\n",
      "K/_admire -> ε\n",
      "Y/_admire -> ε\n",
      "G/_admire -> ε\n",
      "P/_admire -> ε\n",
      "E/_admire -> ε\n",
      "L/_admire -> ε\n",
      "V/_admire -> ε\n",
      "A/_admire -> ε\n",
      "C/_admire -> ε\n",
      "M/_admire -> ε\n",
      "R/_admire -> ε\n",
      "N/_admire -> ε\n",
      "Q/_admire -> ε\n",
      "H/_admire -> ε\n",
      "D/_admire -> ε\n",
      "I/_admire -> ε\n",
      "T/_admire -> ε\n",
      "Z/_admire -> ε\n",
      "U/_admire -> ε\n",
      "B/_admire -> ε\n",
      "O/_admire -> ε\n",
      "T/_carol -> c\n",
      "J/_carol -> c\n",
      "I/_carol -> c\n",
      "D/_carol -> c\n",
      "N/_alice -> a\n",
      "Z/_carol -> c\n",
      "J/_alice -> a\n",
      "E/_carol -> c\n",
      "W/_carol -> c\n",
      "B/_alice -> a\n",
      "Y/_carol -> c\n",
      "A/_carol -> c\n",
      "C/_carol -> c\n",
      "P/_carol -> c\n",
      "O/_carol -> c\n",
      "P/_alice -> a\n",
      "D/_alice -> a\n",
      "G/_carol -> c\n",
      "I/_alice -> a\n",
      "U/_carol -> c\n",
      "Q/_alice -> a\n",
      "N/_carol -> c\n",
      "C/_alice -> a\n",
      "X/_alice -> a\n",
      "W/_alice -> a\n",
      "E/_alice -> a\n",
      "R/_alice -> a\n",
      "Y/_alice -> a\n",
      "U/_alice -> a\n",
      "L/_alice -> a\n",
      "M/_carol -> c\n",
      "X/_carol -> c\n",
      "F/_alice -> a\n",
      "K/_carol -> c\n",
      "O/_alice -> a\n",
      "L/_carol -> c\n",
      "V/_alice -> a\n",
      "V/_carol -> c\n",
      "H/_alice -> a\n",
      "F/_carol -> c\n",
      "Z/_alice -> a\n",
      "Q/_carol -> c\n",
      "A/_alice -> a\n",
      "B/_carol -> c\n",
      "M/_alice -> a\n",
      "T/_alice -> a\n",
      "K/_alice -> a\n",
      "H/_carol -> c\n",
      "P/_like -> δ\n",
      "V/_kick -> α\n",
      "Y/_john -> d\n",
      "X/_kick -> α\n",
      "X/_know -> β\n",
      "T/_john -> d\n",
      "P/_know -> β\n",
      "X/_meet -> γ\n",
      "T/_know -> β\n",
      "J/_like -> δ\n",
      "O/_john -> d\n",
      "O/_meet -> γ\n",
      "C/_meet -> γ\n",
      "V/_know -> β\n",
      "O/_know -> β\n",
      "Z/_john -> d\n",
      "C/_kick -> α\n",
      "R/_meet -> γ\n",
      "G/_john -> d\n",
      "D/_john -> d\n",
      "K/_know -> β\n",
      "K/_like -> δ\n",
      "G/_kick -> α\n",
      "H/_meet -> γ\n",
      "G/_like -> δ\n",
      "U/_like -> δ\n",
      "W/_like -> δ\n",
      "L/_john -> d\n",
      "D/_know -> β\n",
      "F/_john -> d\n",
      "Q/_like -> δ\n",
      "D/_kick -> α\n",
      "G/_meet -> γ\n",
      "Y/_kick -> α\n",
      "V/_john -> d\n",
      "T/_like -> δ\n",
      "I/_kick -> α\n",
      "E/_like -> δ\n",
      "L/_like -> δ\n",
      "M/_like -> δ\n",
      "W/_kick -> α\n",
      "P/_meet -> γ\n",
      "X/_john -> d\n",
      "D/_meet -> γ\n",
      "Y/_know -> β\n",
      "B/_know -> β\n",
      "H/_like -> δ\n",
      "A/_meet -> γ\n",
      "E/_know -> β\n",
      "W/_know -> β\n",
      "I/_john -> d\n",
      "O/_like -> δ\n",
      "R/_like -> δ\n",
      "U/_meet -> γ\n",
      "J/_meet -> γ\n",
      "A/_john -> d\n",
      "O/_kick -> α\n",
      "Y/_like -> δ\n",
      "B/_meet -> γ\n",
      "I/_know -> β\n",
      "Q/_meet -> γ\n",
      "N/_like -> δ\n",
      "N/_kick -> α\n",
      "E/_john -> d\n",
      "L/_kick -> α\n",
      "H/_know -> β\n",
      "Y/_meet -> γ\n",
      "Q/_john -> d\n",
      "C/_like -> δ\n",
      "Z/_like -> δ\n",
      "I/_meet -> γ\n",
      "B/_john -> d\n",
      "A/_know -> β\n",
      "R/_know -> β\n",
      "J/_kick -> α\n",
      "T/_kick -> α\n",
      "V/_like -> δ\n",
      "H/_john -> d\n",
      "K/_john -> d\n",
      "U/_kick -> α\n",
      "M/_kick -> α\n",
      "A/_like -> δ\n",
      "X/_like -> δ\n",
      "E/_meet -> γ\n",
      "H/_kick -> α\n",
      "Z/_know -> β\n",
      "M/_meet -> γ\n",
      "Z/_kick -> α\n",
      "K/_meet -> γ\n",
      "A/_kick -> α\n",
      "P/_kick -> α\n",
      "Q/_know -> β\n",
      "Q/_kick -> α\n",
      "L/_know -> β\n",
      "N/_know -> β\n",
      "W/_john -> d\n",
      "G/_know -> β\n",
      "U/_john -> d\n",
      "C/_john -> d\n",
      "K/_kick -> α\n",
      "B/_like -> δ\n",
      "F/_like -> δ\n",
      "R/_john -> d\n",
      "R/_kick -> α\n",
      "F/_kick -> α\n",
      "D/_like -> δ\n",
      "C/_know -> β\n",
      "W/_meet -> γ\n",
      "M/_john -> d\n",
      "T/_meet -> γ\n",
      "F/_meet -> γ\n",
      "Z/_meet -> γ\n",
      "B/_kick -> α\n",
      "L/_meet -> γ\n",
      "U/_know -> β\n",
      "F/_know -> β\n",
      "J/_john -> d\n",
      "J/_know -> β\n",
      "N/_john -> d\n",
      "V/_meet -> γ\n",
      "P/_john -> d\n",
      "M/_know -> β\n",
      "I/_like -> δ\n",
      "P/_bob -> b\n",
      "D/_bob -> b\n",
      "B/_eve -> e\n",
      "Z/_bob -> b\n",
      "C/_bob -> b\n",
      "V/_bob -> b\n",
      "H/_eve -> e\n",
      "V/_eve -> e\n",
      "R/_eve -> e\n",
      "L/_bob -> b\n",
      "N/_bob -> b\n",
      "G/_bob -> b\n",
      "Q/_bob -> b\n",
      "R/_bob -> b\n",
      "F/_bob -> b\n",
      "N/_eve -> e\n",
      "T/_bob -> b\n",
      "D/_eve -> e\n",
      "B/_bob -> b\n",
      "O/_eve -> e\n",
      "K/_eve -> e\n",
      "A/_bob -> b\n",
      "E/_bob -> b\n",
      "F/_eve -> e\n",
      "U/_bob -> b\n",
      "A/_eve -> e\n",
      "L/_eve -> e\n",
      "U/_eve -> e\n",
      "O/_bob -> b\n",
      "J/_eve -> e\n",
      "M/_eve -> e\n",
      "Q/_eve -> e\n",
      "T/_eve -> e\n",
      "X/_eve -> e\n",
      "K/_bob -> b\n",
      "Y/_bob -> b\n",
      "I/_eve -> e\n",
      "Y/_eve -> e\n",
      "M/_bob -> b\n",
      "E/_eve -> e\n",
      "C/_eve -> e\n",
      "G/_eve -> e\n",
      "W/_bob -> b\n",
      "H/_bob -> b\n",
      "Z/_eve -> e\n",
      "W/_eve -> e\n",
      "X/_bob -> b\n",
      "J/_bob -> b\n",
      "I/_bob -> b\n",
      "P/_eve -> e\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(grammar.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate all possible queries\n",
    "query_space = []\n",
    "\n",
    "for verb_sem in _5verbs_sem:\n",
    "    for subj_sem in _5nouns_sem:\n",
    "        for obj_sem in _5nouns_sem:\n",
    "            if subj_sem != obj_sem:\n",
    "                query_space.append(f\"{verb_sem}({subj_sem},{obj_sem})\")\n",
    "            else:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 515,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(query_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = Grammar()\n",
    "grammar.from_string(generate_holistic_rules(100))\n",
    "utterances = grammar.to_string().split(\"\\n\") [:-2]\n",
    "\n",
    "N_GENS = 200\n",
    "\n",
    "# N_GENS世代継承\n",
    "# TODO: 各世代のlog\n",
    "# TODO: <done>全クエリ生成させて、</done>表現度を計算・ボトルネックをサンプリング\n",
    "for i in tqdm(range(N_GENS)):\n",
    "    last_generation = Grammar()\n",
    "    for utterance in utterances:\n",
    "        last_generation.add_rule(utterance)\n",
    "        operation = random.sample(OPERATIONS, 1)[0]\n",
    "        # print(operation)\n",
    "        # print(\"Before: \", last_generation.to_string())\n",
    "        if len(last_generation.rules) > 1:\n",
    "            if operation == \"chunk01\":\n",
    "                last_generation.chunk01()\n",
    "            if operation == \"chunk02\":\n",
    "                last_generation.chunk02()\n",
    "            if operation == \"merge\":\n",
    "                last_generation.merge()\n",
    "            if operation == \"replace\":\n",
    "                last_generation.chunk01()\n",
    "        # print(\"After: \", last_generation.to_string())\n",
    "\n",
    "    utterances = []\n",
    "\n",
    "    if i != (N_GENS - 1):\n",
    "        queries = query_space # all queries\n",
    "        for query in queries:\n",
    "            utterance = last_generation.generate(query) # generate all\n",
    "            utterances.append(utterance)\n",
    "        utterances = random.sample(utterances, 50) # sample 50 instances\n",
    "\n",
    "print(last_generation.to_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
