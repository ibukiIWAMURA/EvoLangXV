{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "661d77a0-5182-4c42-91be-a865f8d7859a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.sem.logic import *\n",
    "import ast\n",
    "import random\n",
    "import string\n",
    "import copy\n",
    "\n",
    "NOUNS = [\n",
    "    \"david\", \"alice\", \"bob\", \"carol\", \"eve\", \"frank\", \"grace\", \"helen\", \n",
    "    \"ivan\", \"jack\", \"karen\", \"larry\", \"mike\", \"nina\", \"oscar\", \"paul\", \n",
    "    \"quincy\", \"rachel\", \"steve\", \"tracy\", \"ursula\", \"victor\", \"wendy\", \n",
    "    \"xander\", \"yasmine\", \"zach\"\n",
    "]\n",
    "\n",
    "VERBS = [\n",
    "    \"know\", \"admire\", \"like\", \"kick\", \"meet\", \"call\", \"defend\", \"encourage\"\n",
    "    , \"follow\", \"greet\", \"help\", \"invite\", \"judge\", \"notice\", \"obey\", \n",
    "    \"praise\", \"question\", \"respect\", \"support\", \"trust\", \"understand\", \n",
    "    \"value\", \"warn\", \"x-ray\", \"yell\", \"zap\"\n",
    "]\n",
    "\n",
    "VERBS_PSS = [\n",
    "    \"known\", \"admired\", \"liked\", \"kicked\", \"met\"\n",
    "]\n",
    "\n",
    "def generate_noun(stopwords = []):\n",
    "    noun_chars_all = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "    noun_chars = [char for char in noun_chars_all if char not in stopwords]\n",
    "    return random.choice(noun_chars)\n",
    "\n",
    "def generate_verb():\n",
    "    verb_chars = [\"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\"]\n",
    "    return random.choice(verb_chars)\n",
    "\n",
    "def generate_noun_sem(n_sorts: str, stopword: str = \"\"):\n",
    "    assert n_sorts <= len(NOUNS)\n",
    "    chars_wo_stopword = [char for char in NOUNS[:n_sorts] if char not in stopword]\n",
    "    return random.choice(chars_wo_stopword)\n",
    "\n",
    "def generate_verb_sem(n_sorts: str, stopword: str = \"\"):\n",
    "    chars_wo_stopword = [char for char in (VERBS[:n_sorts]+VERBS_PSS[:n_sorts]) if char not in stopword]\n",
    "    return random.choice(chars_wo_stopword)\n",
    "\n",
    "def replace_at_index(s, index, replacement):\n",
    "    return s[:index] + replacement + s[index + 1:]\n",
    "\n",
    "N_SORTS = 5\n",
    "NONTERMINALS = [char for char in string.ascii_uppercase if char != \"S\"]\n",
    "VERB_CATS = NONTERMINALS[:10]\n",
    "NOUN_CATS = NONTERMINALS[10:]\n",
    "INDIVIDUAL_VARIABLES = [\"x\", \"y\"]\n",
    "FUNCTIONAL_VARIABLES = [\"X\"]\n",
    "# VERBS = [\"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\"]\n",
    "# NOUNS = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "\n",
    "class Grammar():\n",
    "    def __init__(self):\n",
    "        self.rules = []\n",
    "\n",
    "    def from_string(self, string: str):\n",
    "        self.rules = []\n",
    "        rules_str = string.split(\"\\n\")\n",
    "        for rule_str in rules_str:\n",
    "            if rule_str != \"\\n\":\n",
    "                if len(rule_str.split(\"\\t\")) == 2:\n",
    "                    rule_body, assignments = rule_str.split(\"\\t\")[0], rule_str.split(\"\\t\")[1]\n",
    "                    assignments = assignments.replace(\"(\", \"\").replace(\")\", \"\")\n",
    "                    asignments_list = assignments.split(\", \")\n",
    "                    asignments_dict = {str(assig.split(\":\")[0]):int(assig.split(\":\")[1]) for assig in asignments_list}\n",
    "                    assignments_sorted = sorted(asignments_dict.items())\n",
    "                    asignments_sorted_dict = dict((var, idx) for var, idx in assignments_sorted)\n",
    "                    rule = rule_body.split(\" \")\n",
    "                    lhs = rule[0].split(\"/\")\n",
    "                    rhs = rule[2]\n",
    "                    lhs_cat = lhs[0]\n",
    "                    lhs_sem = lhs[1]\n",
    "                    lhs_con = int(lhs[2])\n",
    "                    rule = {'lhs':{'cat':lhs_cat, 'sem':lhs_sem, 'con':lhs_con}, 'rhs':rhs, 'var':asignments_sorted_dict}\n",
    "                    self.rules.append(rule)\n",
    "                if len(rule_str.split(\"\\t\")) == 1:\n",
    "                    rule = rule_str.split(\" \")\n",
    "                    lhs = rule[0].split(\"/\")\n",
    "                    rhs = rule[2]\n",
    "                    if len(lhs) == 3:\n",
    "                        lhs_cat = lhs[0]\n",
    "                        lhs_sem = lhs[1]\n",
    "                        lhs_con = int(lhs[2])\n",
    "                        rule = {'lhs':{'cat':lhs_cat, 'sem':lhs_sem, 'con':lhs_con}, 'rhs':rhs}\n",
    "                    if len(lhs) == 2:\n",
    "                        lhs_cat = lhs[0]\n",
    "                        lhs_sem = lhs[1]\n",
    "                        rule = {'lhs':{'cat':lhs_cat, 'sem':lhs_sem}, 'rhs':rhs}\n",
    "                    self.rules.append(rule)\n",
    "        self.rules = sorted(self.rules, key=lambda x: (-len(x['rhs']), x['rhs'], x['lhs']['cat']))\n",
    "    \n",
    "    def add_rule(self, string: str):\n",
    "        rules_str = string.split(\"\\n\")\n",
    "        for rule_str in rules_str:\n",
    "            if rule_str != \"\\n\":\n",
    "                if len(rule_str.split(\"\\t\")) == 2:\n",
    "                    rule_body, assignments = rule_str.split(\"\\t\")[0], rule_str.split(\"\\t\")[1]\n",
    "                    assignments = assignments.replace(\"(\", \"\").replace(\")\", \"\")\n",
    "                    asignments_list = assignments.split(\", \")\n",
    "                    asignments_dict = {str(assig.split(\":\")[0]):int(assig.split(\":\")[1]) for assig in asignments_list}\n",
    "                    assignments_sorted = sorted(asignments_dict.items())\n",
    "                    asignments_sorted_dict = dict((var, idx) for var, idx in assignments_sorted)\n",
    "                    rule = rule_body.split(\" \")\n",
    "                    lhs = rule[0].split(\"/\")\n",
    "                    rhs = rule[2]\n",
    "                    lhs_cat = lhs[0]\n",
    "                    lhs_sem = lhs[1]\n",
    "                    lhs_con = int(lhs[2])\n",
    "                    rule = {'lhs':{'cat':lhs_cat, 'sem':lhs_sem, 'con':lhs_con}, 'rhs':rhs, 'var':asignments_sorted_dict}\n",
    "                    self.rules.append(rule)\n",
    "                if len(rule_str.split(\"\\t\")) == 1:\n",
    "                    rule = rule_str.split(\" \")\n",
    "                    lhs = rule[0].split(\"/\")\n",
    "                    rhs = rule[2]\n",
    "                    if len(lhs) == 3:\n",
    "                        lhs_cat = lhs[0]\n",
    "                        lhs_sem = lhs[1]\n",
    "                        lhs_con = int(lhs[2])\n",
    "                        rule = {'lhs':{'cat':lhs_cat, 'sem':lhs_sem, 'con':lhs_con}, 'rhs':rhs}\n",
    "                    if len(lhs) == 2:\n",
    "                        lhs_cat = lhs[0]\n",
    "                        lhs_sem = lhs[1]\n",
    "                        rule = {'lhs':{'cat':lhs_cat, 'sem':lhs_sem}, 'rhs':rhs}\n",
    "                    self.rules.append(rule)\n",
    "        self.rules = sorted(self.rules, key=lambda x: (-len(x['rhs']), x['rhs'], x['lhs']['cat']))\n",
    "    \n",
    "    def to_string(self):\n",
    "        rules_list = []\n",
    "        rules_str = \"\"\n",
    "        for rule in self.rules:\n",
    "            if 'var' in rule:\n",
    "                asigs_str = \"(\" + \", \".join(f\"{k}:{v}\" for k, v in rule['var'].items()) + \")\"\n",
    "                rules_list.append(f\"{rule['lhs']['cat']}/{rule['lhs']['sem']}/{rule['lhs']['con']} -> {rule['rhs']}\\t{asigs_str}\\n\")\n",
    "            elif 'con' in rule['lhs']:\n",
    "                rules_list.append(f\"{rule['lhs']['cat']}/{rule['lhs']['sem']}/{rule['lhs']['con']} -> {rule['rhs']}\\n\")\n",
    "            else:\n",
    "                rules_list.append(f\"{rule['lhs']['cat']}/{rule['lhs']['sem']} -> {rule['rhs']}\\n\")\n",
    "        sorted_list = sorted(sorted(rules_list), key=lambda x: -len(x))\n",
    "        rules_str += ''.join(sorted_list)+\"\\n\"\n",
    "        return rules_str\n",
    "    \n",
    "    def str2dict(self, string):\n",
    "        return ast.literal_eval(string)\n",
    "    \n",
    "    def sem_list(self):\n",
    "        return [d['lhs']['sem'] for d in self.rules]\n",
    "    \n",
    "    def cat_list(self):\n",
    "        return [d['lhs']['cat'] for d in self.rules]\n",
    "    \n",
    "    def sentence_list(self):\n",
    "        return [d['rhs'] for d in self.rules]\n",
    "    \n",
    "    def is_well_formed(self, expr_str, form):\n",
    "        expr = Expression.fromstring(expr_str)\n",
    "        num_var_expr = len([term for term in expr.free()])\n",
    "        num_uppercase = sum(1 for char in form if char.isupper())\n",
    "        return num_var_expr == num_uppercase\n",
    "\n",
    "    def is_well_assigned(self, form, assignments):\n",
    "        num_uppercase = sum(1 for char in form if char.isupper())\n",
    "        num_assigs = len(assignments)\n",
    "        return num_assigs == num_uppercase\n",
    "\n",
    "    def can_chunk01(self, rule1, rule2):\n",
    "        str1, str2 = rule1[\"rhs\"], rule2[\"rhs\"]\n",
    "        sem1_logic = Expression.fromstring(rule1[\"lhs\"][\"sem\"])\n",
    "        sem2_logic = Expression.fromstring(rule2[\"lhs\"][\"sem\"])\n",
    "        con1 = rule1[\"lhs\"][\"con\"]\n",
    "        con2 = rule2[\"lhs\"][\"con\"]\n",
    "\n",
    "        if len(str1) != 3 or len(str2) != 3: # TODO: Better to judge by category (not sentence length)\n",
    "            return None, None, False\n",
    "        if con1 != con2:\n",
    "            return None, None, False\n",
    "        sem1, sem2 = [sem1_logic.pred, sem1_logic.args[0], sem1_logic.args[1]], [sem2_logic.pred, sem2_logic.args[0], sem2_logic.args[1]]\n",
    "        # print(str1, str2)\n",
    "        diff_count_str = 0\n",
    "        diff_positions_str = []\n",
    "        for i, (char1, char2) in enumerate(zip(str1, str2)):\n",
    "            if (char1 != char2):\n",
    "                # diff_count_str += 1\n",
    "                diff_positions_str.append(i)\n",
    "        diff_count_sem = 0\n",
    "        diff_positions_sem = []\n",
    "        for i, (elm1, elm2) in enumerate(zip(sem1, sem2)):\n",
    "            if (elm1 != elm2):\n",
    "                # diff_count_sem += 1\n",
    "                diff_positions_sem.append(i)\n",
    "        if (len(diff_positions_str) == len(diff_positions_sem) == 1) and ((isinstance(sem1[diff_positions_sem[0]], ConstantExpression)) and (isinstance(sem2[diff_positions_sem[0]], ConstantExpression))):\n",
    "            if str1[diff_positions_str[0]].islower() and str2[diff_positions_str[0]].islower():\n",
    "                return diff_positions_str[0], diff_positions_sem[0], True\n",
    "            else:\n",
    "                return None, None, False\n",
    "        else:\n",
    "            return None, None, False\n",
    "\n",
    "    def find_diff_position_for_chunk01(self, str1, str2):\n",
    "        if len(str1) != 3 or len(str2) != 3:\n",
    "            return None\n",
    "\n",
    "        diff_count = 0\n",
    "        diff_index = None\n",
    "        for index, (char1, char2) in enumerate(zip(str1, str2)):\n",
    "            if char1 != char2:\n",
    "                diff_count += 1\n",
    "                diff_index = index\n",
    "                if not (char1.islower() and char2.islower()):\n",
    "                    return None\n",
    "\n",
    "        if diff_count == 1:\n",
    "            if diff_index == 0:\n",
    "                diff_index_sem = 1\n",
    "            if diff_index == 1:\n",
    "                diff_index_sem = 0\n",
    "            if diff_index == 2:\n",
    "                diff_index_sem = 2\n",
    "            return diff_index, diff_index_sem\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def can_chunk02(self, rule1, rule2):\n",
    "        str1, str2 = rule1[\"rhs\"], rule2[\"rhs\"]\n",
    "        sem1_logic = Expression.fromstring(rule1[\"lhs\"][\"sem\"])\n",
    "        sem2_logic = Expression.fromstring(rule2[\"lhs\"][\"sem\"])\n",
    "        sem1, sem2 = [sem1_logic.pred, sem1_logic.args[0], sem1_logic.args[1]], [sem2_logic.pred, sem2_logic.args[0], sem2_logic.args[1]]\n",
    "        con1 = rule1[\"lhs\"][\"con\"]\n",
    "        con2 = rule2[\"lhs\"][\"con\"]\n",
    "\n",
    "        if len(str1) != 3 or len(str2) != 3: # TODO: Better to judge by category (not sentence length)\n",
    "            return None, None, False, None, None\n",
    "        if con1 != con2:\n",
    "            return None, None, False, None, None\n",
    "        # print(str1, str2)\n",
    "        diff_count_str = 0\n",
    "        diff_positions_str = []\n",
    "        for i, (char1, char2) in enumerate(zip(str1, str2)):\n",
    "            if (char1 != char2):\n",
    "                # if (char1.islower() and char2.isupper()):\n",
    "                diff_count_str += 1\n",
    "                diff_positions_str.append(i)\n",
    "                # elif (char1.isupper() and char2.islower()):\n",
    "                # else:\n",
    "                #    None, False, None, None\n",
    "        diff_count_sem = 0\n",
    "        diff_positions_sem = []\n",
    "        for i, (elm1, elm2) in enumerate(zip(sem1, sem2)):\n",
    "            if (elm1 != elm2):\n",
    "                diff_count_sem += 1\n",
    "                diff_positions_sem.append(i)\n",
    "        if len(diff_positions_str) == len(diff_positions_sem) == 1:\n",
    "            if (str1[diff_positions_str[0]].islower() and str2[diff_positions_str[0]].isupper()):\n",
    "                upper_in_str, lower_in_str = 1, 0\n",
    "                return diff_positions_str[0], diff_positions_sem[0], True, upper_in_str, lower_in_str\n",
    "            elif (str1[diff_positions_str[0]].isupper() and str2[diff_positions_str[0]].islower()):\n",
    "                upper_in_str, lower_in_str = 0, 1\n",
    "                return diff_positions_str[0], diff_positions_sem[0], True, upper_in_str, lower_in_str\n",
    "            else:\n",
    "                return None, None, False, None, None\n",
    "        else:\n",
    "            return None, None, False, None, None\n",
    "    \n",
    "    def find_diff_position_for_chunk02(self, str1, str2):\n",
    "        if len(str1) != 3 or len(str2) != 3:\n",
    "            return None, None\n",
    "\n",
    "        diff_count = 0\n",
    "        diff_index = None\n",
    "        upper_in_str = None\n",
    "\n",
    "        for index, (char1, char2) in enumerate(zip(str1, str2)):\n",
    "            if char1 != char2:\n",
    "                diff_count += 1\n",
    "                diff_index = index\n",
    "                if (char1.islower() and char2.isupper()):\n",
    "                    lower_in_str = 0\n",
    "                    upper_in_str = 1\n",
    "                elif (char1.isupper() and char2.islower()):\n",
    "                    lower_in_str = 1\n",
    "                    upper_in_str = 0\n",
    "                else:\n",
    "                    return None, None\n",
    "\n",
    "        if diff_count == 1:\n",
    "            if diff_index == 0:\n",
    "                diff_index_sem = 1\n",
    "            if diff_index == 1:\n",
    "                diff_index_sem = 0\n",
    "            if diff_index == 2:\n",
    "                diff_index_sem = 2\n",
    "            return diff_index, diff_index_sem, upper_in_str, lower_in_str\n",
    "        else:\n",
    "            return None, None\n",
    "\n",
    "    def highlight_for_replace(self, str1, str2):\n",
    "        if (len(str1) == 3) & (len(str2) == 1):\n",
    "            if str1.count(str2) == 1:\n",
    "                non_word_level, word_level = 0, 1\n",
    "                return non_word_level, word_level\n",
    "            else:\n",
    "                return None, None\n",
    "        elif (len(str1) == 1) & (len(str2) == 3):\n",
    "            if str2.count(str1) == 1:\n",
    "                non_word_level, word_level = 1, 0\n",
    "                return non_word_level, word_level\n",
    "            else:\n",
    "                return None, None\n",
    "        else:\n",
    "            return None, None\n",
    "\n",
    "    def find_diff_position_for_replace(self, str1, str2):\n",
    "        if len(str1) != 3 or len(str2) != 1:\n",
    "            return None\n",
    "\n",
    "        for i, char in enumerate(str1):\n",
    "            if str2 == char:\n",
    "                diff_index = i\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        if diff_index == 0:\n",
    "            diff_index_sem = 1\n",
    "        if diff_index == 1:\n",
    "            diff_index_sem = 0\n",
    "        if diff_index == 2:\n",
    "            diff_index_sem = 2\n",
    "        return diff_index, diff_index_sem\n",
    "    \n",
    "    def existing_variables(self, elements: list):\n",
    "        result = [str(element) for element in elements if not isinstance(element, ConstantExpression)]\n",
    "        return result\n",
    "\n",
    "    # TODO: Sentence-meaning positions do not necessarily have to correspond\n",
    "    def chunk01(self):\n",
    "        rules = self.rules\n",
    "\n",
    "        chuncked_rules = []\n",
    "        new_rules = []\n",
    "\n",
    "        for i in range(len(rules)):\n",
    "            for j in range(i+1, len(rules)):\n",
    "                first_sem = Expression.fromstring(rules[i][\"lhs\"][\"sem\"])\n",
    "                second_sem = Expression.fromstring(rules[j][\"lhs\"][\"sem\"])\n",
    "                first_sentence = rules[i][\"rhs\"]\n",
    "                second_sentence = rules[j][\"rhs\"]\n",
    "                if isinstance(first_sem, ApplicationExpression) & isinstance(second_sem, ApplicationExpression):\n",
    "                    diff_index, diff_index_sem, can_chunk = self.can_chunk01(rules[i], rules[j])\n",
    "                    if can_chunk:\n",
    "                        #print(rules[i])\n",
    "                        #print(rules[j])\n",
    "                        assert rules[i][\"lhs\"][\"con\"] == rules[j][\"lhs\"][\"con\"]\n",
    "                        con = rules[i][\"lhs\"][\"con\"]\n",
    "                        first_sem_elements = [first_sem.pred, first_sem.args[0], first_sem.args[1]]\n",
    "                        second_sem_elements = [second_sem.pred, second_sem.args[0], second_sem.args[1]]\n",
    "\n",
    "                        if \"var\" in rules[i]:\n",
    "                            assignments = copy.deepcopy(rules[i][\"var\"])\n",
    "                        else:\n",
    "                            assignments = {}\n",
    "\n",
    "                        chuncked_rules.append(rules[i])\n",
    "                        chuncked_rules.append(rules[j])\n",
    "\n",
    "                        if diff_index_sem == 0:\n",
    "                            random_category = random.choice(VERB_CATS)\n",
    "                            var = Expression.fromstring(\"X\")\n",
    "                        else:\n",
    "                            # FIX: efficiency\n",
    "                            random_category = random.choice(NOUN_CATS)\n",
    "                            existing_variables = self.existing_variables(first_sem_elements)\n",
    "                            var = [var for var in INDIVIDUAL_VARIABLES if var not in existing_variables][0]\n",
    "                        print(\"New cat: \", random_category)\n",
    "                        first_sem_elements_abstracted = copy.deepcopy(first_sem_elements)\n",
    "                        first_sem_elements_abstracted[diff_index_sem] = var\n",
    "                        new_sem_str = f\"{str(first_sem_elements_abstracted[0])}({str(first_sem_elements_abstracted[1])},{str(first_sem_elements_abstracted[2])})\"\n",
    "                        # random_category = random.choice(NONTERMINALS)\n",
    "                        new_sen_str = replace_at_index(first_sentence, diff_index, random_category)\n",
    "                        assignments[str(var)] = diff_index\n",
    "                        assignments_sorted = sorted(assignments.items())\n",
    "                        asignments_sorted_dict = dict((var, idx) for var, idx in assignments_sorted)\n",
    "                        \n",
    "                        new_rule_0 = {\"lhs\": {\"cat\":\"S\", \"sem\": new_sem_str, \"con\":con}, \"rhs\": new_sen_str, \"var\":asignments_sorted_dict}\n",
    "                        \n",
    "                        assert self.is_well_formed(new_sem_str, new_sen_str), f\"Ill-formed rule generated: {new_rule_0}\"\n",
    "                        assert self.is_well_assigned(new_sen_str, asignments_sorted_dict), f\"Ill-assigned rule generated: {new_rule_0}\"\n",
    "\n",
    "                        new_rule_1 = {\"lhs\": {\"cat\":random_category, \"sem\": str(first_sem_elements[diff_index_sem])}, \"rhs\": first_sentence[diff_index]}\n",
    "                        new_rule_2 = {\"lhs\": {\"cat\":random_category, \"sem\": str(second_sem_elements[diff_index_sem])}, \"rhs\": second_sentence[diff_index]}\n",
    "                        #print(\"⌄⌄⌄ Chunk1 Results ⌄⌄⌄\")\n",
    "                        #print(new_rule_0)\n",
    "                        #print(new_rule_1)\n",
    "                        #print(new_rule_2)\n",
    "                        #print(\"⌃⌃⌃ Chunk1 Results ⌃⌃⌃\")\n",
    "                        new_rules += [new_rule_0] + [new_rule_1] + [new_rule_2]\n",
    "        rules = [rule for rule in rules if rule not in chuncked_rules]\n",
    "        rules = rules + new_rules\n",
    "        rules_unique = list(set([str(rule) for rule in rules]))\n",
    "        self.rules = [self.str2dict(rule) for rule in rules_unique]\n",
    "        self.rules = sorted(self.rules, key=lambda x: (-len(x['rhs']), x['rhs'], x['lhs']['cat']))\n",
    "\n",
    "    def chunk02(self):\n",
    "        rules = self.rules\n",
    "\n",
    "        chuncked_rules = []\n",
    "        new_rules = []\n",
    "\n",
    "        for i in range(len(rules)):\n",
    "            for j in range(i+1, len(rules)):\n",
    "                first_sem = Expression.fromstring(rules[i][\"lhs\"][\"sem\"])\n",
    "                second_sem = Expression.fromstring(rules[j][\"lhs\"][\"sem\"])\n",
    "                first_sentence = rules[i][\"rhs\"]\n",
    "                second_sentence = rules[j][\"rhs\"]\n",
    "                if isinstance(first_sem, ApplicationExpression) & isinstance(second_sem, ApplicationExpression):\n",
    "                    diff_index, diff_index_sem, can_chunk, upper_in_str, lower_in_str = self.can_chunk02(rules[i], rules[j])\n",
    "                    if can_chunk:\n",
    "                        assert rules[i][\"lhs\"][\"con\"] == rules[j][\"lhs\"][\"con\"]\n",
    "                        con = rules[i][\"lhs\"][\"con\"]\n",
    "                        first_sem_elements = [first_sem.pred, first_sem.args[0], first_sem.args[1]]\n",
    "                        second_sem_elements = [second_sem.pred, second_sem.args[0], second_sem.args[1]]\n",
    "\n",
    "                        target_position = [i,j][lower_in_str]\n",
    "                        nontarget_position = [i,j][upper_in_str]\n",
    "                        chuncked_rules.append(rules[target_position])\n",
    "\n",
    "                        target_sem = Expression.fromstring(rules[target_position][\"lhs\"][\"sem\"])\n",
    "                        target_sem_elements = [target_sem.pred, target_sem.args[0], target_sem.args[1]]\n",
    "                        target_sentence = rules[target_position][\"rhs\"]\n",
    "                        nontarget_sentence = rules[nontarget_position][\"rhs\"]\n",
    "                        new_rule = {\"lhs\": {\"cat\":nontarget_sentence[diff_index], \"sem\": str(target_sem_elements[diff_index_sem])}, \"rhs\": target_sentence[diff_index]}\n",
    "                        new_rules.append(new_rule)\n",
    "                        # print(new_rule)\n",
    "        rules = [rule for rule in rules if rule not in chuncked_rules]\n",
    "        rules = rules + new_rules\n",
    "        rules_unique = list(set([str(rule) for rule in rules]))\n",
    "        self.rules = [self.str2dict(rule) for rule in rules_unique]\n",
    "        self.rules = sorted(self.rules, key=lambda x: (-len(x['rhs']), x['rhs'], x['lhs']['cat']))\n",
    "        \n",
    "    def abstract(self, element, var, idx, diff_idx):\n",
    "        if idx == diff_idx:\n",
    "            return var\n",
    "        else:\n",
    "            return element\n",
    "    \n",
    "    def can_replace(self, rule1, rule2):\n",
    "        str1, str2 = rule1[\"rhs\"], rule2[\"rhs\"]\n",
    "        cat1, cat2 = rule1[\"lhs\"][\"cat\"], rule2[\"lhs\"][\"cat\"]\n",
    "        sem1_logic = Expression.fromstring(rule1[\"lhs\"][\"sem\"])\n",
    "        sem2_logic = Expression.fromstring(rule2[\"lhs\"][\"sem\"])\n",
    "        \n",
    "        if (cat1 == \"S\") and (cat2 != \"S\"):\n",
    "            sem1, sem2 = [sem1_logic.pred, sem1_logic.args[0], sem1_logic.args[1]], sem2_logic\n",
    "            same_positions_str = []\n",
    "            same_positions_sem = []\n",
    "            for i, char in enumerate(str1):\n",
    "                if str2 == char:\n",
    "                    same_positions_str.append(i)\n",
    "            for i, elm in enumerate(sem1):\n",
    "                if sem2 == elm:\n",
    "                    same_positions_sem.append(i)\n",
    "            if len(same_positions_str) == len(same_positions_sem) > 0:\n",
    "                return True, same_positions_str[0], same_positions_sem[0], 0, 1\n",
    "                # 0: non-word-rule, 1: word-rule\n",
    "            else:\n",
    "                return False, None, None, None, None\n",
    "        elif (cat2 == \"S\") and (cat1 != \"S\"):\n",
    "            sem2, sem1 = [sem2_logic.pred, sem2_logic.args[0], sem2_logic.args[1]], sem1_logic\n",
    "            same_positions_str = []\n",
    "            same_positions_sem = []\n",
    "            for i, char in enumerate(str2):\n",
    "                if str1 == char:\n",
    "                    same_positions_str.append(i)\n",
    "            for i, elm in enumerate(sem2):\n",
    "                if sem1 == elm:\n",
    "                    same_positions_sem.append(i)\n",
    "            if len(same_positions_str) == len(same_positions_sem) > 0:\n",
    "                return True, same_positions_str[0], same_positions_sem[0], 1, 0\n",
    "                # 1: non-word-rule, 0: word-rule\n",
    "            else:\n",
    "                return False, None, None, None, None\n",
    "        else:\n",
    "            return False, None, None, None, None\n",
    "\n",
    "    def replace(self):\n",
    "        rules = self.rules\n",
    "\n",
    "        replaced_rules = []\n",
    "        new_rules = []\n",
    "\n",
    "        for i in range(len(rules)):\n",
    "            for j in range(i+1, len(rules)):\n",
    "                # first_sem = Expression.fromstring(rules[i][\"lhs\"][\"sem\"])\n",
    "                # second_sem = Expression.fromstring(rules[j][\"lhs\"][\"sem\"])\n",
    "                first_sentence = rules[i][\"rhs\"]\n",
    "                second_sentence = rules[j][\"rhs\"]\n",
    "                # non_word_level, word_level = self.highlight_for_replace(first_sentence, second_sentence)\n",
    "                can_replace, same_index_str, same_index_sem, sentence_level, word_level = self.can_replace(rules[i], rules[j])\n",
    "                if can_replace:\n",
    "                    # print(rules[i])\n",
    "                    # print(rules[j])\n",
    "                    sentencerule_index = [i,j][sentence_level]\n",
    "                    wordrule_index = [i,j][word_level]\n",
    "                    sentencerule = rules[sentencerule_index]\n",
    "                    wordrule = rules[wordrule_index]\n",
    "                    replaced_rules.append(sentencerule)\n",
    "                    sentencerule_sem = Expression.fromstring(sentencerule[\"lhs\"][\"sem\"])\n",
    "                    sentencerule_sem_elms = [sentencerule_sem.pred, sentencerule_sem.args[0], sentencerule_sem.args[1]]\n",
    "                    \n",
    "                    if \"var\" in sentencerule:\n",
    "                        assignments = copy.deepcopy(sentencerule[\"var\"])\n",
    "                    else:\n",
    "                        assignments = {}\n",
    "                    \n",
    "                    if same_index_sem == 0:\n",
    "                        var = Expression.fromstring(\"X\")\n",
    "                    else:\n",
    "                        # FIX: efficiency\n",
    "                        # existing_variables = self.existing_variables(sentencerule_sem_elms)\n",
    "                        # print([var for var in INDIVIDUAL_VARIABLES if var not in existing_variables])\n",
    "                        var = [var for var in INDIVIDUAL_VARIABLES if var not in assignments][0]\n",
    "                    sentencerule_rhs = sentencerule[\"rhs\"]\n",
    "                    abstracted_sem_elms = []\n",
    "                    for i, elm in enumerate(sentencerule_sem_elms):\n",
    "                        if i == same_index_sem:\n",
    "                            abstracted_sem_elms.append(var)\n",
    "                        else:\n",
    "                            abstracted_sem_elms.append(sentencerule_sem_elms[i])\n",
    "                    abstracted_sem = f\"{str(abstracted_sem_elms[0])}({str(abstracted_sem_elms[1])},{abstracted_sem_elms[2]})\"\n",
    "\n",
    "                    sentencerule_con = sentencerule[\"lhs\"][\"con\"]\n",
    "\n",
    "                    assignments[str(var)] = same_index_str\n",
    "                    assignments_sorted = sorted(assignments.items())\n",
    "                    asignments_sorted_dict = dict((var, idx) for var, idx in assignments_sorted)\n",
    "\n",
    "                    wordrule_rhs = wordrule[\"rhs\"]\n",
    "                    wordrule_cat = wordrule[\"lhs\"][\"cat\"]\n",
    "                    abstracted_sentencerule_rhs = sentencerule_rhs.replace(wordrule_rhs, wordrule_cat)\n",
    "                    \n",
    "                    new_rule = {\"lhs\": {\"cat\":\"S\", \"sem\":abstracted_sem, \"con\":sentencerule_con}, \"rhs\": abstracted_sentencerule_rhs, \"var\":asignments_sorted_dict}\n",
    "                    \n",
    "                    assert self.is_well_formed(abstracted_sem, abstracted_sentencerule_rhs), f\"Ill-formed rule generated: {new_rule}\"\n",
    "                    assert self.is_well_assigned(abstracted_sentencerule_rhs, asignments_sorted_dict), f\"Ill-assigned rule generated: \\nBefore:\\t{sentencerule}\\n\\t{wordrule}\\nAfter: {new_rule}\"\n",
    "                    \n",
    "                    # print(\"Replace result: \", new_rule)\n",
    "                    new_rules.append(new_rule)\n",
    "        rules = [rule for rule in rules if rule not in replaced_rules]\n",
    "        rules = rules + new_rules\n",
    "        rules_unique = list(set([str(rule) for rule in rules]))\n",
    "        self.rules = [self.str2dict(rule) for rule in rules_unique]\n",
    "        self.rules = sorted(self.rules, key=lambda x: (-len(x['rhs']), x['rhs'], x['lhs']['cat']))\n",
    "    \n",
    "    def repaint_rule(self, rule, replacee, replacer):\n",
    "        print(f\"Before Merge: {rule}\")\n",
    "        # print(rule, replacee, replacer)\n",
    "        if rule[\"lhs\"][\"cat\"] == replacee[\"lhs\"][\"cat\"]:\n",
    "            assert len(rule[\"rhs\"]) == 1\n",
    "            # print(rule[\"lhs\"][\"cat\"], \"->\",replacer[\"lhs\"][\"cat\"])\n",
    "            rule[\"lhs\"][\"cat\"] = replacer[\"lhs\"][\"cat\"]\n",
    "        else:\n",
    "            rhs = copy.deepcopy(rule[\"rhs\"])\n",
    "            new_rhs = rhs.replace(replacee[\"lhs\"][\"cat\"], replacer[\"lhs\"][\"cat\"])\n",
    "            rule[\"rhs\"] = new_rhs\n",
    "            # print(rhs, \"->\", new_rhs)\n",
    "        assert self.is_well_formed(rule[\"lhs\"][\"sem\"], rule[\"rhs\"]), f\"Ill-formed rule generated: {rule}\"\n",
    "        if \"var\" in rule:\n",
    "            assert self.is_well_assigned(rule[\"rhs\"], rule[\"var\"]), f\"Ill-assigned rule generated: {rule}\"\n",
    "        print(f\"After Merge: {rule}\")\n",
    "        return rule\n",
    "    \n",
    "    def is_WordRule(self, rule):\n",
    "        assert type(rule) == dict\n",
    "        if rule[\"lhs\"][\"cat\"] == \"S\":\n",
    "            return False # SentenceRule\n",
    "        else:\n",
    "            return True # WordRule\n",
    "\n",
    "    def can_merge(self, rule1, rule2):\n",
    "        rule1_cat = copy.deepcopy(rule1[\"lhs\"][\"cat\"])\n",
    "        rule1_sem = copy.deepcopy(rule1[\"lhs\"][\"sem\"])\n",
    "        rule1_rhs = copy.deepcopy(rule1[\"rhs\"])\n",
    "        rule2_cat = copy.deepcopy(rule2[\"lhs\"][\"cat\"])\n",
    "        rule2_sem = copy.deepcopy(rule2[\"lhs\"][\"sem\"])\n",
    "        rule2_rhs = copy.deepcopy(rule2[\"rhs\"])\n",
    "        if self.is_WordRule(rule1) and self.is_WordRule(rule2):\n",
    "            if (rule1_sem == rule2_sem) & (rule1_rhs == rule2_rhs) & (rule1_cat != rule2_cat):\n",
    "                cats = [rule1_cat, rule2_cat]\n",
    "                random.shuffle(cats)\n",
    "                print(\"selected_cat: \", cats[0])\n",
    "                selected_cat = cats[0]\n",
    "                deleted_cat = cats[1]\n",
    "                assert (selected_cat.isupper()) and (deleted_cat.isupper())\n",
    "                return True, selected_cat, deleted_cat\n",
    "            else:\n",
    "                return False, None, None\n",
    "        else:\n",
    "            return False, None, None\n",
    "        \n",
    "    def merge(self):\n",
    "        for i in range(len(self.rules)):\n",
    "            for j in range(i+1, len(self.rules)):\n",
    "                can_merge, selected_cat, deleted_cat = self.can_merge(self.rules[i], self.rules[j])\n",
    "                if can_merge:\n",
    "                    for rule in self.rules:\n",
    "                        rule[\"lhs\"][\"cat\"] = rule[\"lhs\"][\"cat\"].replace(deleted_cat, selected_cat)\n",
    "                        rule[\"rhs\"] = rule[\"rhs\"].replace(deleted_cat, selected_cat)\n",
    "                        assert self.is_well_formed(rule[\"lhs\"][\"sem\"], rule[\"rhs\"]), f\"Ill-formed rule generated: {rule}\"\n",
    "                        if \"var\" in rule:\n",
    "                            assert self.is_well_assigned(rule[\"rhs\"], rule[\"var\"]), f\"Ill-assigned rule generated: {rule}\"\n",
    "        rules = self.rules\n",
    "        rules_unique = list(set([str(rule) for rule in rules]))\n",
    "        self.rules = [self.str2dict(rule) for rule in rules_unique]\n",
    "        self.rules = sorted(self.rules, key=lambda x: (-len(x['rhs']), x['rhs'], x['lhs']['cat']))\n",
    "\n",
    "    def leg_merge(self):\n",
    "        rules = self.rules\n",
    "        invited = []\n",
    "        replacer_cats = []\n",
    "        for i in range(len(rules)):\n",
    "            for j in range(i+1, len(rules)):\n",
    "                if self.can_merge(rules[i], rules[j]):\n",
    "                    # print(f\"can merge {rules[i]} and {rules[j]}\")\n",
    "                    if (rules[i][\"lhs\"][\"cat\"] not in replacer_cats) and (rules[j][\"lhs\"][\"cat\"] not in replacer_cats):\n",
    "                        indices = [i, j]\n",
    "                        # print(indices)\n",
    "                        random.shuffle(indices)\n",
    "                        replacer_rule = copy.deepcopy(rules[indices[0]])\n",
    "                        replacee_rule = copy.deepcopy(rules[indices[1]])\n",
    "                        replacer_cats.append(replacer_rule[\"lhs\"][\"cat\"])\n",
    "                        # print(\"replacee_rule: \", replacee_rule)\n",
    "                        # print(\"replacer_rule: \", replacer_rule)\n",
    "                        rules = [self.repaint_rule(rule, replacee_rule, replacer_rule) for rule in rules]\n",
    "                    elif (rules[i][\"lhs\"][\"cat\"] in replacer_cats) and (rules[j][\"lhs\"][\"cat\"] in replacer_cats):\n",
    "                        continue\n",
    "                    elif rules[i][\"lhs\"][\"cat\"] in replacer_cats:\n",
    "                        replacer_rule = copy.deepcopy(rules[i])\n",
    "                        replacee_rule = copy.deepcopy(rules[j])\n",
    "                        # print(\"replacee_rule: \", replacee_rule)\n",
    "                        # print(\"replacer_rule: \", replacer_rule)\n",
    "                        rules = [self.repaint_rule(rule, replacee_rule, replacer_rule) for rule in rules]\n",
    "                    else:\n",
    "                        replacer_rule = copy.deepcopy(rules[j])\n",
    "                        replacee_rule = copy.deepcopy(rules[i])\n",
    "                        # print(\"replacee_rule: \", replacee_rule)\n",
    "                        # print(\"replacer_rule: \", replacer_rule)\n",
    "                        rules = [self.repaint_rule(rule, replacee_rule, replacer_rule) for rule in rules]\n",
    "                else:\n",
    "                    continue\n",
    "        for rule in rules:\n",
    "            assert self.is_well_formed(rule[\"lhs\"][\"sem\"], rule[\"rhs\"]), f\"Ill-formed rule generated: {rule}\"\n",
    "            if \"var\" in rule:\n",
    "                assert self.is_well_assigned(rule[\"rhs\"], rule[\"var\"]), f\"Ill-assigned rule generated: {rule}\"\n",
    "        rules_unique = list(set([str(rule) for rule in rules]))\n",
    "        self.rules = [self.str2dict(rule) for rule in rules_unique]\n",
    "\n",
    "    def invent_wordrule(self, sem):\n",
    "        if str(sem).replace(\"_\",\"\") in NOUNS:\n",
    "            random_category = random.choice(NOUN_CATS)\n",
    "            rhs = generate_noun()\n",
    "        if str(sem).replace(\"_\",\"\") in (VERBS+VERBS_PSS):\n",
    "            random_category = random.choice(VERB_CATS)\n",
    "            rhs = generate_verb()\n",
    "        self.add_rule(f\"{random_category}/{str(sem)} -> {rhs}\")\n",
    "        return rhs\n",
    "\n",
    "    def invent_holisticrule(self, sem, query_con):\n",
    "        subj = generate_noun()\n",
    "        obj = generate_noun([subj])\n",
    "        verb = generate_verb()\n",
    "        sentence = f\"{subj}{verb}{obj}\"\n",
    "        self.add_rule(f\"S/{sem}/{query_con} -> {sentence}\")\n",
    "        return sentence\n",
    "    # TODO: test\n",
    "    def generate(self, query_str, query_con, debug=False):\n",
    "        rules = self.rules\n",
    "\n",
    "        if debug:\n",
    "            print(\"query: \", query_str)\n",
    "\n",
    "        query = Expression.fromstring(query_str)\n",
    "        query_elements = [query.args[0], query.pred, query.args[1]]\n",
    "\n",
    "        sem_list = self.sem_list()\n",
    "        sentence_list = self.sentence_list()\n",
    "\n",
    "        if (query_str in sem_list) and (rules[sem_list.index(query_str)]['lhs']['con']==query_con):\n",
    "            holistic_rule = rules[sem_list.index(query_str)]\n",
    "            holistic_rule_rhs = holistic_rule['rhs']\n",
    "            if debug:\n",
    "                print(\"generated by a holictic rule: \", f\"S/{str(query)}/{query_con} -> {holistic_rule_rhs}\")\n",
    "            return f\"S/{str(query)}/{query_con} -> {holistic_rule_rhs}\", \"by-holistic-rule\"\n",
    "        else:\n",
    "            for i, sem_str in enumerate(sem_list):\n",
    "                sem = Expression.fromstring(sem_str)\n",
    "                if isinstance(sem, ApplicationExpression):\n",
    "                    rule = rules[i]\n",
    "                    con = rule[\"lhs\"][\"con\"]\n",
    "                    sem_elements = [sem.args[0], sem.pred, sem.args[1]]\n",
    "                    matches = [(i, (query_element,sem_element)) for i, (query_element,sem_element) in enumerate(zip(query_elements, sem_elements)) if (query_element==sem_element) & (isinstance(query_element, ConstantExpression) & isinstance(sem_element, ConstantExpression))]\n",
    "                    slots = [(i, (query_element,sem_element)) for i, (query_element,sem_element) in enumerate(zip(query_elements, sem_elements)) if (query_element!=sem_element) & (not isinstance(sem_element, ConstantExpression))]\n",
    "                    if (len(matches)==2) & (len(slots)==1) & (query_con==con):\n",
    "                        sentence_with_slot = sentence_list[i]\n",
    "                        slot_position = slots[0][0]\n",
    "                        slot_sem = slots[0][1][0]\n",
    "                        slot_var_sem = slots[0][1][1]\n",
    "                        substitute_position = rule[\"var\"][str(slot_var_sem)]\n",
    "                        slot_category = sentence_with_slot[substitute_position]\n",
    "                        word_rules = [rule for rule in rules if (rule['lhs']['cat']==slot_category) & (rule['lhs']['sem']==str(slot_sem))]\n",
    "                        if len(word_rules) > 0:\n",
    "                            selected_rule = self.str2dict(str(random.sample(word_rules, 1)[0]))\n",
    "                            if substitute_position == 0:\n",
    "                                generated_sentence = selected_rule['rhs'] + sentence_with_slot[1:]\n",
    "                            if substitute_position == 1:\n",
    "                                generated_sentence = sentence_with_slot[0] + selected_rule['rhs'] + sentence_with_slot[2]\n",
    "                            if substitute_position == 2:\n",
    "                                generated_sentence = sentence_with_slot[:2] + selected_rule['rhs']\n",
    "                            assert generated_sentence.islower()\n",
    "                            return f\"S/{str(query)}/{query_con} -> {generated_sentence}\", \"by-composition\"\n",
    "                        if len(word_rules) == 0:\n",
    "                            invented_form = self.invent_wordrule(slot_sem)\n",
    "                            if substitute_position == 0:\n",
    "                                generated_sentence = invented_form + sentence_with_slot[1:]\n",
    "                            if substitute_position == 1:\n",
    "                                generated_sentence = sentence_with_slot[0] + invented_form + sentence_with_slot[2]\n",
    "                            if substitute_position == 2:\n",
    "                                generated_sentence = sentence_with_slot[:2] + invented_form\n",
    "                            assert generated_sentence.islower()\n",
    "                            return f\"S/{str(query)}/{query_con} -> {generated_sentence}\", \"by-word-invention\"\n",
    "                    if (len(matches)==1) & (len(slots)==2) & (query_con==con):\n",
    "                        sentence_with_slot = sentence_list[i]\n",
    "                        slot_position = slots[0][0]\n",
    "                        slot_sem = slots[0][1][0]\n",
    "                        slot_var_sem = slots[0][1][1]\n",
    "                        substitute_position = rule[\"var\"][str(slot_var_sem)]\n",
    "                        slot_category = sentence_with_slot[substitute_position]\n",
    "                        word_rules = [rule for rule in rules if (rule['lhs']['cat']==slot_category) & (rule['lhs']['sem']==str(slot_sem))]\n",
    "                        if len(word_rules) > 0:\n",
    "                            selected_rule = self.str2dict(str(random.sample(word_rules, 1)[0]))\n",
    "                            if substitute_position == 0:\n",
    "                                new_sentence_with_slot = selected_rule['rhs'] + sentence_with_slot[1:]\n",
    "                            if substitute_position == 1:\n",
    "                                new_sentence_with_slot = sentence_with_slot[0] + selected_rule['rhs'] + sentence_with_slot[2]\n",
    "                            if substitute_position == 2:\n",
    "                                new_sentence_with_slot = sentence_with_slot[:2] + selected_rule['rhs']\n",
    "                            strategy = \"by-composition\"\n",
    "                        if len(word_rules) == 0:\n",
    "                            invented_form = self.invent_wordrule(slot_sem)\n",
    "                            if substitute_position == 0:\n",
    "                                new_sentence_with_slot = invented_form + sentence_with_slot[1:]\n",
    "                            if substitute_position == 1:\n",
    "                                new_sentence_with_slot = sentence_with_slot[0] + invented_form + sentence_with_slot[2]\n",
    "                            if substitute_position == 2:\n",
    "                                new_sentence_with_slot = sentence_with_slot[:2] + invented_form\n",
    "                            strategy = \"by-word-invention\"\n",
    "                        slot_position = slots[1][0]\n",
    "                        slot_sem = slots[1][1][0]\n",
    "                        slot_var_sem = slots[1][1][1]\n",
    "                        substitute_position = rule[\"var\"][str(slot_var_sem)]\n",
    "                        slot_category = sentence_with_slot[substitute_position]\n",
    "                        word_rules = [rule for rule in rules if (rule['lhs']['cat']==slot_category) & (rule['lhs']['sem']==str(slot_sem))]\n",
    "                        if len(word_rules) > 0:\n",
    "                            selected_rule = self.str2dict(str(random.sample(word_rules, 1)[0]))\n",
    "                            if substitute_position == 0:\n",
    "                                generated_sentence = selected_rule['rhs'] + new_sentence_with_slot[1:]\n",
    "                            if substitute_position == 1:\n",
    "                                generated_sentence = new_sentence_with_slot[0] + selected_rule['rhs'] + new_sentence_with_slot[2]\n",
    "                            if substitute_position == 2:\n",
    "                                generated_sentence = new_sentence_with_slot[:2] + selected_rule['rhs']\n",
    "                            assert generated_sentence.islower()\n",
    "                            if strategy == \"by-composition\":\n",
    "                                return f\"S/{str(query)}/{query_con} -> {generated_sentence}\", strategy\n",
    "                            else:\n",
    "                                return f\"S/{str(query)}/{query_con} -> {generated_sentence}\", strategy\n",
    "                        if len(word_rules) == 0:\n",
    "                            if strategy == \"by-composition\":\n",
    "                                invented_form = self.invent_wordrule(slot_sem)\n",
    "                                if substitute_position == 0:\n",
    "                                    generated_sentence = invented_form + new_sentence_with_slot[1:]\n",
    "                                if substitute_position == 1:\n",
    "                                    generated_sentence = new_sentence_with_slot[0] + invented_form + new_sentence_with_slot[2]\n",
    "                                if substitute_position == 2:\n",
    "                                    generated_sentence = new_sentence_with_slot[:2] + invented_form\n",
    "                                assert generated_sentence.islower(), query_str\n",
    "                                strategy = \"by-word-invention\"\n",
    "                                return f\"S/{str(query)}/{query_con} -> {generated_sentence}\", strategy\n",
    "                            else:\n",
    "                                self.rules = self.rules[:-1]\n",
    "                                generated_sentence = self.invent_holisticrule(query, query_con)\n",
    "                                assert generated_sentence.islower()\n",
    "                                return f\"S/{str(query)}/{query_con} -> {generated_sentence}\", \"by-holistic-invention\"\n",
    "                    if (len(matches)==0) & (len(slots)==3) & (query_con==con):\n",
    "                        sentence_with_slot = sentence_list[i]\n",
    "                        slot_position = slots[0][0]\n",
    "                        slot_sem = slots[0][1][0]\n",
    "                        slot_var_sem = slots[0][1][1]\n",
    "                        substitute_position = rule[\"var\"][str(slot_var_sem)]\n",
    "                        slot_category = sentence_with_slot[substitute_position]\n",
    "                        word_rules = [rule for rule in rules if (rule['lhs']['cat']==slot_category) & (rule['lhs']['sem']==str(slot_sem))]\n",
    "                        if len(word_rules) > 0:\n",
    "                            selected_rule = self.str2dict(str(random.sample(word_rules, 1)[0]))\n",
    "                            if substitute_position == 0:\n",
    "                                new_sentence_with_slot = selected_rule['rhs'] + sentence_with_slot[1:]\n",
    "                            if substitute_position == 1:\n",
    "                                new_sentence_with_slot = sentence_with_slot[0] + selected_rule['rhs'] + sentence_with_slot[2]\n",
    "                            if substitute_position == 2:\n",
    "                                new_sentence_with_slot = sentence_with_slot[:2] + selected_rule['rhs']\n",
    "                            strategy = \"by-composition\"\n",
    "                        if len(word_rules) == 0:\n",
    "                            invented_form = self.invent_wordrule(slot_sem)\n",
    "                            if substitute_position == 0:\n",
    "                                new_sentence_with_slot = invented_form + sentence_with_slot[1:]\n",
    "                            if substitute_position == 1:\n",
    "                                new_sentence_with_slot = sentence_with_slot[0] + invented_form + sentence_with_slot[2]\n",
    "                            if substitute_position == 2:\n",
    "                                new_sentence_with_slot = sentence_with_slot[:2] + invented_form\n",
    "                            strategy = \"by-word-invention\"\n",
    "                        slot_sem = slots[1][1][0]\n",
    "                        slot_var_sem = slots[1][1][1]\n",
    "                        substitute_position = rule[\"var\"][str(slot_var_sem)]\n",
    "                        slot_category = sentence_with_slot[substitute_position]\n",
    "                        word_rules = [rule for rule in rules if (rule['lhs']['cat']==slot_category) & (rule['lhs']['sem']==str(slot_sem))]\n",
    "                        if len(word_rules) > 0:\n",
    "                            selected_rule = self.str2dict(str(random.sample(word_rules, 1)[0]))\n",
    "                            if substitute_position == 0:\n",
    "                                new_sentence_with_slot = selected_rule['rhs'] + new_sentence_with_slot[1:]\n",
    "                            if substitute_position == 1:\n",
    "                                new_sentence_with_slot = new_sentence_with_slot[0] + selected_rule['rhs'] + new_sentence_with_slot[2]\n",
    "                            if substitute_position == 2:\n",
    "                                new_sentence_with_slot = new_sentence_with_slot[:2] + selected_rule['rhs']\n",
    "                        if len(word_rules) == 0:\n",
    "                            if strategy == \"by-composition\":\n",
    "                                invented_form = self.invent_wordrule(slot_sem)\n",
    "                                if substitute_position == 0:\n",
    "                                    new_sentence_with_slot = invented_form + new_sentence_with_slot[1:]\n",
    "                                if substitute_position == 1:\n",
    "                                    new_sentence_with_slot = new_sentence_with_slot[0] + invented_form + new_sentence_with_slot[2]\n",
    "                                if substitute_position == 2:\n",
    "                                    new_sentence_with_slot = new_sentence_with_slot[:2] + invented_form\n",
    "                                strategy = \"by-word-invention\"\n",
    "                            else:\n",
    "                                self.rules = self.rules[:-1]\n",
    "                                generated_sentence = self.invent_holisticrule(query, query_con)\n",
    "                                assert generated_sentence.islower()\n",
    "                                return f\"S/{str(query)}/{query_con} -> {generated_sentence}\", \"by-holistic-invention\"\n",
    "                        slot_position = slots[2][0]\n",
    "                        slot_sem = slots[2][1][0]\n",
    "                        slot_var_sem = slots[2][1][1]\n",
    "                        substitute_position = rule[\"var\"][str(slot_var_sem)]\n",
    "                        slot_category = sentence_with_slot[substitute_position]\n",
    "                        word_rules = [rule for rule in rules if (rule['lhs']['cat']==slot_category) & (rule['lhs']['sem']==str(slot_sem))]\n",
    "                        if len(word_rules) > 0:\n",
    "                            selected_rule = self.str2dict(str(random.sample(word_rules, 1)[0]))\n",
    "                            if substitute_position == 0:\n",
    "                                generated_sentence = selected_rule['rhs'] + new_sentence_with_slot[1:]\n",
    "                            if substitute_position == 1:\n",
    "                                generated_sentence = new_sentence_with_slot[0] + selected_rule['rhs'] + new_sentence_with_slot[2]\n",
    "                            if substitute_position == 2:\n",
    "                                generated_sentence = new_sentence_with_slot[:2] + selected_rule['rhs']\n",
    "                            assert generated_sentence.islower()\n",
    "                            return f\"S/{str(query)}/{query_con} -> {generated_sentence}\", strategy\n",
    "                        if len(word_rules) == 0:\n",
    "                            if strategy == \"by-composition\":\n",
    "                                invented_form = self.invent_wordrule(slot_sem)\n",
    "                                if substitute_position == 0:\n",
    "                                    generated_sentence = invented_form + new_sentence_with_slot[1:]\n",
    "                                if substitute_position == 1:\n",
    "                                    generated_sentence = new_sentence_with_slot[0] + invented_form + new_sentence_with_slot[2]\n",
    "                                if substitute_position == 2:\n",
    "                                    generated_sentence = new_sentence_with_slot[:2] + invented_form\n",
    "                                strategy = \"by-word-invention\"\n",
    "                                assert generated_sentence.islower()\n",
    "                                return f\"S/{str(query)}/{query_con} -> {generated_sentence}\", strategy\n",
    "                            else:\n",
    "                                self.rules = self.rules[:-1]\n",
    "                                generated_sentence = self.invent_holisticrule(query, query_con)\n",
    "                                assert generated_sentence.islower()\n",
    "                                return f\"S/{str(query)}/{query_con} -> {generated_sentence}\", \"by-holistic-invention\"\n",
    "                    else:\n",
    "                        continue\n",
    "                else:\n",
    "                    continue\n",
    "            generated_sentence = self.invent_holisticrule(query, query_con)\n",
    "            assert generated_sentence.islower()\n",
    "            return f\"S/{str(query)}/{query_con} -> {generated_sentence}\", \"by-holistic-invention\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b700a1a7-8410-4877-825a-b6816e54cae6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grammar = Grammar()\n",
    "grammar.from_string(\"S/_admire(_alice,_carol)/0 -> afc\\nS/_admire(_john,_carol)/0 -> bfc\\nS/_know(_carol,_john)/0 -> cgb\\nR/_know -> g\\nA/_john -> b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ea37044-66d4-43e4-b851-cf8c5d6fbe40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(grammar.rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef417f63-833e-4d0e-9c40-d827dac2a8d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S/_admire(_alice,_carol)/0 -> afc\n",
      "S/_admire(_john,_carol)/0 -> bfc\n",
      "S/_know(_carol,_john)/0 -> cgb\n",
      "A/_john -> b\n",
      "R/_know -> g\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(grammar.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a82a982d-7069-4415-a024-9ef1729e48d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New cat:  V\n"
     ]
    }
   ],
   "source": [
    "grammar.chunk01()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d514c7b2-3020-4606-991d-f1af8e488d77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S/_admire(x,_carol)/0 -> Vfc\t(x:0)\n",
      "S/_know(_carol,_john)/0 -> cgb\n",
      "V/_alice -> a\n",
      "A/_john -> b\n",
      "R/_know -> g\n",
      "V/_john -> b\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(grammar.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80aa70c4-6722-4565-8173-c1b753032432",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected_cat:  V\n"
     ]
    }
   ],
   "source": [
    "grammar.merge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d177987-b4f4-4b41-9dc9-17e8a5e94c03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S/_admire(x,_carol)/0 -> Vfc\t(x:0)\n",
      "S/_know(_carol,_john)/0 -> cgb\n",
      "V/_alice -> a\n",
      "R/_know -> g\n",
      "V/_john -> b\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(grammar.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "073b244d-7499-422f-adcc-7b071cbe0167",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grammar.chunk02()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8614f71-6e92-4b37-b34a-57b1addb7515",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S/_admire(x,_carol)/0 -> Vfc\t(x:0)\n",
      "S/_know(_carol,_john)/0 -> cgb\n",
      "V/_alice -> a\n",
      "R/_know -> g\n",
      "V/_john -> b\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(grammar.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ed27ece-57f2-4df3-ac01-94443ed99509",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grammar.replace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e3d7c4e-eb97-4ac8-bb35-412968e153ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S/_admire(x,_carol)/0 -> Vfc\t(x:0)\n",
      "S/X(_carol,x)/0 -> cRV\t(X:1, x:2)\n",
      "V/_alice -> a\n",
      "R/_know -> g\n",
      "V/_john -> b\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(grammar.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2409e6eb-dec3-48cb-aa14-4d04481d96e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grammar.replace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e99821b8-c845-43e8-a373-9e75ea0f389e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S/_admire(x,_carol)/0 -> Vfc\t(x:0)\n",
      "S/X(_carol,x)/0 -> cRV\t(X:1, x:2)\n",
      "V/_alice -> a\n",
      "R/_know -> g\n",
      "V/_john -> b\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(grammar.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7730e09e-e427-4c2a-bbdf-3da6697fa75b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grammar.merge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e1a6f0b-1e56-4c1e-a019-5c9de2e0005b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S/_admire(x,_carol)/0 -> Vfc\t(x:0)\n",
      "S/X(_carol,x)/0 -> cRV\t(X:1, x:2)\n",
      "V/_alice -> a\n",
      "R/_know -> g\n",
      "V/_john -> b\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(grammar.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9431fd05-a6ec-448e-a3bf-6f88caf9a217",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('S/_know(_carol,_john)/0 -> cgb', 'by-composition')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grammar.generate(\"_know(_carol,_john)\", 0) #cgb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2a0d9e99-97da-4a41-8088-9eb8e1652ee6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('S/_admire(_alice,_john)/0 -> bma', 'by-holistic-invention')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grammar.generate(\"_admire(_alice,_john)\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e7fbdd17-70c9-456a-86e0-aa5a4e7b3d8f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('S/_admire(_alice,_carol)/0 -> afc', 'by-composition')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grammar.generate(\"_admire(_alice,_carol)\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6c389a-dd9b-488a-9462-4786b7487766",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a3d9e467-f4d7-4525-8543-617bd78386b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'random_category' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m grammar\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_admire(_,_carol)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 722\u001b[0m, in \u001b[0;36mGrammar.generate\u001b[0;34m(self, query_str, query_con, debug)\u001b[0m\n\u001b[1;32m    720\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mS/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(query)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery_con\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgenerated_sentence\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby-composition\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(word_rules) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 722\u001b[0m     invented_form \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvent_wordrule(slot_sem)\n\u001b[1;32m    723\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m substitute_position \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    724\u001b[0m         generated_sentence \u001b[38;5;241m=\u001b[39m invented_form \u001b[38;5;241m+\u001b[39m sentence_with_slot[\u001b[38;5;241m1\u001b[39m:]\n",
      "Cell \u001b[0;32mIn[2], line 665\u001b[0m, in \u001b[0;36mGrammar.invent_wordrule\u001b[0;34m(self, sem)\u001b[0m\n\u001b[1;32m    663\u001b[0m     random_category \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice(VERB_CATS)\n\u001b[1;32m    664\u001b[0m     rhs \u001b[38;5;241m=\u001b[39m generate_verb()\n\u001b[0;32m--> 665\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_rule(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrandom_category\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(sem)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrhs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rhs\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'random_category' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "grammar.generate(\"_admire(_,_carol)\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cafb7de-3e1a-479a-95bc-99350edfd8fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
