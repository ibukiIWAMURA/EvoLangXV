{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2131061-7a3b-4cc6-ba90-5c97ca0fcc15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.sem.logic import *\n",
    "import ast\n",
    "import random\n",
    "import string\n",
    "import copy\n",
    "\n",
    "NOUNS = [\n",
    "    \"david\", \"alice\", \"bob\", \"carol\", \"eve\", \"frank\", \"grace\", \"helen\", \n",
    "    \"ivan\", \"jack\", \"karen\", \"larry\", \"mike\", \"nina\", \"oscar\", \"paul\", \n",
    "    \"quincy\", \"rachel\", \"steve\", \"tracy\", \"ursula\", \"victor\", \"wendy\", \n",
    "    \"xander\", \"yasmine\", \"zach\"\n",
    "]\n",
    "\n",
    "VERBS = [\n",
    "    \"know\", \"admire\", \"like\", \"kick\", \"meet\", \"call\", \"defend\", \"encourage\"\n",
    "    , \"follow\", \"greet\", \"help\", \"invite\", \"judge\", \"notice\", \"obey\", \n",
    "    \"praise\", \"question\", \"respect\", \"support\", \"trust\", \"understand\", \n",
    "    \"value\", \"warn\", \"x-ray\", \"yell\", \"zap\"\n",
    "]\n",
    "\n",
    "VERBS_PSS = [\n",
    "    \"known\", \"admired\", \"liked\", \"kicked\", \"met\"\n",
    "]\n",
    "\n",
    "def generate_noun(n_sorts: str, stopword: str = \"\"):\n",
    "    assert n_sorts <= len(string.ascii_lowercase)\n",
    "    chars_wo_stopword = [char for char in string.ascii_lowercase[:n_sorts] if char not in stopword]\n",
    "    return random.choice(chars_wo_stopword)\n",
    "\n",
    "def generate_verb(n_sorts: str, stopword: str = \"\"):\n",
    "    english_subset = list(string.ascii_lowercase)[5:15]  # 'f'から'o'までの文字を含むリストを生成\n",
    "    chars_wo_stopword = [char for char in english_subset if char not in stopword]\n",
    "    return random.choice(chars_wo_stopword)\n",
    "\n",
    "def generate_noun_sem(n_sorts: str, stopword: str = \"\"):\n",
    "    assert n_sorts <= len(NOUNS)\n",
    "    chars_wo_stopword = [char for char in NOUNS[:n_sorts] if char not in stopword]\n",
    "    return random.choice(chars_wo_stopword)\n",
    "\n",
    "def generate_verb_sem(n_sorts: str, stopword: str = \"\"):\n",
    "    chars_wo_stopword = [char for char in (VERBS[:n_sorts]+VERBS_PSS[:n_sorts]) if char not in stopword]\n",
    "    return random.choice(chars_wo_stopword)\n",
    "\n",
    "def replace_at_index(s, index, replacement):\n",
    "    return s[:index] + replacement + s[index + 1:]\n",
    "\n",
    "N_SORTS = 5\n",
    "NONTERMINALS = [char for char in string.ascii_uppercase if char != \"S\"]\n",
    "VERB_CATS = NONTERMINALS[:10]\n",
    "NOUN_CATS = NONTERMINALS[10:]\n",
    "INDIVIDUAL_VARIABLES = [\"x\", \"y\"]\n",
    "FUNCTIONAL_VARIABLES = [\"X\"]\n",
    "ENGLISH_LOWER = [char for char in string.ascii_lowercase[:N_SORTS] if char not in [\"x\", \"y\"]]\n",
    "\n",
    "start_index = N_SORTS\n",
    "end_index = start_index + N_SORTS\n",
    "VERB_LOWER = [char for char in string.ascii_lowercase[:start_index] if char not in [\"x\", \"y\"]]\n",
    "\n",
    "class Grammar():\n",
    "    def __init__(self):\n",
    "        self.rules = []\n",
    "\n",
    "    def from_string(self, string: str):\n",
    "        self.rules = []\n",
    "        rules_str = string.split(\"\\n\")\n",
    "        for rule_str in rules_str:\n",
    "            if rule_str != \"\\n\":\n",
    "                if len(rule_str.split(\"\\t\")) == 2:\n",
    "                    rule_body, assignments = rule_str.split(\"\\t\")[0], rule_str.split(\"\\t\")[1]\n",
    "                    assignments = assignments.replace(\"(\", \"\").replace(\")\", \"\")\n",
    "                    asignments_list = assignments.split(\", \")\n",
    "                    asignments_dict = {str(assig.split(\":\")[0]):int(assig.split(\":\")[1]) for assig in asignments_list}\n",
    "                    assignments_sorted = sorted(asignments_dict.items())\n",
    "                    asignments_sorted_dict = dict((var, idx) for var, idx in assignments_sorted)\n",
    "                    rule = rule_body.split(\" \")\n",
    "                    lhs = rule[0].split(\"/\")\n",
    "                    rhs = rule[2]\n",
    "                    lhs_cat = lhs[0]\n",
    "                    lhs_sem = lhs[1]\n",
    "                    lhs_con = int(lhs[2])\n",
    "                    rule = {'lhs':{'cat':lhs_cat, 'sem':lhs_sem, 'con':lhs_con}, 'rhs':rhs, 'var':asignments_sorted_dict}\n",
    "                    self.rules.append(rule)\n",
    "                if len(rule_str.split(\"\\t\")) == 1:\n",
    "                    rule = rule_str.split(\" \")\n",
    "                    lhs = rule[0].split(\"/\")\n",
    "                    rhs = rule[2]\n",
    "                    if len(lhs) == 3:\n",
    "                        lhs_cat = lhs[0]\n",
    "                        lhs_sem = lhs[1]\n",
    "                        lhs_con = int(lhs[2])\n",
    "                        rule = {'lhs':{'cat':lhs_cat, 'sem':lhs_sem, 'con':lhs_con}, 'rhs':rhs}\n",
    "                    if len(lhs) == 2:\n",
    "                        lhs_cat = lhs[0]\n",
    "                        lhs_sem = lhs[1]\n",
    "                        rule = {'lhs':{'cat':lhs_cat, 'sem':lhs_sem}, 'rhs':rhs}\n",
    "                    self.rules.append(rule)\n",
    "    \n",
    "    def add_rule(self, string: str):\n",
    "        rules_str = string.split(\"\\n\")\n",
    "        for rule_str in rules_str:\n",
    "            if rule_str != \"\\n\":\n",
    "                if len(rule_str.split(\"\\t\")) == 2:\n",
    "                    rule_body, assignments = rule_str.split(\"\\t\")[0], rule_str.split(\"\\t\")[1]\n",
    "                    assignments = assignments.replace(\"(\", \"\").replace(\")\", \"\")\n",
    "                    asignments_list = assignments.split(\", \")\n",
    "                    asignments_dict = {str(assig.split(\":\")[0]):int(assig.split(\":\")[1]) for assig in asignments_list}\n",
    "                    assignments_sorted = sorted(asignments_dict.items())\n",
    "                    asignments_sorted_dict = dict((var, idx) for var, idx in assignments_sorted)\n",
    "                    rule = rule_body.split(\" \")\n",
    "                    lhs = rule[0].split(\"/\")\n",
    "                    rhs = rule[2]\n",
    "                    lhs_cat = lhs[0]\n",
    "                    lhs_sem = lhs[1]\n",
    "                    lhs_con = int(lhs[2])\n",
    "                    rule = {'lhs':{'cat':lhs_cat, 'sem':lhs_sem, 'con':lhs_con}, 'rhs':rhs, 'var':asignments_sorted_dict}\n",
    "                    self.rules.append(rule)\n",
    "                if len(rule_str.split(\"\\t\")) == 1:\n",
    "                    rule = rule_str.split(\" \")\n",
    "                    lhs = rule[0].split(\"/\")\n",
    "                    rhs = rule[2]\n",
    "                    if len(lhs) == 3:\n",
    "                        lhs_cat = lhs[0]\n",
    "                        lhs_sem = lhs[1]\n",
    "                        lhs_con = int(lhs[2])\n",
    "                        rule = {'lhs':{'cat':lhs_cat, 'sem':lhs_sem, 'con':lhs_con}, 'rhs':rhs}\n",
    "                    if len(lhs) == 2:\n",
    "                        lhs_cat = lhs[0]\n",
    "                        lhs_sem = lhs[1]\n",
    "                        rule = {'lhs':{'cat':lhs_cat, 'sem':lhs_sem}, 'rhs':rhs}\n",
    "                    self.rules.append(rule)\n",
    "    \n",
    "    def to_string(self):\n",
    "        rules_list = []\n",
    "        rules_str = \"\"\n",
    "        for rule in self.rules:\n",
    "            if 'var' in rule:\n",
    "                asigs_str = \"(\" + \", \".join(f\"{k}:{v}\" for k, v in rule['var'].items()) + \")\"\n",
    "                rules_list.append(f\"{rule['lhs']['cat']}/{rule['lhs']['sem']}/{rule['lhs']['con']} -> {rule['rhs']}\\t{asigs_str}\\n\")\n",
    "            elif 'con' in rule['lhs']:\n",
    "                rules_list.append(f\"{rule['lhs']['cat']}/{rule['lhs']['sem']}/{rule['lhs']['con']} -> {rule['rhs']}\\n\")\n",
    "            else:\n",
    "                rules_list.append(f\"{rule['lhs']['cat']}/{rule['lhs']['sem']} -> {rule['rhs']}\\n\")\n",
    "        sorted_list = sorted(sorted(rules_list), key=lambda x: -len(x))\n",
    "        rules_str += ''.join(sorted_list)+\"\\n\"\n",
    "        return rules_str\n",
    "    \n",
    "    def str2dict(self, string):\n",
    "        return ast.literal_eval(string)\n",
    "    \n",
    "    def sem_list(self):\n",
    "        return [d['lhs']['sem'] for d in self.rules]\n",
    "    \n",
    "    def cat_list(self):\n",
    "        return [d['lhs']['cat'] for d in self.rules]\n",
    "    \n",
    "    def sentence_list(self):\n",
    "        return [d['rhs'] for d in self.rules]\n",
    "    \n",
    "    def is_well_formed(self, expr_str, form):\n",
    "        expr = Expression.fromstring(expr_str)\n",
    "        num_var_expr = len([term for term in expr.free()])\n",
    "        num_uppercase = sum(1 for char in form if char.isupper())\n",
    "        return num_var_expr == num_uppercase\n",
    "\n",
    "    def is_well_assigned(self, form, assignments):\n",
    "        num_uppercase = sum(1 for char in form if char.isupper())\n",
    "        num_assigs = len(assignments)\n",
    "        return num_assigs == num_uppercase\n",
    "\n",
    "    def can_chunk01(self, rule1, rule2):\n",
    "        str1, str2 = rule1[\"rhs\"], rule2[\"rhs\"]\n",
    "        sem1_logic = Expression.fromstring(rule1[\"lhs\"][\"sem\"])\n",
    "        sem2_logic = Expression.fromstring(rule2[\"lhs\"][\"sem\"])\n",
    "        con1 = rule1[\"lhs\"][\"con\"]\n",
    "        con2 = rule2[\"lhs\"][\"con\"]\n",
    "\n",
    "        if len(str1) != 3 or len(str2) != 3: # TODO: Better to judge by category (not sentence length)\n",
    "            return None, None, False\n",
    "        if con1 != con2:\n",
    "            return None, None, False\n",
    "        sem1, sem2 = [sem1_logic.pred, sem1_logic.args[0], sem1_logic.args[1]], [sem2_logic.pred, sem2_logic.args[0], sem2_logic.args[1]]\n",
    "        # print(str1, str2)\n",
    "        diff_count_str = 0\n",
    "        diff_positions_str = []\n",
    "        for i, (char1, char2) in enumerate(zip(str1, str2)):\n",
    "            if (char1 != char2):\n",
    "                # diff_count_str += 1\n",
    "                diff_positions_str.append(i)\n",
    "        diff_count_sem = 0\n",
    "        diff_positions_sem = []\n",
    "        for i, (elm1, elm2) in enumerate(zip(sem1, sem2)):\n",
    "            if (elm1 != elm2):\n",
    "                # diff_count_sem += 1\n",
    "                diff_positions_sem.append(i)\n",
    "        if (len(diff_positions_str) == len(diff_positions_sem) == 1) and ((isinstance(sem1[diff_positions_sem[0]], ConstantExpression)) and (isinstance(sem2[diff_positions_sem[0]], ConstantExpression))):\n",
    "            if str1[diff_positions_str[0]].islower() and str2[diff_positions_str[0]].islower():\n",
    "                return diff_positions_str[0], diff_positions_sem[0], True\n",
    "            else:\n",
    "                return None, None, False\n",
    "        else:\n",
    "            return None, None, False\n",
    "\n",
    "    def find_diff_position_for_chunk01(self, str1, str2):\n",
    "        if len(str1) != 3 or len(str2) != 3:\n",
    "            return None\n",
    "\n",
    "        diff_count = 0\n",
    "        diff_index = None\n",
    "        for index, (char1, char2) in enumerate(zip(str1, str2)):\n",
    "            if char1 != char2:\n",
    "                diff_count += 1\n",
    "                diff_index = index\n",
    "                if not (char1.islower() and char2.islower()):\n",
    "                    return None\n",
    "\n",
    "        if diff_count == 1:\n",
    "            if diff_index == 0:\n",
    "                diff_index_sem = 1\n",
    "            if diff_index == 1:\n",
    "                diff_index_sem = 0\n",
    "            if diff_index == 2:\n",
    "                diff_index_sem = 2\n",
    "            return diff_index, diff_index_sem\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def can_chunk02(self, rule1, rule2):\n",
    "        str1, str2 = rule1[\"rhs\"], rule2[\"rhs\"]\n",
    "        sem1_logic = Expression.fromstring(rule1[\"lhs\"][\"sem\"])\n",
    "        sem2_logic = Expression.fromstring(rule2[\"lhs\"][\"sem\"])\n",
    "        sem1, sem2 = [sem1_logic.pred, sem1_logic.args[0], sem1_logic.args[1]], [sem2_logic.pred, sem2_logic.args[0], sem2_logic.args[1]]\n",
    "        con1 = rule1[\"lhs\"][\"con\"]\n",
    "        con2 = rule2[\"lhs\"][\"con\"]\n",
    "\n",
    "        if len(str1) != 3 or len(str2) != 3: # TODO: Better to judge by category (not sentence length)\n",
    "            return None, None, False, None, None\n",
    "        if con1 != con2:\n",
    "            return None, None, False, None, None\n",
    "        # print(str1, str2)\n",
    "        diff_count_str = 0\n",
    "        diff_positions_str = []\n",
    "        for i, (char1, char2) in enumerate(zip(str1, str2)):\n",
    "            if (char1 != char2):\n",
    "                # if (char1.islower() and char2.isupper()):\n",
    "                diff_count_str += 1\n",
    "                diff_positions_str.append(i)\n",
    "                # elif (char1.isupper() and char2.islower()):\n",
    "                # else:\n",
    "                #    None, False, None, None\n",
    "        diff_count_sem = 0\n",
    "        diff_positions_sem = []\n",
    "        for i, (elm1, elm2) in enumerate(zip(sem1, sem2)):\n",
    "            if (elm1 != elm2):\n",
    "                diff_count_sem += 1\n",
    "                diff_positions_sem.append(i)\n",
    "        if len(diff_positions_str) == len(diff_positions_sem) == 1:\n",
    "            if (str1[diff_positions_str[0]].islower() and str2[diff_positions_str[0]].isupper()):\n",
    "                upper_in_str, lower_in_str = 1, 0\n",
    "                return diff_positions_str[0], diff_positions_sem[0], True, upper_in_str, lower_in_str\n",
    "            elif (str1[diff_positions_str[0]].isupper() and str2[diff_positions_str[0]].islower()):\n",
    "                upper_in_str, lower_in_str = 0, 1\n",
    "                return diff_positions_str[0], diff_positions_sem[0], True, upper_in_str, lower_in_str\n",
    "            else:\n",
    "                return None, None, False, None, None\n",
    "        else:\n",
    "            return None, None, False, None, None\n",
    "    \n",
    "    def find_diff_position_for_chunk02(self, str1, str2):\n",
    "        if len(str1) != 3 or len(str2) != 3:\n",
    "            return None, None\n",
    "\n",
    "        diff_count = 0\n",
    "        diff_index = None\n",
    "        upper_in_str = None\n",
    "\n",
    "        for index, (char1, char2) in enumerate(zip(str1, str2)):\n",
    "            if char1 != char2:\n",
    "                diff_count += 1\n",
    "                diff_index = index\n",
    "                if (char1.islower() and char2.isupper()):\n",
    "                    lower_in_str = 0\n",
    "                    upper_in_str = 1\n",
    "                elif (char1.isupper() and char2.islower()):\n",
    "                    lower_in_str = 1\n",
    "                    upper_in_str = 0\n",
    "                else:\n",
    "                    return None, None\n",
    "\n",
    "        if diff_count == 1:\n",
    "            if diff_index == 0:\n",
    "                diff_index_sem = 1\n",
    "            if diff_index == 1:\n",
    "                diff_index_sem = 0\n",
    "            if diff_index == 2:\n",
    "                diff_index_sem = 2\n",
    "            return diff_index, diff_index_sem, upper_in_str, lower_in_str\n",
    "        else:\n",
    "            return None, None\n",
    "\n",
    "    def highlight_for_replace(self, str1, str2):\n",
    "        if (len(str1) == 3) & (len(str2) == 1):\n",
    "            if str1.count(str2) == 1:\n",
    "                non_word_level, word_level = 0, 1\n",
    "                return non_word_level, word_level\n",
    "            else:\n",
    "                return None, None\n",
    "        elif (len(str1) == 1) & (len(str2) == 3):\n",
    "            if str2.count(str1) == 1:\n",
    "                non_word_level, word_level = 1, 0\n",
    "                return non_word_level, word_level\n",
    "            else:\n",
    "                return None, None\n",
    "        else:\n",
    "            return None, None\n",
    "\n",
    "    def find_diff_position_for_replace(self, str1, str2):\n",
    "        if len(str1) != 3 or len(str2) != 1:\n",
    "            return None\n",
    "\n",
    "        for i, char in enumerate(str1):\n",
    "            if str2 == char:\n",
    "                diff_index = i\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        if diff_index == 0:\n",
    "            diff_index_sem = 1\n",
    "        if diff_index == 1:\n",
    "            diff_index_sem = 0\n",
    "        if diff_index == 2:\n",
    "            diff_index_sem = 2\n",
    "        return diff_index, diff_index_sem\n",
    "    \n",
    "    def existing_variables(self, elements: list):\n",
    "        result = [str(element) for element in elements if not isinstance(element, ConstantExpression)]\n",
    "        return result\n",
    "\n",
    "    # TODO: Sentence-meaning positions do not necessarily have to correspond\n",
    "    def chunk01(self):\n",
    "        rules = self.rules\n",
    "\n",
    "        chuncked_rules = []\n",
    "        new_rules = []\n",
    "\n",
    "        for i in range(len(rules)):\n",
    "            for j in range(i+1, len(rules)):\n",
    "                first_sem = Expression.fromstring(rules[i][\"lhs\"][\"sem\"])\n",
    "                second_sem = Expression.fromstring(rules[j][\"lhs\"][\"sem\"])\n",
    "                first_sentence = rules[i][\"rhs\"]\n",
    "                second_sentence = rules[j][\"rhs\"]\n",
    "                if isinstance(first_sem, ApplicationExpression) & isinstance(second_sem, ApplicationExpression):\n",
    "                    diff_index, diff_index_sem, can_chunk = self.can_chunk01(rules[i], rules[j])\n",
    "                    if can_chunk:\n",
    "                        #print(rules[i])\n",
    "                        #print(rules[j])\n",
    "                        assert rules[i][\"lhs\"][\"con\"] == rules[j][\"lhs\"][\"con\"]\n",
    "                        con = rules[i][\"lhs\"][\"con\"]\n",
    "                        first_sem_elements = [first_sem.pred, first_sem.args[0], first_sem.args[1]]\n",
    "                        second_sem_elements = [second_sem.pred, second_sem.args[0], second_sem.args[1]]\n",
    "\n",
    "                        if \"var\" in rules[i]:\n",
    "                            assignments = copy.deepcopy(rules[i][\"var\"])\n",
    "                        else:\n",
    "                            assignments = {}\n",
    "\n",
    "                        chuncked_rules.append(rules[i])\n",
    "                        chuncked_rules.append(rules[j])\n",
    "\n",
    "                        if diff_index_sem == 0:\n",
    "                            random_category = random.choice(VERB_CATS)\n",
    "                            var = Expression.fromstring(\"X\")\n",
    "                        else:\n",
    "                            # FIX: efficiency\n",
    "                            random_category = random.choice(NOUN_CATS)\n",
    "                            existing_variables = self.existing_variables(first_sem_elements)\n",
    "                            var = [var for var in INDIVIDUAL_VARIABLES if var not in existing_variables][0]\n",
    "                        first_sem_elements_abstracted = copy.deepcopy(first_sem_elements)\n",
    "                        first_sem_elements_abstracted[diff_index_sem] = var\n",
    "                        new_sem_str = f\"{str(first_sem_elements_abstracted[0])}({str(first_sem_elements_abstracted[1])},{str(first_sem_elements_abstracted[2])})\"\n",
    "                        # random_category = random.choice(NONTERMINALS)\n",
    "                        new_sen_str = replace_at_index(first_sentence, diff_index, random_category)\n",
    "                        assignments[str(var)] = diff_index\n",
    "                        assignments_sorted = sorted(assignments.items())\n",
    "                        asignments_sorted_dict = dict((var, idx) for var, idx in assignments_sorted)\n",
    "                        \n",
    "                        new_rule_0 = {\"lhs\": {\"cat\":\"S\", \"sem\": new_sem_str, \"con\":con}, \"rhs\": new_sen_str, \"var\":asignments_sorted_dict}\n",
    "                        \n",
    "                        assert self.is_well_formed(new_sem_str, new_sen_str), f\"Ill-formed rule generated: {new_rule_0}\"\n",
    "                        assert self.is_well_assigned(new_sen_str, asignments_sorted_dict), f\"Ill-assigned rule generated: {new_rule_0}\"\n",
    "\n",
    "                        new_rule_1 = {\"lhs\": {\"cat\":random_category, \"sem\": str(first_sem_elements[diff_index_sem])}, \"rhs\": first_sentence[diff_index]}\n",
    "                        new_rule_2 = {\"lhs\": {\"cat\":random_category, \"sem\": str(second_sem_elements[diff_index_sem])}, \"rhs\": second_sentence[diff_index]}\n",
    "                        #print(\"⌄⌄⌄ Chunk1 Results ⌄⌄⌄\")\n",
    "                        #print(new_rule_0)\n",
    "                        #print(new_rule_1)\n",
    "                        #print(new_rule_2)\n",
    "                        #print(\"⌃⌃⌃ Chunk1 Results ⌃⌃⌃\")\n",
    "                        new_rules += [new_rule_0] + [new_rule_1] + [new_rule_2]\n",
    "        rules = [rule for rule in rules if rule not in chuncked_rules]\n",
    "        rules = rules + new_rules\n",
    "        rules_unique = list(set([str(rule) for rule in rules]))\n",
    "        self.rules = [self.str2dict(rule) for rule in rules_unique]\n",
    "\n",
    "    def chunk02(self):\n",
    "        rules = self.rules\n",
    "\n",
    "        chuncked_rules = []\n",
    "        new_rules = []\n",
    "\n",
    "        for i in range(len(rules)):\n",
    "            for j in range(i+1, len(rules)):\n",
    "                first_sem = Expression.fromstring(rules[i][\"lhs\"][\"sem\"])\n",
    "                second_sem = Expression.fromstring(rules[j][\"lhs\"][\"sem\"])\n",
    "                first_sentence = rules[i][\"rhs\"]\n",
    "                second_sentence = rules[j][\"rhs\"]\n",
    "                if isinstance(first_sem, ApplicationExpression) & isinstance(second_sem, ApplicationExpression):\n",
    "                    diff_index, diff_index_sem, can_chunk, upper_in_str, lower_in_str = self.can_chunk02(rules[i], rules[j])\n",
    "                    if can_chunk:\n",
    "                        assert rules[i][\"lhs\"][\"con\"] == rules[j][\"lhs\"][\"con\"]\n",
    "                        con = rules[i][\"lhs\"][\"con\"]\n",
    "                        first_sem_elements = [first_sem.pred, first_sem.args[0], first_sem.args[1]]\n",
    "                        second_sem_elements = [second_sem.pred, second_sem.args[0], second_sem.args[1]]\n",
    "\n",
    "                        target_position = [i,j][lower_in_str]\n",
    "                        nontarget_position = [i,j][upper_in_str]\n",
    "                        chuncked_rules.append(rules[target_position])\n",
    "\n",
    "                        target_sem = Expression.fromstring(rules[target_position][\"lhs\"][\"sem\"])\n",
    "                        target_sem_elements = [target_sem.pred, target_sem.args[0], target_sem.args[1]]\n",
    "                        target_sentence = rules[target_position][\"rhs\"]\n",
    "                        nontarget_sentence = rules[nontarget_position][\"rhs\"]\n",
    "                        new_rule = {\"lhs\": {\"cat\":nontarget_sentence[diff_index], \"sem\": str(target_sem_elements[diff_index_sem])}, \"rhs\": target_sentence[diff_index]}\n",
    "                        new_rules.append(new_rule)\n",
    "                        # print(new_rule)\n",
    "        rules = [rule for rule in rules if rule not in chuncked_rules]\n",
    "        rules = rules + new_rules\n",
    "        rules_unique = list(set([str(rule) for rule in rules]))\n",
    "        self.rules = [self.str2dict(rule) for rule in rules_unique]\n",
    "        \n",
    "    def abstract(self, element, var, idx, diff_idx):\n",
    "        if idx == diff_idx:\n",
    "            return var\n",
    "        else:\n",
    "            return element\n",
    "    \n",
    "    def can_replace(self, rule1, rule2):\n",
    "        str1, str2 = rule1[\"rhs\"], rule2[\"rhs\"]\n",
    "        cat1, cat2 = rule1[\"lhs\"][\"cat\"], rule2[\"lhs\"][\"cat\"]\n",
    "        sem1_logic = Expression.fromstring(rule1[\"lhs\"][\"sem\"])\n",
    "        sem2_logic = Expression.fromstring(rule2[\"lhs\"][\"sem\"])\n",
    "        \n",
    "        if (cat1 == \"S\") and (cat2 != \"S\"):\n",
    "            sem1, sem2 = [sem1_logic.pred, sem1_logic.args[0], sem1_logic.args[1]], sem2_logic\n",
    "            same_positions_str = []\n",
    "            same_positions_sem = []\n",
    "            for i, char in enumerate(str1):\n",
    "                if str2 == char:\n",
    "                    same_positions_str.append(i)\n",
    "            for i, elm in enumerate(sem1):\n",
    "                if sem2 == elm:\n",
    "                    same_positions_sem.append(i)\n",
    "            if len(same_positions_str) == len(same_positions_sem) > 0:\n",
    "                return True, same_positions_str[0], same_positions_sem[0], 0, 1\n",
    "                # 0: non-word-rule, 1: word-rule\n",
    "            else:\n",
    "                return False, None, None, None, None\n",
    "        elif (cat2 == \"S\") and (cat1 != \"S\"):\n",
    "            sem2, sem1 = [sem2_logic.pred, sem2_logic.args[0], sem2_logic.args[1]], sem1_logic\n",
    "            same_positions_str = []\n",
    "            same_positions_sem = []\n",
    "            for i, char in enumerate(str2):\n",
    "                if str1 == char:\n",
    "                    same_positions_str.append(i)\n",
    "            for i, elm in enumerate(sem2):\n",
    "                if sem1 == elm:\n",
    "                    same_positions_sem.append(i)\n",
    "            if len(same_positions_str) == len(same_positions_sem) > 0:\n",
    "                return True, same_positions_str[0], same_positions_sem[0], 1, 0\n",
    "                # 1: non-word-rule, 0: word-rule\n",
    "            else:\n",
    "                return False, None, None, None, None\n",
    "        else:\n",
    "            return False, None, None, None, None\n",
    "\n",
    "    def replace(self):\n",
    "        rules = self.rules\n",
    "\n",
    "        replaced_rules = []\n",
    "        new_rules = []\n",
    "\n",
    "        for i in range(len(rules)):\n",
    "            for j in range(i+1, len(rules)):\n",
    "                # first_sem = Expression.fromstring(rules[i][\"lhs\"][\"sem\"])\n",
    "                # second_sem = Expression.fromstring(rules[j][\"lhs\"][\"sem\"])\n",
    "                first_sentence = rules[i][\"rhs\"]\n",
    "                second_sentence = rules[j][\"rhs\"]\n",
    "                # non_word_level, word_level = self.highlight_for_replace(first_sentence, second_sentence)\n",
    "                can_replace, same_index_str, same_index_sem, sentence_level, word_level = self.can_replace(rules[i], rules[j])\n",
    "                if can_replace:\n",
    "                    # print(rules[i])\n",
    "                    # print(rules[j])\n",
    "                    sentencerule_index = [i,j][sentence_level]\n",
    "                    wordrule_index = [i,j][word_level]\n",
    "                    sentencerule = rules[sentencerule_index]\n",
    "                    wordrule = rules[wordrule_index]\n",
    "                    replaced_rules.append(sentencerule)\n",
    "                    sentencerule_sem = Expression.fromstring(sentencerule[\"lhs\"][\"sem\"])\n",
    "                    sentencerule_sem_elms = [sentencerule_sem.pred, sentencerule_sem.args[0], sentencerule_sem.args[1]]\n",
    "                    \n",
    "                    if \"var\" in sentencerule:\n",
    "                        assignments = copy.deepcopy(sentencerule[\"var\"])\n",
    "                    else:\n",
    "                        assignments = {}\n",
    "                    \n",
    "                    if same_index_sem == 0:\n",
    "                        var = Expression.fromstring(\"X\")\n",
    "                    else:\n",
    "                        # FIX: efficiency\n",
    "                        # existing_variables = self.existing_variables(sentencerule_sem_elms)\n",
    "                        # print([var for var in INDIVIDUAL_VARIABLES if var not in existing_variables])\n",
    "                        var = [var for var in INDIVIDUAL_VARIABLES if var not in assignments][0]\n",
    "                    sentencerule_rhs = sentencerule[\"rhs\"]\n",
    "                    abstracted_sem_elms = []\n",
    "                    for i, elm in enumerate(sentencerule_sem_elms):\n",
    "                        if i == same_index_sem:\n",
    "                            abstracted_sem_elms.append(var)\n",
    "                        else:\n",
    "                            abstracted_sem_elms.append(sentencerule_sem_elms[i])\n",
    "                    abstracted_sem = f\"{str(abstracted_sem_elms[0])}({str(abstracted_sem_elms[1])},{abstracted_sem_elms[2]})\"\n",
    "\n",
    "                    sentencerule_con = sentencerule[\"lhs\"][\"con\"]\n",
    "\n",
    "                    assignments[str(var)] = same_index_str\n",
    "                    assignments_sorted = sorted(assignments.items())\n",
    "                    asignments_sorted_dict = dict((var, idx) for var, idx in assignments_sorted)\n",
    "\n",
    "                    wordrule_rhs = wordrule[\"rhs\"]\n",
    "                    wordrule_cat = wordrule[\"lhs\"][\"cat\"]\n",
    "                    abstracted_sentencerule_rhs = sentencerule_rhs.replace(wordrule_rhs, wordrule_cat)\n",
    "                    \n",
    "                    new_rule = {\"lhs\": {\"cat\":\"S\", \"sem\":abstracted_sem, \"con\":sentencerule_con}, \"rhs\": abstracted_sentencerule_rhs, \"var\":asignments_sorted_dict}\n",
    "                    \n",
    "                    assert self.is_well_formed(abstracted_sem, abstracted_sentencerule_rhs), f\"Ill-formed rule generated: {new_rule}\"\n",
    "                    assert self.is_well_assigned(abstracted_sentencerule_rhs, asignments_sorted_dict), f\"Ill-assigned rule generated: \\nBefore:\\t{sentencerule}\\n\\t{wordrule}\\nAfter: {new_rule}\"\n",
    "                    \n",
    "                    # print(\"Replace result: \", new_rule)\n",
    "                    new_rules.append(new_rule)\n",
    "        rules = [rule for rule in rules if rule not in replaced_rules]\n",
    "        rules = rules + new_rules\n",
    "        rules_unique = list(set([str(rule) for rule in rules]))\n",
    "        self.rules = [self.str2dict(rule) for rule in rules_unique]\n",
    "    \n",
    "    def repaint_rule(self, rule, replacee, replacer):\n",
    "        print(f\"Before Merge: {rule}\")\n",
    "        # print(rule, replacee, replacer)\n",
    "        if rule[\"lhs\"][\"cat\"] == replacee[\"lhs\"][\"cat\"]:\n",
    "            assert len(rule[\"rhs\"]) == 1\n",
    "            # print(rule[\"lhs\"][\"cat\"], \"->\",replacer[\"lhs\"][\"cat\"])\n",
    "            rule[\"lhs\"][\"cat\"] = replacer[\"lhs\"][\"cat\"]\n",
    "        else:\n",
    "            rhs = copy.deepcopy(rule[\"rhs\"])\n",
    "            new_rhs = rhs.replace(replacee[\"lhs\"][\"cat\"], replacer[\"lhs\"][\"cat\"])\n",
    "            rule[\"rhs\"] = new_rhs\n",
    "            # print(rhs, \"->\", new_rhs)\n",
    "        assert self.is_well_formed(rule[\"lhs\"][\"sem\"], rule[\"rhs\"]), f\"Ill-formed rule generated: {rule}\"\n",
    "        if \"var\" in rule:\n",
    "            assert self.is_well_assigned(rule[\"rhs\"], rule[\"var\"]), f\"Ill-assigned rule generated: {rule}\"\n",
    "        print(f\"After Merge: {rule}\")\n",
    "        return rule\n",
    "    \n",
    "    def is_WordRule(self, rule):\n",
    "        assert type(rule) == dict\n",
    "        if rule[\"lhs\"][\"cat\"] == \"S\":\n",
    "            return False # SentenceRule\n",
    "        else:\n",
    "            return True # WordRule\n",
    "\n",
    "    def can_merge(self, rule1, rule2):\n",
    "        rule1_cat = copy.deepcopy(rule1[\"lhs\"][\"cat\"])\n",
    "        rule1_sem = copy.deepcopy(rule1[\"lhs\"][\"sem\"])\n",
    "        rule1_rhs = copy.deepcopy(rule1[\"rhs\"])\n",
    "        rule2_cat = copy.deepcopy(rule2[\"lhs\"][\"cat\"])\n",
    "        rule2_sem = copy.deepcopy(rule2[\"lhs\"][\"sem\"])\n",
    "        rule2_rhs = copy.deepcopy(rule2[\"rhs\"])\n",
    "        if self.is_WordRule(rule1) and self.is_WordRule(rule2):\n",
    "            if (rule1_sem == rule2_sem) & (rule1_rhs == rule2_rhs) & (rule1_cat != rule2_cat):\n",
    "                cats = [rule1_cat, rule2_cat]\n",
    "                random.shuffle(cats)\n",
    "                selected_cat = cats[0]\n",
    "                deleted_cat = cats[1]\n",
    "                assert (selected_cat.isupper()) and (deleted_cat.isupper())\n",
    "                return True, selected_cat, deleted_cat\n",
    "            else:\n",
    "                return False, None, None\n",
    "        else:\n",
    "            return False, None, None\n",
    "        \n",
    "    def merge(self):\n",
    "        for i in range(len(self.rules)):\n",
    "            for j in range(i+1, len(self.rules)):\n",
    "                can_merge, selected_cat, deleted_cat = self.can_merge(self.rules[i], self.rules[j])\n",
    "                if can_merge:\n",
    "                    for rule in self.rules:\n",
    "                        rule[\"lhs\"][\"cat\"] = rule[\"lhs\"][\"cat\"].replace(deleted_cat, selected_cat)\n",
    "                        rule[\"rhs\"] = rule[\"rhs\"].replace(deleted_cat, selected_cat)\n",
    "                        assert self.is_well_formed(rule[\"lhs\"][\"sem\"], rule[\"rhs\"]), f\"Ill-formed rule generated: {rule}\"\n",
    "                        if \"var\" in rule:\n",
    "                            assert self.is_well_assigned(rule[\"rhs\"], rule[\"var\"]), f\"Ill-assigned rule generated: {rule}\"\n",
    "        rules = self.rules\n",
    "        rules_unique = list(set([str(rule) for rule in rules]))\n",
    "        self.rules = [self.str2dict(rule) for rule in rules_unique]\n",
    "\n",
    "    def leg_merge(self):\n",
    "        rules = self.rules\n",
    "        invited = []\n",
    "        replacer_cats = []\n",
    "        for i in range(len(rules)):\n",
    "            for j in range(i+1, len(rules)):\n",
    "                if self.can_merge(rules[i], rules[j]):\n",
    "                    # print(f\"can merge {rules[i]} and {rules[j]}\")\n",
    "                    if (rules[i][\"lhs\"][\"cat\"] not in replacer_cats) and (rules[j][\"lhs\"][\"cat\"] not in replacer_cats):\n",
    "                        indices = [i, j]\n",
    "                        # print(indices)\n",
    "                        random.shuffle(indices)\n",
    "                        replacer_rule = copy.deepcopy(rules[indices[0]])\n",
    "                        replacee_rule = copy.deepcopy(rules[indices[1]])\n",
    "                        replacer_cats.append(replacer_rule[\"lhs\"][\"cat\"])\n",
    "                        # print(\"replacee_rule: \", replacee_rule)\n",
    "                        # print(\"replacer_rule: \", replacer_rule)\n",
    "                        rules = [self.repaint_rule(rule, replacee_rule, replacer_rule) for rule in rules]\n",
    "                    elif (rules[i][\"lhs\"][\"cat\"] in replacer_cats) and (rules[j][\"lhs\"][\"cat\"] in replacer_cats):\n",
    "                        continue\n",
    "                    elif rules[i][\"lhs\"][\"cat\"] in replacer_cats:\n",
    "                        replacer_rule = copy.deepcopy(rules[i])\n",
    "                        replacee_rule = copy.deepcopy(rules[j])\n",
    "                        # print(\"replacee_rule: \", replacee_rule)\n",
    "                        # print(\"replacer_rule: \", replacer_rule)\n",
    "                        rules = [self.repaint_rule(rule, replacee_rule, replacer_rule) for rule in rules]\n",
    "                    else:\n",
    "                        replacer_rule = copy.deepcopy(rules[j])\n",
    "                        replacee_rule = copy.deepcopy(rules[i])\n",
    "                        # print(\"replacee_rule: \", replacee_rule)\n",
    "                        # print(\"replacer_rule: \", replacer_rule)\n",
    "                        rules = [self.repaint_rule(rule, replacee_rule, replacer_rule) for rule in rules]\n",
    "                else:\n",
    "                    continue\n",
    "        for rule in rules:\n",
    "            assert self.is_well_formed(rule[\"lhs\"][\"sem\"], rule[\"rhs\"]), f\"Ill-formed rule generated: {rule}\"\n",
    "            if \"var\" in rule:\n",
    "                assert self.is_well_assigned(rule[\"rhs\"], rule[\"var\"]), f\"Ill-assigned rule generated: {rule}\"\n",
    "        rules_unique = list(set([str(rule) for rule in rules]))\n",
    "        self.rules = [self.str2dict(rule) for rule in rules_unique]\n",
    "\n",
    "    def invent_wordrule(self, sem):\n",
    "        if str(sem).replace(\"_\",\"\") in NOUNS:\n",
    "            random_category = random.choice(NOUN_CATS)\n",
    "            rhs = generate_noun(5, [\"x\", \"y\"])\n",
    "        if str(sem).replace(\"_\",\"\") in (VERBS+VERBS_PSS):\n",
    "            random_category = random.choice(VERB_CATS)\n",
    "            rhs = generate_verb(5)\n",
    "        self.add_rule(f\"{random_category}/{str(sem)} -> {rhs}\")\n",
    "        return rhs\n",
    "\n",
    "    def invent_holisticrule(self, sem, query_con):\n",
    "        subj = generate_noun(5, [\"x\", \"y\"])\n",
    "        obj = generate_noun(5, [subj, \"x\", \"y\"])\n",
    "        verb = generate_verb(5)\n",
    "        sentence = f\"{subj}{verb}{obj}\"\n",
    "        self.add_rule(f\"S/{sem}/{query_con} -> {sentence}\")\n",
    "        return sentence\n",
    "    # TODO: test\n",
    "    def generate(self, query_str, query_con, debug=False):\n",
    "        rules = self.rules\n",
    "\n",
    "        if debug:\n",
    "            print(\"query: \", query_str)\n",
    "\n",
    "        query = Expression.fromstring(query_str)\n",
    "        query_elements = [query.args[0], query.pred, query.args[1]]\n",
    "\n",
    "        sem_list = self.sem_list()\n",
    "        sentence_list = self.sentence_list()\n",
    "\n",
    "        if (query_str in sem_list) and (rules[sem_list.index(query_str)]['lhs']['con']==query_con):\n",
    "            holistic_rule = rules[sem_list.index(query_str)]\n",
    "            holistic_rule_rhs = holistic_rule['rhs']\n",
    "            if debug:\n",
    "                print(\"generated by a holictic rule: \", f\"S/{str(query)}/{query_con} -> {holistic_rule_rhs}\")\n",
    "            return f\"S/{str(query)}/{query_con} -> {holistic_rule_rhs}\", \"by-holistic-rule\"\n",
    "        else:\n",
    "            for i, sem_str in enumerate(sem_list):\n",
    "                sem = Expression.fromstring(sem_str)\n",
    "                if isinstance(sem, ApplicationExpression):\n",
    "                    rule = rules[i]\n",
    "                    con = rule[\"lhs\"][\"con\"]\n",
    "                    sem_elements = [sem.args[0], sem.pred, sem.args[1]]\n",
    "                    matches = [(i, (query_element,sem_element)) for i, (query_element,sem_element) in enumerate(zip(query_elements, sem_elements)) if (query_element==sem_element) & (isinstance(query_element, ConstantExpression) & isinstance(sem_element, ConstantExpression))]\n",
    "                    slots = [(i, (query_element,sem_element)) for i, (query_element,sem_element) in enumerate(zip(query_elements, sem_elements)) if (query_element!=sem_element) & (not isinstance(sem_element, ConstantExpression))]\n",
    "                    if (len(matches)==2) & (len(slots)==1) & (query_con==con):\n",
    "                        sentence_with_slot = sentence_list[i]\n",
    "                        slot_position = slots[0][0]\n",
    "                        slot_sem = slots[0][1][0]\n",
    "                        slot_var_sem = slots[0][1][1]\n",
    "                        substitute_position = rule[\"var\"][str(slot_var_sem)]\n",
    "                        slot_category = sentence_with_slot[substitute_position]\n",
    "                        word_rules = [rule for rule in rules if (rule['lhs']['cat']==slot_category) & (rule['lhs']['sem']==str(slot_sem))]\n",
    "                        if len(word_rules) > 0:\n",
    "                            selected_rule = self.str2dict(str(random.sample(word_rules, 1)[0]))\n",
    "                            if substitute_position == 0:\n",
    "                                generated_sentence = selected_rule['rhs'] + sentence_with_slot[1:]\n",
    "                            if substitute_position == 1:\n",
    "                                generated_sentence = sentence_with_slot[0] + selected_rule['rhs'] + sentence_with_slot[2]\n",
    "                            if substitute_position == 2:\n",
    "                                generated_sentence = sentence_with_slot[:2] + selected_rule['rhs']\n",
    "                            assert generated_sentence.islower()\n",
    "                            return f\"S/{str(query)}/{query_con} -> {generated_sentence}\", \"by-composition\"\n",
    "                        if len(word_rules) == 0:\n",
    "                            invented_form = self.invent_wordrule(slot_sem)\n",
    "                            if substitute_position == 0:\n",
    "                                generated_sentence = invented_form + sentence_with_slot[1:]\n",
    "                            if substitute_position == 1:\n",
    "                                generated_sentence = sentence_with_slot[0] + invented_form + sentence_with_slot[2]\n",
    "                            if substitute_position == 2:\n",
    "                                generated_sentence = sentence_with_slot[:2] + invented_form\n",
    "                            assert generated_sentence.islower()\n",
    "                            return f\"S/{str(query)}/{query_con} -> {generated_sentence}\", \"by-word-invention\"\n",
    "                    if (len(matches)==1) & (len(slots)==2) & (query_con==con):\n",
    "                        sentence_with_slot = sentence_list[i]\n",
    "                        slot_position = slots[0][0]\n",
    "                        slot_sem = slots[0][1][0]\n",
    "                        slot_var_sem = slots[0][1][1]\n",
    "                        substitute_position = rule[\"var\"][str(slot_var_sem)]\n",
    "                        slot_category = sentence_with_slot[substitute_position]\n",
    "                        word_rules = [rule for rule in rules if (rule['lhs']['cat']==slot_category) & (rule['lhs']['sem']==str(slot_sem))]\n",
    "                        if len(word_rules) > 0:\n",
    "                            selected_rule = self.str2dict(str(random.sample(word_rules, 1)[0]))\n",
    "                            if substitute_position == 0:\n",
    "                                new_sentence_with_slot = selected_rule['rhs'] + sentence_with_slot[1:]\n",
    "                            if substitute_position == 1:\n",
    "                                new_sentence_with_slot = sentence_with_slot[0] + selected_rule['rhs'] + sentence_with_slot[2]\n",
    "                            if substitute_position == 2:\n",
    "                                new_sentence_with_slot = sentence_with_slot[:2] + selected_rule['rhs']\n",
    "                            strategy = \"by-composition\"\n",
    "                        if len(word_rules) == 0:\n",
    "                            invented_form = self.invent_wordrule(slot_sem)\n",
    "                            if substitute_position == 0:\n",
    "                                new_sentence_with_slot = invented_form + sentence_with_slot[1:]\n",
    "                            if substitute_position == 1:\n",
    "                                new_sentence_with_slot = sentence_with_slot[0] + invented_form + sentence_with_slot[2]\n",
    "                            if substitute_position == 2:\n",
    "                                new_sentence_with_slot = sentence_with_slot[:2] + invented_form\n",
    "                            strategy = \"by-word-invention\"\n",
    "                        slot_position = slots[1][0]\n",
    "                        slot_sem = slots[1][1][0]\n",
    "                        slot_var_sem = slots[1][1][1]\n",
    "                        substitute_position = rule[\"var\"][str(slot_var_sem)]\n",
    "                        slot_category = sentence_with_slot[substitute_position]\n",
    "                        word_rules = [rule for rule in rules if (rule['lhs']['cat']==slot_category) & (rule['lhs']['sem']==str(slot_sem))]\n",
    "                        if len(word_rules) > 0:\n",
    "                            selected_rule = self.str2dict(str(random.sample(word_rules, 1)[0]))\n",
    "                            if substitute_position == 0:\n",
    "                                generated_sentence = selected_rule['rhs'] + new_sentence_with_slot[1:]\n",
    "                            if substitute_position == 1:\n",
    "                                generated_sentence = new_sentence_with_slot[0] + selected_rule['rhs'] + new_sentence_with_slot[2]\n",
    "                            if substitute_position == 2:\n",
    "                                generated_sentence = new_sentence_with_slot[:2] + selected_rule['rhs']\n",
    "                            assert generated_sentence.islower()\n",
    "                            if strategy == \"by-composition\":\n",
    "                                return f\"S/{str(query)}/{query_con} -> {generated_sentence}\", strategy\n",
    "                            else:\n",
    "                                return f\"S/{str(query)}/{query_con} -> {generated_sentence}\", strategy\n",
    "                        if len(word_rules) == 0:\n",
    "                            if strategy == \"by-composition\":\n",
    "                                invented_form = self.invent_wordrule(slot_sem)\n",
    "                                if substitute_position == 0:\n",
    "                                    generated_sentence = invented_form + new_sentence_with_slot[1:]\n",
    "                                if substitute_position == 1:\n",
    "                                    generated_sentence = new_sentence_with_slot[0] + invented_form + new_sentence_with_slot[2]\n",
    "                                if substitute_position == 2:\n",
    "                                    generated_sentence = new_sentence_with_slot[:2] + invented_form\n",
    "                                assert generated_sentence.islower(), query_str\n",
    "                                strategy = \"by-word-invention\"\n",
    "                                return f\"S/{str(query)}/{query_con} -> {generated_sentence}\", strategy\n",
    "                            else:\n",
    "                                self.rules = self.rules[:-1]\n",
    "                                generated_sentence = self.invent_holisticrule(query, query_con)\n",
    "                                assert generated_sentence.islower()\n",
    "                                return f\"S/{str(query)}/{query_con} -> {generated_sentence}\", \"by-holistic-invention\"\n",
    "                    if (len(matches)==0) & (len(slots)==3) & (query_con==con):\n",
    "                        sentence_with_slot = sentence_list[i]\n",
    "                        slot_position = slots[0][0]\n",
    "                        slot_sem = slots[0][1][0]\n",
    "                        slot_var_sem = slots[0][1][1]\n",
    "                        substitute_position = rule[\"var\"][str(slot_var_sem)]\n",
    "                        slot_category = sentence_with_slot[substitute_position]\n",
    "                        word_rules = [rule for rule in rules if (rule['lhs']['cat']==slot_category) & (rule['lhs']['sem']==str(slot_sem))]\n",
    "                        if len(word_rules) > 0:\n",
    "                            selected_rule = self.str2dict(str(random.sample(word_rules, 1)[0]))\n",
    "                            if substitute_position == 0:\n",
    "                                new_sentence_with_slot = selected_rule['rhs'] + sentence_with_slot[1:]\n",
    "                            if substitute_position == 1:\n",
    "                                new_sentence_with_slot = sentence_with_slot[0] + selected_rule['rhs'] + sentence_with_slot[2]\n",
    "                            if substitute_position == 2:\n",
    "                                new_sentence_with_slot = sentence_with_slot[:2] + selected_rule['rhs']\n",
    "                            strategy = \"by-composition\"\n",
    "                        if len(word_rules) == 0:\n",
    "                            invented_form = self.invent_wordrule(slot_sem)\n",
    "                            if substitute_position == 0:\n",
    "                                new_sentence_with_slot = invented_form + sentence_with_slot[1:]\n",
    "                            if substitute_position == 1:\n",
    "                                new_sentence_with_slot = sentence_with_slot[0] + invented_form + sentence_with_slot[2]\n",
    "                            if substitute_position == 2:\n",
    "                                new_sentence_with_slot = sentence_with_slot[:2] + invented_form\n",
    "                            strategy = \"by-word-invention\"\n",
    "                        slot_sem = slots[1][1][0]\n",
    "                        slot_var_sem = slots[1][1][1]\n",
    "                        substitute_position = rule[\"var\"][str(slot_var_sem)]\n",
    "                        slot_category = sentence_with_slot[substitute_position]\n",
    "                        word_rules = [rule for rule in rules if (rule['lhs']['cat']==slot_category) & (rule['lhs']['sem']==str(slot_sem))]\n",
    "                        if len(word_rules) > 0:\n",
    "                            selected_rule = self.str2dict(str(random.sample(word_rules, 1)[0]))\n",
    "                            if substitute_position == 0:\n",
    "                                new_sentence_with_slot = selected_rule['rhs'] + new_sentence_with_slot[1:]\n",
    "                            if substitute_position == 1:\n",
    "                                new_sentence_with_slot = new_sentence_with_slot[0] + selected_rule['rhs'] + new_sentence_with_slot[2]\n",
    "                            if substitute_position == 2:\n",
    "                                new_sentence_with_slot = new_sentence_with_slot[:2] + selected_rule['rhs']\n",
    "                        if len(word_rules) == 0:\n",
    "                            if strategy == \"by-composition\":\n",
    "                                invented_form = self.invent_wordrule(slot_sem)\n",
    "                                if substitute_position == 0:\n",
    "                                    new_sentence_with_slot = invented_form + new_sentence_with_slot[1:]\n",
    "                                if substitute_position == 1:\n",
    "                                    new_sentence_with_slot = new_sentence_with_slot[0] + invented_form + new_sentence_with_slot[2]\n",
    "                                if substitute_position == 2:\n",
    "                                    new_sentence_with_slot = new_sentence_with_slot[:2] + invented_form\n",
    "                                strategy = \"by-word-invention\"\n",
    "                            else:\n",
    "                                self.rules = self.rules[:-1]\n",
    "                                generated_sentence = self.invent_holisticrule(query, query_con)\n",
    "                                assert generated_sentence.islower()\n",
    "                                return f\"S/{str(query)}/{query_con} -> {generated_sentence}\", \"by-holistic-invention\"\n",
    "                        slot_position = slots[2][0]\n",
    "                        slot_sem = slots[2][1][0]\n",
    "                        slot_var_sem = slots[2][1][1]\n",
    "                        substitute_position = rule[\"var\"][str(slot_var_sem)]\n",
    "                        slot_category = sentence_with_slot[substitute_position]\n",
    "                        word_rules = [rule for rule in rules if (rule['lhs']['cat']==slot_category) & (rule['lhs']['sem']==str(slot_sem))]\n",
    "                        if len(word_rules) > 0:\n",
    "                            selected_rule = self.str2dict(str(random.sample(word_rules, 1)[0]))\n",
    "                            if substitute_position == 0:\n",
    "                                generated_sentence = selected_rule['rhs'] + new_sentence_with_slot[1:]\n",
    "                            if substitute_position == 1:\n",
    "                                generated_sentence = new_sentence_with_slot[0] + selected_rule['rhs'] + new_sentence_with_slot[2]\n",
    "                            if substitute_position == 2:\n",
    "                                generated_sentence = new_sentence_with_slot[:2] + selected_rule['rhs']\n",
    "                            assert generated_sentence.islower()\n",
    "                            return f\"S/{str(query)}/{query_con} -> {generated_sentence}\", strategy\n",
    "                        if len(word_rules) == 0:\n",
    "                            if strategy == \"by-composition\":\n",
    "                                invented_form = self.invent_wordrule(slot_sem)\n",
    "                                if substitute_position == 0:\n",
    "                                    generated_sentence = invented_form + new_sentence_with_slot[1:]\n",
    "                                if substitute_position == 1:\n",
    "                                    generated_sentence = new_sentence_with_slot[0] + invented_form + new_sentence_with_slot[2]\n",
    "                                if substitute_position == 2:\n",
    "                                    generated_sentence = new_sentence_with_slot[:2] + invented_form\n",
    "                                strategy = \"by-word-invention\"\n",
    "                                assert generated_sentence.islower()\n",
    "                                return f\"S/{str(query)}/{query_con} -> {generated_sentence}\", strategy\n",
    "                            else:\n",
    "                                self.rules = self.rules[:-1]\n",
    "                                generated_sentence = self.invent_holisticrule(query, query_con)\n",
    "                                assert generated_sentence.islower()\n",
    "                                return f\"S/{str(query)}/{query_con} -> {generated_sentence}\", \"by-holistic-invention\"\n",
    "                    else:\n",
    "                        continue\n",
    "                else:\n",
    "                    continue\n",
    "            generated_sentence = self.invent_holisticrule(query, query_con)\n",
    "            assert generated_sentence.islower()\n",
    "            return f\"S/{str(query)}/{query_con} -> {generated_sentence}\", \"by-holistic-invention\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dec4493-f8b9-45d9-b9f6-e7d9ec9892b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
