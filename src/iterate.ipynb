{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b91612b-fe5d-4075-841b-a499d2b15cdf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "multiple exception types must be parenthesized (mc_bin_client.py, line 278)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3526\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[2], line 2\u001b[0m\n    from tap import Tap\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tap.py:6\u001b[0;36m\n\u001b[0;31m    from mc_bin_client import mc_bin_client, memcacheConstants as Constants\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/mc_bin_client/mc_bin_client.py:278\u001b[0;36m\u001b[0m\n\u001b[0;31m    except MemcachedError, e:\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m multiple exception types must be parenthesized\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tap import Tap\n",
    "import logging\n",
    "import os\n",
    "from grammar import Grammar\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "from visualize import eps_len, TopSim\n",
    "import json\n",
    "from create_initial_grammar import generate_random_holistics\n",
    "from Levenshtein import distance\n",
    "from utils import sem_distance\n",
    "import numpy as np\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class Args(Tap):\n",
    "    data: str = \"data\"\n",
    "    initial_grammar: str = None\n",
    "    P: float = 0.8\n",
    "    n_gens: int = 1\n",
    "    n_samples: int = 100\n",
    "    visualize: bool = True\n",
    "    zeros_size: float = 0.5\n",
    "    separated: bool = False\n",
    "    seed: int = 1\n",
    "    search_order: str = \"c m r\"\n",
    "    compositional_size: float = None\n",
    "    output_grammars: bool = False\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = Args().parse_args()\n",
    "    logging.basicConfig(\n",
    "        format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "        level=logging.INFO\n",
    "    )\n",
    "    random.seed(args.seed)\n",
    "    P = args.P\n",
    "    now = datetime.datetime.now()\n",
    "    now_fmt = now.strftime('%Y%m%d_%H%M%S')\n",
    "    os.mkdir(f\"out/exp{now_fmt}\")\n",
    "    if args.output_grammars:\n",
    "        os.mkdir(f\"out/exp{now_fmt}/generations\")\n",
    "\n",
    "    def infer_concept(gold, p_correct):\n",
    "        if random.random() < p_correct:\n",
    "            return gold\n",
    "        else:\n",
    "            return 1 - gold\n",
    "\n",
    "    if args.initial_grammar == \"compositional\":\n",
    "        semspace_file = os.path.join(args.data, \"semantic_space.txt\")\n",
    "        with open(semspace_file, \"r\") as f:\n",
    "            semantic_space = f.read()[:-1]\n",
    "    elif args.initial_grammar != None:\n",
    "        semspace_file = args.initial_grammar\n",
    "        with open(semspace_file, \"r\") as f:\n",
    "            semantic_space = f.read()[:-1]\n",
    "    elif args.compositional_size != None:\n",
    "        assert 0 <= args.compositional_size <= 1\n",
    "        n_comps = int((args.n_samples)*args.compositional_size)\n",
    "        n_randoms = args.n_samples - n_comps\n",
    "        semspace_file = os.path.join(args.data, \"semantic_space.txt\")\n",
    "        with open(semspace_file, \"r\") as f:\n",
    "            compositionals = f.read()[:-1]\n",
    "            compositionals_list = compositionals.split(\"\\n\")\n",
    "            compositionals_list_samples = random.sample(compositionals_list, n_comps)\n",
    "        randoms = generate_random_holistics(args.seed,\n",
    "                                                n_randoms,\n",
    "                                                args.data,\n",
    "                                                args.zeros_size)\n",
    "        randoms_list = randoms.split(\"\\n\")\n",
    "        semantic_space_list = compositionals_list_samples + randoms_list\n",
    "        semantic_space = \"\\n\".join(semantic_space_list)\n",
    "    else:\n",
    "        semantic_space = generate_random_holistics(args.seed,\n",
    "                                                    args.n_samples,\n",
    "                                                    args.data,\n",
    "                                                    args.zeros_size)\n",
    "\n",
    "    qspace_file = os.path.join(args.data, \"query_space.txt\")\n",
    "    with open(qspace_file, \"r\") as f:\n",
    "        query_lines = f.read().splitlines()\n",
    "        query_space = [(q.split(\"\\t\")[0], int(q.split(\"\\t\")[1])) for q in query_lines]\n",
    "        query_zeros_space = [(q.split(\"\\t\")[0], int(q.split(\"\\t\")[1])) for q in query_lines if int(q.split(\"\\t\")[1])==0]\n",
    "        query_ones_space = [(q.split(\"\\t\")[0], int(q.split(\"\\t\")[1])) for q in query_lines if int(q.split(\"\\t\")[1])==1]\n",
    "\n",
    "    OPERATIONS = [\"chunk01\", \"chunk02\", \"merge\", \"replace\"]\n",
    "    SEARCH_ORDER = args.search_order.split(\" \")\n",
    "\n",
    "    def apply_rule(grammar, search_order):\n",
    "        changed = None\n",
    "        if args.separated:\n",
    "            assert len(search_order) == 4, \"Since separated is True, search_order must consist of the following three values: c1, c2, m, r (e.g., \\\"c1 c2 m r\\\")\"\n",
    "            while changed != False:\n",
    "                for rule in search_order:\n",
    "                    if rule == \"c1\":\n",
    "                        rules_before = grammar.to_string()\n",
    "                        grammar.chunk01()\n",
    "                        rules_after = grammar.to_string()\n",
    "                        if rules_before != rules_after:\n",
    "                            changed == True\n",
    "                            break\n",
    "                        else:\n",
    "                            changed = False\n",
    "                            continue\n",
    "                    if rule == \"c2\":\n",
    "                        rules_before = grammar.to_string()\n",
    "                        grammar.chunk02()\n",
    "                        rules_after = grammar.to_string()\n",
    "                        if rules_before != rules_after:\n",
    "                            changed == True\n",
    "                            break\n",
    "                        else:\n",
    "                            changed = False\n",
    "                            continue\n",
    "                    if rule == \"m\":\n",
    "                        rules_before = grammar.to_string()\n",
    "                        grammar.merge()\n",
    "                        rules_after = grammar.to_string()\n",
    "                        if rules_before != rules_after:\n",
    "                            changed == True\n",
    "                            break\n",
    "                        else:\n",
    "                            changed = False\n",
    "                            continue\n",
    "                    if rule == \"r\":\n",
    "                        rules_before = grammar.to_string()\n",
    "                        grammar.replace()\n",
    "                        rules_after = grammar.to_string()\n",
    "                        if rules_before != rules_after:\n",
    "                            changed == True\n",
    "                            break\n",
    "                        else:\n",
    "                            changed = False\n",
    "                            continue\n",
    "        if args.separated == False:\n",
    "            assert len(search_order) == 3, \"Since separated is False, search_order must consist of the following three values: c, m, r (e.g., \\\"c m r\\\")\"\n",
    "            while changed != False:\n",
    "                for rule in search_order:\n",
    "                    if rule == \"c\":\n",
    "                        rules_before = grammar.to_string()\n",
    "                        grammar.chunk01()\n",
    "                        grammar.chunk02()\n",
    "                        rules_after = grammar.to_string()\n",
    "                        if rules_before != rules_after:\n",
    "                            changed == True\n",
    "                            break\n",
    "                        else:\n",
    "                            changed = False\n",
    "                            continue\n",
    "                    if rule == \"m\":\n",
    "                        rules_before = grammar.to_string()\n",
    "                        grammar.merge()\n",
    "                        rules_after = grammar.to_string()\n",
    "                        if rules_before != rules_after:\n",
    "                            changed == True\n",
    "                            break\n",
    "                        else:\n",
    "                            changed = False\n",
    "                            continue\n",
    "                    if rule == \"r\":\n",
    "                        rules_before = grammar.to_string()\n",
    "                        grammar.replace()\n",
    "                        rules_after = grammar.to_string()\n",
    "                        if rules_before != rules_after:\n",
    "                            changed == True\n",
    "                            break\n",
    "                        else:\n",
    "                            changed = False\n",
    "                            continue\n",
    "        return grammar\n",
    "\n",
    "    grammar = Grammar()\n",
    "    grammar.from_string(semantic_space)\n",
    "    utterances = grammar.to_string().split(\"\\n\") [:-2]\n",
    "\n",
    "    n_gens = args.n_gens\n",
    "    n_samples = args.n_samples\n",
    "\n",
    "    use_rules_only_hist = []\n",
    "    grammars = []\n",
    "    epsilons = []\n",
    "    lengths = []\n",
    "    topsims_zero = []\n",
    "    topsims_one = []\n",
    "\n",
    "    for i in range(n_gens):\n",
    "        logging.info(f\"GENERATION {i} STARTS LEARNING\")\n",
    "        last_generation = Grammar()\n",
    "        # n_chunk01 = 0\n",
    "        # n_chunk02 = 0\n",
    "        # n_merge = 0\n",
    "        # n_replace = 0\n",
    "        for utterance in tqdm(utterances):\n",
    "            # print(utterance)\n",
    "            utt_split = utterance.split(\" \")\n",
    "            lhs = utt_split[0].split(\"/\")\n",
    "            rhs = utt_split[2]\n",
    "            lhs_cat = lhs[0]\n",
    "            lhs_sem = lhs[1]\n",
    "            lhs_con_gold = int(lhs[2])\n",
    "            if args.zeros_size != 1:\n",
    "                infered_con = infer_concept(lhs_con_gold, P)\n",
    "            else:\n",
    "                infered_con = 0\n",
    "            infered_utt = f\"S/{lhs_sem}/{infered_con} -> {rhs}\"\n",
    "            last_generation.add_rule(str(infered_utt))\n",
    "            # operation = random.sample(OPERATIONS, 1)[0]\n",
    "            last_generation = apply_rule(last_generation, SEARCH_ORDER)\n",
    "        length = len(last_generation.rules)\n",
    "        grammars.append(last_generation.rules)\n",
    "        logging.info(f\"Learning finished. Result: out/exp{now_fmt}/generations/gen-{i}.txt\")\n",
    "        # print(f\"chunk01: {n_chunk01}\\nchunk02: {n_chunk02}\\nmerge: {n_merge}\\nreplace: {n_replace}\")\n",
    "        if args.output_grammars:\n",
    "            with open(f\"out/exp{now_fmt}/generations/gen-{i}.txt\", \"w\") as f:\n",
    "                f.write(last_generation.to_string())\n",
    "\n",
    "        utterances = []\n",
    "        use_rules_only = 0\n",
    "        use_invention = 0\n",
    "        if args.zeros_size != 1:\n",
    "            queries = random.sample(query_space, n_samples) # bottleneck\n",
    "        else:\n",
    "            queries = random.sample(query_zeros_space, n_samples) # bottleneck\n",
    "        generator = Grammar()\n",
    "        generator.from_string(last_generation.to_string()[:-2])\n",
    "        \n",
    "        logging.info(f\"generating {n_samples} utterances\")\n",
    "        for query, query_con in queries:\n",
    "            utterance, strategy = generator.generate(query, query_con) # generate n_samples utterances\n",
    "            utterances.append(utterance)\n",
    "        logging.info(f\"evaluating\")\n",
    "        if args.zeros_size != 1: # considering conceptualization\n",
    "            selected_queries_zero = []\n",
    "            selected_queries_one = []\n",
    "            uttered_forms_zero = []\n",
    "            uttered_forms_one = []\n",
    "            for query, query_con in query_space:\n",
    "                generator = Grammar()\n",
    "                generator.from_string(last_generation.to_string()[:-2])\n",
    "                uttered, strategy = generator.generate(query, query_con)\n",
    "                if query_con == 0:\n",
    "                    selected_queries_zero.append(query)\n",
    "                    uttered_forms_zero.append(uttered.split(\" \")[2])\n",
    "                    if (strategy == \"by-composition\") or (strategy == \"by-holistic-rule\"):\n",
    "                        use_rules_only += 1\n",
    "                    else:\n",
    "                        use_invention += 1\n",
    "                if query_con == 1:\n",
    "                    selected_queries_one.append(query)\n",
    "                    uttered_forms_one.append(uttered.split(\" \")[2])\n",
    "                    if (strategy == \"by-composition\") or (strategy == \"by-holistic-rule\"):\n",
    "                        use_rules_only += 1\n",
    "                    else:\n",
    "                        use_invention += 1\n",
    "            epsilons.append((use_rules_only/(len(query_space)))*100)\n",
    "            lengths.append(length)\n",
    "            print(f\"ε: {epsilons}\")\n",
    "            print(f\"Length: {lengths}\")\n",
    "        else: # conventional ILM\n",
    "            selected_queries = []\n",
    "            uttered_forms = []\n",
    "            for query, query_con in query_zeros_space:\n",
    "                selected_queries.append(query)\n",
    "                generator = Grammar()\n",
    "                generator.from_string(last_generation.to_string()[:-2])\n",
    "                uttered, strategy = generator.generate(query, query_con)\n",
    "                uttered_forms.append(uttered.split(\" \")[2])\n",
    "                if (strategy == \"by-composition\") or (strategy == \"by-holistic-rule\"):\n",
    "                    use_rules_only += 1\n",
    "                else:\n",
    "                    use_invention += 1\n",
    "            epsilons.append((use_rules_only/len(query_zeros_space))*100)\n",
    "            lengths.append(length)\n",
    "            print(f\"epsilon: {use_rules_only}/{len(query_zeros_space)}\")\n",
    "            print(\"History:\")\n",
    "            print(f\"\\tε: {epsilons}\")\n",
    "            print(f\"\\tLength: {lengths}\")\n",
    "        if args.zeros_size != 1:\n",
    "            sem_distances_zero = []\n",
    "            form_distances_zero = []\n",
    "            sem_distances_one = []\n",
    "            form_distances_one = []\n",
    "            for i in range(len(selected_queries_zero)):\n",
    "                for j in range(i+1, len(selected_queries_zero)):\n",
    "                    sem_i, sem_j = selected_queries_zero[i], selected_queries_zero[j]\n",
    "                    utt_i, utt_j = uttered_forms_zero[i], uttered_forms_zero[j]\n",
    "                    sem_distances_zero.append(sem_distance(sem_i, sem_j))\n",
    "                    form_distances_zero.append(distance(utt_i, utt_j))\n",
    "            for i in range(len(selected_queries_one)):\n",
    "                for j in range(i+1, len(selected_queries_one)):\n",
    "                    sem_i, sem_j = selected_queries_one[i], selected_queries_one[j]\n",
    "                    utt_i, utt_j = uttered_forms_one[i], uttered_forms_one[j]\n",
    "                    sem_distances_one.append(sem_distance(sem_i, sem_j))\n",
    "                    form_distances_one.append(distance(utt_i, utt_j))\n",
    "        else:\n",
    "            sem_distances_zero = []\n",
    "            form_distances_zero = []\n",
    "            for i in range(len(selected_queries)):\n",
    "                for j in range(i+1, len(selected_queries)):\n",
    "                    sem_i, sem_j = selected_queries[i], selected_queries[j]\n",
    "                    utt_i, utt_j = uttered_forms[i], uttered_forms[j]\n",
    "                    sem_distances_zero.append(sem_distance(sem_i, sem_j))\n",
    "                    form_distances_zero.append(distance(utt_i, utt_j))\n",
    "        topsim_zero = np.corrcoef(sem_distances_zero, form_distances_zero)[0][1]\n",
    "        topsims_zero.append(topsim_zero)\n",
    "        if args.zeros_size != 1:\n",
    "            topsim_one = np.corrcoef(sem_distances_one, form_distances_one)[0][1]\n",
    "            topsims_one.append(topsim_one)\n",
    "        print(f\"\\tTopSims_0: {topsims_zero}\")\n",
    "        print(f\"\\tTopSims_1: {topsims_one}\")\n",
    "    with open(f\"out/exp{now_fmt}/states.json\", \"w\") as f:\n",
    "        settings = {\"initial_grammar\":args.initial_grammar, \"n_gens\":args.n_gens,\n",
    "                    \"n_samples\":args.n_samples, \"p\":args.P,\n",
    "                    \"zeros_size\":args.zeros_size,\n",
    "                    \"compositional_size\": args.compositional_size,\n",
    "                    \"seed\":args.seed}\n",
    "        results = {\"epsilons\":epsilons, \"lengths\":lengths, \"topsims_0\":topsims_zero, \"topsims_1\":topsims_one}\n",
    "        states = {\"settings\":settings, \"results\":results}\n",
    "        states_json = json.dumps(states)\n",
    "        f.write(states_json)\n",
    "    if args.visualize:\n",
    "        eps_len(epsilons, lengths, n_gens, f\"out/exp{now_fmt}/eps_len.png\")\n",
    "        TopSim(topsims_zero, n_gens, f\"out/exp{now_fmt}/topsim_0.png\")\n",
    "        if args.zeros_size != 1:\n",
    "            TopSim(topsims_one, n_gens, f\"out/exp{now_fmt}/topsim_1.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
